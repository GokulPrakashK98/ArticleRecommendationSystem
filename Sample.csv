pmcid,pmid,title,abstract,introduction,methods,results,discussion,doi
PMC10705478,38076930,Predicting Impacts of Contact Tracing on Epidemiological Inference from Phylogenetic Data,"Robust sampling methods are foundational to many inference problems in the phylodynamic field, yet the impact of using contact tracing, a type of non-uniform sampling used in public health applications, is not well understood. To investigate and quantify how this non-uniform sampling method influences recovered phylogenetic tree structure, we developed a new simulation tool called SEEPS (Sequence Evolution and Epidemiological Process Simulator) that allows for the simulation of contact tracing and the resulting transmission tree, pathogen phylogeny, and corresponding virus genetic sequences. Importantly, SEEPS takes within-host evolution into account when generating pathogen phylogenies and sequences from transmission histories. Using SEEPS, we demonstrate that contact tracing can significantly impact the structure of the resulting tree as described by popular tree statistics. Contact tracing generates phylogenies that are less balanced than the underlying transmission process, less representative of the larger epidemiological process, and affects the internal/external branch length ratios that characterize specific epidemiological scenarios. We also examine a 2007–2008 Swedish HIV-1 outbreak and the broader 1998–2010 European HIV-1 epidemic to highlight the differences in contact tracing and expected phylogenies. Aided by SEEPS, we show that the Swedish outbreak was strongly influenced by contact tracing even after downsampling, while the broader European Union epidemic showed little evidence of universal contact tracing, agreeing with the known epidemiological information about sampling and spread. SEEPS is available at github.com/MolEvolEpid/SEEPS.","The growth and prevalence of communicable diseases, in which a human individual transmits a pathogen to another individual, without an intermediate vector or reservoir, has led to the development of a variety of detection and surveillance strategies. With the notable exception of zoonotic spillover events, each infection can be attributed to another, older, infection. This basic insight lead to the remarkable development of contact tracing as a core method to efficiently identify closely related infections resulting in significant contributions to public health. Indeed, while contact tracing has been successfully used to trace many infectious diseases, including the recent SARS-CoV-2 epidemic, and been evaluated in mathematical models, there remains little knowledge on how contact tracing may impact and interact with genetic sequence data analyses. Due to the non-random nature of contact tracing, one might expect that the phylogenetic tree structure of the spreading pathogen could be impacted by such sampling. That raises fundamental questions about the nature of the data that typically is used for phylogenetic and phylodynamic reconstruction of pathogen epidemics: how robust are mathematical assumptions made about the collection of data in practice, and how significant are deviations from these assumptions in real data? While the contact network that pathogens spread across can be informative of the pathogen’s phylogeny, it remains largely unknown how sampling with contact tracing impacts the observable phylogeny. A standard form of contact tracing is “iterative contact tracing”, in which an initial index case is interviewed to identify contacts which may be infected. Identified contacts are tested, and the interview process is repeated for contacts with positive test status. From a statistical perspective, contact tracing provides a correlation structure to the detection process informed by the transmission network structure. If an individual tests positive for HIV, their contacts have a better than uniformly random probability of being sampled. Importantly, first degree contacts are expected to be evolutionary closer to an index case than could be expected from a random sample of the larger population, hence displaying smaller genetic distances. This could consequently impact both distributions of pairwise distances and phylogenetic tree structures in complex ways. A motivating example to consider is the spread of HIV-1 circulating recombinant form 1 (CRF01) in Europe in the late 1990’s and 2000’s (Figure 1). HIV-1 CRF01 was originally introduced in southeast Asia from Africa, and later spread from there to other parts of the world, including several countries in Europe, on many occasions and still ongoing today. Thus, the available HIV-1 CRF01 sequences from Europe cannot be strongly influenced by contact tracing, as they are not closely related within Europe nor have there been cross-border coordinated sampling efforts. In contrast, a Swedish HIV-1 CRF01 outbreak among injection drug users in 2007–2008 elicited a strong public health response resulting in identifying further persons who had been in contact with those infected with this HIV-1 variant, generating many closely related sequences. Hence, part of the resulting European HIV-1 CRF01 phylogeny comes from strong contact tracing while the larger part comes from essentially random sampling. The two parts of the European HIV-1 CRF01 tree highlights the strong impact contact tracing may have on the tree structure, affecting both topological and branch length statistics. Several simulation techniques have been proposed for generating detailed pathogen phylogenies such as FAVITES, BEAST2, or PopART IBM. These software tools facilitate simulations of individual, contact network, and population-level models and thus are able to generate phylogenies with a broad variety of features to investigate potential epidemiological assumptions. However, none include contact tracing or similar sampling methods. FAVITES comes close in offering a sampling method weighted by the number of transmission events, but this method results in sampling towards more interconnected individuals, rather than following local structures. As a result, these simulation tools are not suitable for investigating the impact of contact tracing on phylogenies. Since contact tracing is very common practice, and may have strong impact on phylogenetic structures (Figure 1), a new simulation software capable of emulating contact tracing is needed to formally include its impact on pathogen phylogenies. Here, we explore the impact of contact tracing on phylogenetic tree structure. To directly address questions about the significance of contact tracing, we developed a new simulation suite in R called SEEPS (Sequence Evolution and Epidemiological Process Simulator) that allows for the simulation of contact tracing, the resulting transmission trees, and pathogen phylogenies. By using an agent-based model, trees can be directly simulated, only specifying simple rules and behaviors. Using SEEPS, we show that both topological and distance based tree measures are sensitive to the presence of contact tracing, demonstrating that contact tracing can significantly impact the structure of the resulting tree. We show that SEEPS can simulate the Swedish HIV-1 CRF01 outbreak and the corresponding broader EU HIV-1 CRF01 epidemics presented in Figure 1, and give a coarse estimate of the performance of contact tracing present in these data. In agreement with known epidemiological data, we find that the Swedish outbreak is strongly influenced by contact tracing, while the broader EU epidemic shows little evidence of contact tracing. New Approaches Simple models for contact tracing and transmission dynamics can be directly implemented in a computational environment using an agent based simulation. Building off the agent-based HIV-1 model in, we developed SEEPS (Sequence Evolution and Epidemiological Process Simulator), an end-to-end modern and modular simulator for investigating the connection between evolutionary and epidemiological mechanisms. Written in R, SEEPS is a flexible and extensible framework for simulating phylodynamic and evolutionary processes at a population level. SEEPS stores the entire transmission history, allowing for models of contact tracing to be run on top of the transmission history and directly compared. Individuals in SEEPS are considered active if they are capable of generating secondary infections. SEEPS offers both high level and low level modeling tools, enabling both coarse and fine-grained mechanisms to be modeled. An experiment in SEEPS begins by simulating a population of infections with user-defined expected offspring generation rates. The population is sampled at user-defined time points, with sampled individuals being removed from the simulation. Once the transmission tree is sampled, SEEPS offers a module for simulating the within-host diversity using a coalescent process from. By modeling each infected individual as a host where further viral evolution can occur and offspring are sampled from, SEEPS can explicitly convert a transmission history into a possible phylogeny by taking the within-host evolutionary diversification into account, often resulting in reordering the host transmission history tree into a pathogen phylogeny. Both genomic sequences and phylogenetic trees are often used to test analysis pipelines. Sequence simulation is available with a GTR+I+Γ model using Seq-Gen and the PhyClust R package. SEEPS can export trees in Newick format for use in other standard phylogenetic analysis software, such as the R package ape. Distance matrix representations of the data are also available for export, either using cophenetic distances for trees, or pairwise evolutionary distances (such as TN93) for sequences. A general schematic of simple workflows available in SEEPS is shown in Fig 2. To study the impact of contact tracing, we implemented a simple algorithm to describe contact tracing in SEEPS. Our model captures the fundamental aspects of iterative contact tracing where each positive contact is discovered with probability 0 ≤ p ≤ 1, and is similar to the popular breadth-first-search algorithm, but with the variation that the discovery of edges is randomized with a prescribed failure probability 1 − p. Using this model, we can generate complex trees reflecting a wide variety of scenarios. In Fig. 3AB, we show two example phylogenies generated by SEEPS. Crucially, SEEPS tracks the entire transmission history of the sampled taxa because it is needed for the proper modeling of the resulting phylogeny, as it informs all intermediate transmission bottlenecks that impact the diversification of the sampled viruses. In Fig 3CD, we removed the unsampled taxa from the trees, trimmed the resulting internal branches, and collapsed any resulting internal nodes of degree 2. The trees in Fig 3AC were generated with a high contact tracing discovery probability, while the trees in Fig 3BD were generated with a low contact tracing discovery probability. Both scenarios were generated in two stages: First, we simulated an outbreak as exponential growth, and second, a constant population size. Sampled individuals were removed from the active population to reflect that they were either on efficient antiviral treatment or otherwise non-infectious after diagnosis. The trees are visually distinct, with high contact tracing resulting in large clusters being identified which are not closely related to each other. In contrast, low contact tracing identifies small clusters that are loosely related. Thus, varying the sampling method to compare contact tracing against random sampling (which is typically assumed in phylodynamic inferences) may give very different impressions of what appeared to have happened.","Simulations in SEEPS We developed SEEPS to perform generative computational experiments on HIV-1 phylodynamics, generalizing the model and framework from. All experiments in the present study were performed in SEEPS v0.2.0 available at github.com/molEvolEpid/SEEPS. Code associated with specific analyses is available at github.com/molEvolEpid/ContactTracingForPhylogenies. SEEPS provides a stochastic forward simulation that tracks the transmission history of the entire simulation (including non-sampled individuals) and maintains a list of active individuals that are capable of generating new offspring. We implemented an agent-based discrete time model of transmission dynamics which randomized the length of each infection between one and three years, with the expected number of lifetime transmissions being equal to R0. Here, we used a simple biphasic rate for offspring generations where the initial transmission rate was 20-fold higher in the first three months of infection. The simulation time step was 1 month in duration. When multiple sampling time points were used, the simulation stopped at each, samples were taken and removed from the population, and then the simulation resumed. After prescribing the population dynamics (here, exponential growth or constant size), samples were taken at fixed time points through contact tracing and the transmission history for the subset of sampled individuals was reconstructed. We reconstructed both 1) a reduced transmission history, where unsampled tips were removed and any internal nodes of order two were collapsed, and 2) a complete transmission history for the samples where only pruned branches were removed. The reduced transmission history was converted into a transmission tree. The complete transmission history for the sampled individuals was used to obtain a phylogeny by simulating a (neutral) coalescent process along the transmission history. Within-host diversity is modeled assuming a = 5 and b = 5 as in. This process reconstructs a possible pathogen phylogeny within the transmission tree and forces coalescence events to occur before time 0. Individuals in the transmission history that are not sampled are simulated where needed to ensure that the entire evolutionary history of the samples is correct with respect to transmission bottlenecks and diversification level until the next transmission event. This process can introduce additional tips into the phylogeny that do not correspond to sampled individuals. These were removed in our main analyses, but are available within SEEPS for inspection (as shown in Fig 3). Both the transmission tree and sampled phylogeny were exported to the R package ape. Contact tracing To study the impact of contact tracing, we developed a simple algorithm to describe contact tracing as follows: An initial index case is randomly discovered in the population, and all first order contacts (secondary infections and the source itself) are identified by examining the transmission history. Then, each identified contact is independently discovered at probability p ∈ [0, 1]. Hence, the parameter p denotes the contact tracing discovery probability of each contact. If p = 0, there is no contact tracing, while p = 1 corresponds to perfect contact tracing. The discovered individuals are added to a list of discovered individuals, and the identify-and-probabilistically-discover process is repeated for each newly discovered individual. This is repeated until there are no discovered cases to trace, or until a maximum number of active individuals have been identified. If the desired number of individuals are not sampled, the process is re-started with a new random index case and repeated until the desired number of individuals have been sampled. Tree statistics As phylogenetic trees are complex objects, there are many statistics, indexes, and measures that have been proposed for analyzing trees. In this work, we focused on some of the most popular statistics that have been widely used to analyze phylogenetic trees and that we expect to be influenced by contact tracing, including both topological and branch length effects. We considered Sackin’s index using the R package treebalance to assess topological effects, and the internal/external (I/E) branch length ratio to assess branch length effects. Sackin’s index measures the imbalance of a tree. It is maximized in a caterpillar (ladder-like) topology, while being minimized in a fully balanced tree. In the absence of contact tracing, i.e., with uniform random sampling, we expect the Sackin’s index to be low, because then the topology would be informed by random ancestral relationships during the initial exponential growth phase. In the presence of contact tracing, we expect to primarily recover recent information about the transmission history related to epidemiologically significant clusters. While such clusters also are linked together by ancestral relationships, if the contact tracing is good, we expect the majority of the tree to reflect recent transmission events. We used parsimony to assess how representative a sample of taxa is of a larger epidemic. We did this by sampling twice in an epidemic and compared how representative the second sample was of the phylogeny obtained from the first sample. The taxa of the first sample were labeled “A” and the taxa of the second sample “B”. Given the phylogeny of both “A” and “B” labeled taxa, we calculated the number of A → B transitions, i.e., the parsimony score of label transitions. We computed the parsimony score using the R package phangorn, and the process was repeated 200 times for each combination of R0, time point, sample size, and contact tracing discovery probability p. If uniform random sampling was performed, we expect the parsimony score to be high, because the taxa are drawn from the entire epidemic, i.e., the entire phylogeny, independently or nearly independently. If contact tracing was performed, we expect each group of taxa to contain more cluster-like relationships, which would be more informative about local spread but not the entire epidemic. Thus, because more taxa are closely related when contact tracing has occurred, fewer “A” → “B” transitions are required, so the resulting tree would have a lower parsimony score. The I/E ratio is informative of the recent evolutionary relationship between taxa as well as the overall tree structure. In the absence of contact tracing, we expect the ratio to be low, as many external branches will connect the taxa back to an ancestral event in the outbreak phase. In contrast, if there is contact tracing, we expect the ratio to be high as the most recent common ancestor between two taxa in an identified cluster will be much more recent. HIV-1 CRF01 European sequence data Data from the European HIV-1 CRF01 epidemic was extracted from the LANL HIV database (hiv.lanl.gov). GenBank accession numbers for all sequences used in this study are available at github.com/molEvolEpid/ContactTracingForPhylogenies. The data consisted of 34 env V3 region sequences (approx. 300 nt) from an intravenous drug user (IDU) outbreak in Stockholm, Sweden, in 2006–2007 and 155 corresponding European sequences from 2003–2007 (including 23 additional Swedish sequences not involved in the IDU outbreak and 132 sequences from 12 other countries). The entire European HIV-1 CRF01 tree was reconstructed using PhyML v3 under a GTR+I+G model by both NNI and SPR search. Using SEEPS, we simulated 110,000 and 3,520,000 sequences for the EU and Swedish outbreaks respectively with varying levels of contact tracing. Sampling dates were selected by sampling the distribution of sample years that the true sequences were taken from. We then computed the tree statistics for each simulated outbreak. To compare the simulated sample distributions against the bootstrap statistic distributions computed from the real data, we used the two-sample Kolmogorov-Smirnov test and the relative difference of means. As these simulations were large, we refrained from reporting a p-value, but instead reported the test statistic as evidence for how close the simulated distribution were to the real data as the contact tracing parameter was varied. For both test statistics, a value closer to zero implies that the simulated distribution was closer to the real data.","Contact tracing makes trees less balanced In a first set of simulation experiments, we consider the impact of contact tracing on Sackin’s index for a collection of taxa taken at a single time point, known as cross-sectional sampling. We simulated 1,000 outbreaks followed by a constant population size for 0 to 10 years (in one year increments), with R0 uniformly distributed between 1.5 and 5. For each outbreak, we sampled either 15 or 50 taxa, with contact tracing performed at either high (p = 0.9) or low (p = 0.1) levels. In total, we generated 22,000 transmission trees and 22,000 phylogenies. We found no clear correlation between Sackin’s index and R0 and no effect of the number of years after the outbreak phase in which a cross-sectional sample was taken. The simulation of the transmission history resulted in an average Sackin’s index close to what could be predicted from a Yule model when contact tracing performance was low (Fig 4). Conversely, when contact tracing was high (p = 0.9), the Sackin’s index became elevated above the Yule expectation. Further, adding the within-host diversification process (phylogeny) increased Sackin’s index only slightly for both low and high levels of contact tracing. In all configurations, the sampled trees include a Sackin’s index close to the minimal possible value for the number of sampled taxa (15 or 50). Trees based on contact tracing in a small sample do not represent the larger epidemic A potential concern with data generated with contact tracing is that it may result in missing, unconnected, or undiagnosed persons, thus making the sample unrepresentative of the larger population. Since we now know that contact tracing biases trees to be more unbalanced, this raises concern about how representative a phylogeny would be of the greater epidemic. Thus, we assessed how representative a second, later in time, sample would be of an earlier sample from the same epidemic. We simulated outbreaks under varying R0 in SEEPS to an effective population size of 1,000 infections and sampled 50 active infections as soon as the effective population surpassed 900 active infections. We let the population replace the removed infections while simulating forward for 3, 24, or 120 months. We then drew another 50 taxa, for a total of 100 sampled taxa. The growth rate parameter R0 took discrete values of 1.1, 1.5, 3, 5, or 10. We used a parsimony score to report the number of “transitions” that were required to render the first sample labels into the second sample labels. Thus, this parsimony score represents how much of the original tree structure the second sample recovers; a smaller score would indicate a different tree while a higher score a more similar tree. We found a strong relationship between the mean parsimony score and both contact tracing and the length of the inter-sampling period (Fig 5 A–E). Increased contact tracing decreased the parsimony score, indicating that the two samples represented different parts of the total epidemic. R0 only weakly influenced the relationship between parsimony scores and contact tracing performance; R0 primarily impacted the parsimony score when contact tracing performance was high by increasing the variance. As Fig 5E demonstrates, setting R0 = 10 indicated that the variance increased after approximately p = 0.5, while in Fig 5A it occurred close to p = 1 when R0 was close to 1. In Fig 5A–E, the symmetric inner 50% is shown as a shaded region to capture the variance. Fig S2 shows additional bands to provide a more complete picture of this effect. We next investigated whether the sample size (number of taxa investigated) at 3 months after the first sampling time influenced the parsimony score. In Fig 5F–J, R0 was varied as before, but we sampled between 5 and 250 taxa at each time point (from a target population size of 1,000). The normalized parsimony score shows little dependence on the sample size, as long as a “small sample” approximation remains reasonable. With as few as five taxa, however, the lowering effect of contact tracing on the parsimony score is diminished because the small tree size limits its range. Mean internal to external branch length ratio is affected by contact tracing The mean internal to external (I/E) branch length ratio quantifies the recent evolutionary relationship between sampled taxa. The external branches are the tips of a tree, ending with the sampled taxa, and the internal branches connect all nodes of the reconstructed ancestors of the sampled taxa. Hence, if the ratio is small, then the tips are longer, suggesting that the taxa are not recently related. If the ratio is large, then the samples are recently more related, suggesting the possibility of an epidemiologically significant cluster. Previous work suggests that the branch length ratio can be informative about possible recent outbreaks in a population, but the impact of contact tracing on the I/E ratio has not been evaluated. Using the same synthetic data set we examined for Sackin’s index, we computed the I/E branch length ratio for each phylogenetic tree (Figure 6). The full data used to generate this figure is available in the supplementary materials online Fig S2. While the R0 growth rate of the outbreak had some impact on the mean I/E branch length ratio, the presence or absence of contact tracing amplified the effect of when sampling occurred relative to the epidemic outbreak on the I/E branch length ratio. R0 was influential only when it was low; epidemics at R0 > 2.5 taken at the same time point had similar I/E branch length ratios, whereas the I/E branch length ratios increased when R0 was R0 < 2.5. Interestingly, the I/E branch length ratio was less sensitive to contact tracing immediately after the peak of the outbreak. At low contact tracing (p = 0.1), the I/E ratio was never able to rebound past the initial outbreak signal. In contrast, at high contact tracing (p = 0.9), the samples taken three years after the end of the exponential growth phase had similar I/E branch length ratios to the samples taken immediately after the peak of the outbreak. Thereafter, the I/E branch length ratio statistic continued to grow with time. This suggests that some amount of contact tracing early in an epidemic, enough to find a recent nearby infection, is necessary to recover a time signal from the internal branches and indicate the age of the outbreak. Contact tracing can be observed in real data Our simulations showed that contact tracing has strong effects on phylogenetic tree reconstructions, and therefore on any epidemiological inference that would be based on such trees. To tests whether we could recover the epidemiological data of our motivating example in the introduction, including the levels of contact tracing in the European and Swedish partitions, we attempted to use SEEPS to simulate the rather complicated European HIV-1 CRF01 epidemic. This epidemic can be divided into three parts: 1) the exponential outgrowth of a new form of HIV-1 in Thailand in the early 1990’s, 2) subsequent introductions of several genetically distant lineages into Europe, and 3) an introduction from Europe (Finland) into a previously uninfected injecting drug-user network in Sweden with an explosive outbreak. To estimate the level of contact tracing, we simulated epidemics in SEEPS similar to the European and Swedish outbreaks with varying levels of contact tracing and compared the I/E branch length ratios from our simulated trees to that of the real data (Figure 1). The parameter values used to generate the simulated data are shown in table S1. While we used different parameters for each outbreak, we used a similar two-phase simulation for both the European and Swedish partitions: In the first phase, we started the simulation with a single infected individual and allowed the population to grow to a small, fixed size. Once the population reached the fixed size, we let it continue at that size until the end of the phase. In the second phase, we increased the effective simulated population size to our target value, and allowed R0 to change. As before, the population was allowed to grow until the end of the phase. To mimic the import to Europe of genetically distant lineages from Thailand, we shifted the sampling time of the European sequences forward by 18 years to reflect the lack of available sequences along the long branches that constitute the introductions into Europe. Finally, we sampled 20 individuals with varying levels of contact tracing discovery probability according to the sample years of the EU and Swedish outbreaks, respectively. We then calculated the I/E branch length ratio for each simulated tree and compared the distribution against the real data distribution. To compare simulated and real I/E ratio distributions, we computed the relative difference of means  and the Kolmogorov-Smirnov test statistic D = supx∈[0,∞] |F (x) − G(x)| where F and G are the cumulative distribution functions for the simulated and the real data, respectively. For these two fitting statistics, we effectively randomized both the population size and R0 parameters, resulting in 10,000 samples to approximate the European epidemic and 320,000 samples to approximate the Swedish outbreak at each level of contact tracing discovery probability p. Both the KS statistic and the relative difference of means suggest that the European data was generated in a situation with negligible contact tracing, with an upper bound on the contact tracing discovery probability of at most 10% (Figure 7). A visual inspection of the phylogenetic tree (Fig 1A) suggests that the most recent common ancestor between most pairs of sequences in Europe is not in the recent history, or that the European sample is sparsely reported by the available sequences (from the LANL HIV database hiv.lanl.gov). This is consistent with multiple HIV-1 CRF01 introductions into Europe, each with limited local spread. In contrast, the subsampled Swedish data was consistent with a significant level of contact tracing. Both the KS statistic and relative difference of the means were indicative of a contact tracing discovery probability of approximately p = 0.6. We expected that the level of contact tracing would have been very high in this intensely followed outbreak. However, two effects may have lowered the estimated contact tracing level: 1) Not all infections in the outbreak may have been included in the sequencing, which would affect some I/E ratios, and 2) subsampling the Swedish outbreak phylogeny removed over half of the taxa in each draw, which further may have lowered the estimated contact tracing level. Taking both effects into consideration, our recovery of a contact tracing discovery probability of approximately p = 0.6 indicates that a strong effort of contact tracing took place in the discovery of the Swedish outbreak.","We developed an epidemiological and evolutionary simulation model that includes contact tracing, available as an R package called SEEPS. Using SEEPS, we showed that there can be a serious impact when pathogen sequences were collected by contact tracing on the resulting phylogenetic tree structure. Overall, contact tracing resulted in a phylogeny that 1) was more unbalanced, 2) was less representative of the larger epidemic, and 3) had its I/E branch length ratio differently impacted depending on when samples were taken relative to an outbreak. We then analyzed a real data set describing a known outbreak of HIV-1 CRF01 in Sweden, derived from Europe, which in turn had derived several lineages from Thailand. This showed that SEEPS was able to simulate a fairly complicated epidemiological scenario and, importantly, also correctly detected contact tracing as it was used in Sweden. Because sequence- and phylogeny-based approaches have the potential to reveal otherwise difficult to measure details about how pathogens spread, previous work has evaluated several tree statistics related to the branching structure and the branch lengths, such as Sackin’s index and internal to external (I/E) branch length ratios. For example, such statistics have been used to tune MCMC methods that explore the space of phylogenetic trees. Here, we showed that both of these classes of tree measurements are affected by contact tracing. Thus, assuming that sequences have been randomly collected, when in fact they were collected as the result from contact tracing may severely mislead analyses and conclusions from sequence- and phylogeny-based epidemiological inferences. One reason for that is that contact tracing results in samples that may be less representative of the larger epidemic. Contact tracing is a non-random sampling strategy that efficiently finds linked infections. While previously largely ignored in sequence- and phylogeny-based epidemiological inferences, non-uniform sampling strategies such as contact tracing have been investigated in other contexts, such as generalized birth and death models and ordinary differential equation compartment models for outbreaks. In, it was shown that characteristic rates for the branching process can be incorrectly inferred if an incorrect sampling scheme is assumed, which agrees with our results showing that contact tracing impacts phylogenetic tree structures. In it was shown that the significance of contact tracing as a control mechanism is sensitive to the degree of superspreading. However, those results were based on the assumption of independence and the mechanism of spread through mass action, which breaks down when using sequence- and phylogeny-based epidemiological inferences. While contact tracing may lead to a phylogeny that does not represent the larger epidemic - from a public health perspective, detecting superspreaders is important because they contribute more to overall disease spread. Contact tracing will be much more likely to find superspreaders than random sampling simply because they are more likely to be traced from any one of the people they infected. SEEPS includes within-host evolution that simulates diversification under a neutral coalescent process. The within-host pathogen diversification is important to account for because it affects the observable phylogeny, which always is different from the non-observable transmission history. However, SEEPS does not simulate the selection of escape mutants driven by the host immune system. Because selection also can cause an imbalance in the tree structure, our simulations may be on the conservative side of the impact contact tracing has on the global tree structure of an epidemic. Furthermore, if superspreaders were active, the simulations under our neutral model may show less impact of superspreading than in real epidemics. Methods that depend on analyzing distances such as HIV-TRACE or machine learning based methods such as convolutional neural network (CNN) models are inherently sensitive to the distribution of pairwise distances. Contact tracing results in samples that can be significantly closer than random. If clusters are interpreted as outbreaks, then clusters discovered by non-uniform sampling may be correctly labeled as transmission clusters, but erroneously inferred as signs of a larger outbreak. Thus, the performance of HIV-TRACE and the CNN model in detecting outbreaks may be sensitive to how samples were collected. Hence, popular analytical methods and computational tools used to trace and reconstruct epidemics need to ensure that the impact of contact tracing is not being overlooked or misinterpreted.",10.1101/2023.11.30.567148
PMC10462030,37645934,Fixational Eye Movements Enhance the Precision of Visual Information Transmitted by the Primate Retina,"Summary Fixational eye movements alter the number and timing of spikes transmitted from the retina to the brain, but whether these changes enhance or degrade the visual signal is unclear. To quantify this, we developed a Bayesian method for reconstructing natural images from the recorded spikes of hundreds of macaque retinal ganglion cells (RGCs) of the major cell types, combining a likelihood model for RGC light responses with the natural image prior implicitly embedded in an artificial neural network optimized for denoising. The method matched or surpassed the performance of previous reconstruction algorithms, and provided an interpretable framework for characterizing the retinal signal. Reconstructions were improved with artificial stimulus jitter that emulated fixational eye movements, even when the jitter trajectory was inferred from retinal spikes. Reconstructions were degraded by small artificial perturbations of spike times, revealing more precise temporal encoding than suggested by previous studies. Finally, reconstructions were substantially degraded when derived from a model that ignored cell-to-cell interactions, indicating the importance of stimulus-evoked correlations. Thus, fixational eye movements enhance the precision of the retinal representation.","Vision begins with the retina, which transforms incoming light into electrical signals, processes these signals, and transmits them to the brain in the spiking activity of retinal ganglion cells (RGCs). This encoding process has been studied for nearly a century, with contemporary models capturing the details of RGC responses with a high degree of precision. But quantifying coding precision does not directly reveal how effectively the visual scene is conveyed by RGCs to the brain, nor how that effectiveness depends on spike timing and cell-to-cell correlations. Nor does it elucidate the degree to which the RGC code is specialized for the stimulus conditions that the visual system evolved to analyze: naturally-occurring patterns of light, with global image shifts arising from eye movements. To probe the retinal code under these conditions, we develop and apply a novel method for reconstructing natural images and movies from the spiking activity of complete populations of RGCs recorded in the primate retina. Rather than fitting a model to directly map recorded RGC spikes to images [; Kim 2020, ], we use a Bayesian formalism – combining a likelihood obtained from the retinal spikes with separately-acquired prior information about the statistical structure of natural images. Specifically, images are reconstructed by numerical optimization of the posterior density, arising from the product of (1) an image likelihood obtained from an encoding model fitted to RGC data that captures the stochastic responses of RGCs to visual stimuli, and (2) a natural image prior implicit in an artificial neural network pre-trained on a natural image database to perform Gaussian denoising. This approach confers unique advantages for analysis and interpretation of the retinal signals. We demonstrate that the method achieves state-of-the-art reconstruction performance, and then use it to demonstrate for the first time the importance of fixational eye movements, spike timing precision, and cell-to-cell correlations in the retinal code for natural visual stimuli.","Multi-electrode array recordings Preparation and recording methods are described in detail elsewhere [, Field 2010, ]. In brief, eyes were enucleated from terminally anesthetized macaque monkeys used by other laboratories, in accordance with Institutional Animal Care and Use Committee requirements. Segments of RPE-attached retina approximately 3 mm in diameter were cut from the eye in dim light, and placed RGC side down on a multi-electrode array (MEA). A 512-channel MEA system] with 60 μm pitch between electrodes and a 2×1 mm rectangular recording area was used to perform the recordings. This system band-passed and digitized the raw recorded voltage traces at 20 kHz. The preparations were perfused with Ames’ solution (30–34 C, pH 7.4) bubbled with 95% O2, 5% CO2 throughout the recordings. Spike sorting was performed with YASS. RGCs of the four numerically dominant types in macaque (ON parasol, OFF parasol, ON midget, OFF midget) were identified manually based on receptive fields and autocorrelation functions characterized with a spatio-temporal white noise stimulus according to previously described procedures [Rhoades 2019], and were matched to spike-sorted units from the natural scenes recordings by matching electrical images (voltage templates). Only identified RGCs of the four major cell types were used in the analysis. The four preparations used for the flashed reconstructions contained 691, 592, 704, and 677 total cells, and the three preparations used for the jitter eye movements reconstructions contained 715, 604, and 775 total cells. Visual stimulus The visual stimulus was presented on a 120 Hz, gamma-corrected CRT monitor (Sony Trinitron Multiscan E100), and was optically reduced and projected onto the retina. All experiments occurred at low photopic light levels (2000, 1800, and 800 isomerizations per second for the L, M, and S cones respectively at 50% illumination; see [Field 2009, Field 2010]). The visual stimulus covered an area of 3.5×1.75 mm, extending well beyond the recording area of the MEA. A 30-minute spatio-temporal white noise stimulus was used to identify RGCs of the major cell types and to characterize the locations of their receptive fields. The stimulus consisted of a grid of pixels (either 44 or 88 μm in size), whose intensities were drawn independently and randomly from a binary distribution. The stimulus was refreshed at either 30 or 60 Hz. Flashed natural images from the ImageNet database were presented to the retina according to. Images were converted to grayscale, cropped to 256×160 resolution, and padded with gray borders. The stimulus extended beyond the boundaries of retinal preparation and fully covered all receptive fields. Each pixel in the image measured approximately 11 × 11 μm when projected on the retina. Each image was displayed for 100 ms (12 frames at 120 Hz), and sequential images were separated by a 400 ms uniform gray screen. The natural movies with simulated fixational eye movements consisted of ImageNet images presented for 500 ms each (60 frames at 120 Hz), with no gray screen separation. For each image, eye movements were simulated by shifting the image during each frame transition according to a discretized 2D Brownian motion with diffusion constant of 10 μm2/frame, consistent with estimates of fixational eye movements in both human and non-human primate [Z.M. Hafed and R.J. Krauzlis, personal communication, June 2008]. Simulated eye movements were drawn independently of the image. The movies were presented in sequence, with no gray screen between movies. The receptive fields of the recorded RGCs covered only a central region of the stimulus field, leaving a perimeter region for which no cells were recorded. To evaluate image quality only over regions of the stimulus corresponding to recorded cells, a valid region was constructed, consisting of the convex hull of the receptive fields of the full RGC population. Only pixels in this valid region were used to compute image quality. Fitting LNBRC models of RGC spiking The linear-nonlinear-Bernoulli with recursive coupling (LNBRC) is a modified form of the model presented in. It generalizes the classical linear-nonlinear-Poisson (LNP) spiking model by incorporating recursive feedback (spike history) and neighboring cell coupling filters to model spike train temporal structure and cell-to-cell correlations (Fig 1b). For RGC i, the LNBRC has the following parameters: (1) mi, the linear spatio-temporal stimulus filter; (2) fi[t], the recursive feedback filter; (3) ci(j)[t], the coupling filters to neighboring RGCs indexed by j. where neighboring cells were included if their receptive field centers fell within twice the median nearest neighbor distance for parasol cells and 2.5 times the median nearest neighbor distance for midget cells; and (4) bi, an additive bias. Let v[t] denote a temporal window of the visual stimulus movie up to and including time t, * a time-domain convolution, and si the spike train of cell i. The instantaneous spiking probability for cell i is computed from the generator signal, gi [t]:  All filters in the LNBRCs were strictly causal, so that the firing probability at time t depended only on the visual stimulus and observed spikes occurring strictly before time t. Time was discretized in 1 ms bins, corresponding approximately to the duration of the refractory period of a neuron. Since at most one spike could occur in each time bin, a Bernoulli random process was used to model spiking, with a sigmoidal nonlinearity of the form  mapping the generator signal to an instantaneous firing probability, resulting in the encoding negative log-likelihood  which is jointly convex in the model parameters. The stimulus filter was assumed to be space-time separable (rank 1), and the stimulus filter spatial component was additionally cropped to a rectangular region surrounding the cell’s receptive field and represented in terms of a 2D cubic spline basis. The feedback, coupling, and time component of the stimulus filter were each parameterized as linear combinations of low-rank 1D raised cosine basis functions. The models were fitted to recorded RGC spikes by maximizing the parameter likelihood, and were regularized with an L1 penalty to induce sparsity on the spatial component of the stimulus filter, and an L2,1 group-sparsity penalty on the cosine basis representation of the coupling filters to eliminate spurious cell-to-cell correlations. Because of the assumed space-time separability of the stimulus filter, the LNBRCs were fitted using coordinate descent, alternating between solving a spatial convex minimization problem in terms of the stimulus spatial filter, feedback filter, coupling filters, and bias, and solving a temporal convex minimization problem in terms of the stimulus time course filter, feedback filter, coupling filters, and bias. All optimization problems were solved using FISTA, an accelerated proximal gradient method, using the formulation for the L2,1-regularized problem presented in. Optimal values for the weights placed on the L1 and L2,1 regularizers were found using a grid search to minimize the mean test negative log-likelihood over four randomly chosen cells of each cell type. Within each preparation, every RGC of a given type used the same hyperparameters. The LNBRCs were fitted separately for each cell, and required about 180 seconds of compute time per cell for the static stimulus models and 500 seconds of compute time per cell for the eye movements models on a single NVIDIA V100 GPU with 32 GB of VRAM. LNBRC simulated spike train generation Simulated spike trains for evaluating model fit quality were generated from the LNBRC by computing the value of the generator signal from the stimulus and using simulated Bernoulli random variables to model random spike generation. The recursive feedback contribution to the generator signal was initialized using real observed spike trains, and subsequent generated spike trains were fed back into the model to compute the feedback contribution for future spikes. Because the firing probability computed with the coupled LNBRC was conditional not only on the visual stimulus and simulated cell spiking history, but also on the spike trains of nearby coupled RGCs, real spike trains from the experimental data were used to compute the coupling contribution to the generator signal. PSTH computation The peri-stimulus time histogram (PSTH) was computed using RGC responses to repeat presentations of the same visual stimulus, by binning the observed spikes into time bins with 1 ms width, smoothing with a Gaussian kernel with standard deviation of 2 ms, and then computing the mean over all repeated presentations of the stimulus. Fitting benchmark LNP encoding models Benchmark linear-nonlinear-Poisson (LNP) encoding models were fitted in a similar manner to the LNBRCs. The same spatial basis sets used for the LNBRCs were used for the LNP models. Spikes trains were binned into counts with 8.33 ms time bins, corresponding to one bin per stimulus frame. LNP models were parameterized by a spatio-temporal stimulus filter mi, and a bias bi, resulting in a generator signal of the form  An exponential nonlinearity was used, resulting in a encoding negative log-likelihood with form  which is convex in the LNP model parameters. LNP spatio-temporal filters were assumed to be rank-1 space-time separable. An L1 penalty was used to induce sparsity in the spatial component of the stimulus filter, and the corresponding weight for that penalty was chosen by performing a grid search with encoding likelihood on the test partition as the objective. Models for each cell were fitted using FISTA. Reconstruction of flashed images with denoising CNN prior An iterative Plug-and-Play algorithm was used to perform MAP reconstruction of flashed static natural images. Rather than solve the MAP problem directly, the algorithm used proximal variable splitting to divide the MAP objective  into an encoding sub-problem  and a prior sub-problem  and iteratively alternated between the two. The encoding sub-problem was solved using unconstrained convex minimization. The prior sub-problem has the form of a MAP estimation problem for images contaminated with additive Gaussian noise. As such, its solution was approximated using a single forward pass of a convolutional neural network (CNN) pretrained for denoising with specified noise variance  Ten iterations of alternating optimization were used. ρ(k) was increased per iteration on a log-spaced schedule, and hyperparameters λ, ρ(1) and ρ(10) were found by performing a grid search on an 80-image subset of the test partition with reconstruction MS-SSIM as the objective. A detailed description of the algorithm can be found in [Wu 2023]. Exact MAP reconstruction with 1/F Gaussian prior Using the 1/F Gaussian prior, the MAP objective had the form  where ak(Y) is the amplitude of the Fourier coefficient at frequency fK. Because both the 1/F prior term and the encoding negative log-likelihood are smooth and convex in the image, the MAP problem is an unconstrained convex minimization problem and hence was solved with gradient descent. The optimal value of the prior weight λ was found with a grid search with reconstruction MS-SSIM as the objective. Approximate MAP reconstruction with known eye movements with denoising CNN prior In the case that the eye movements w are known a priori, the MAP objective can be simplified into the form  which can be solved using the Plug-and-Play algorithm described above for the flashed case. Hyperparameters were found with a grid search with MS-SSIM as the objective. Joint estimation of image and unknown eye movements with denoising CNN prior The expectation-maximization (EM) algorithm was used to perform MAP estimation for joint estimation of images and eye movements. Letting w denote the eye movement trajectory over all timesteps, the exact MAP problem with unknown eye movements has form  which cannot be directly solved because the marginalization over all possible eye movement trajectories w is intractable. MAP-EM offers an iterative approach for estimating the image Y, and consists of alternating steps of: (1) finding the image that maximizes the sum of the evidence lower bound and natural image log prior  over some variational distribution of the eye positions ; and (2) using the resulting estimate of the image Y(i) to update the variational distribution. For computational tractability, we assumed q had form  where r could be an arbitrarily chosen distribution. q was represented approximately using a weighted particle filter with N=10 particles. The particle filter was updated once for each frame transition (every 8.33 ms) using a sequential importance resampling procedure. Specifically, at frame t, the trajectory represented by each particle was updated by sampling a new eye position from the 2D Gaussian transition probability distribution , and then reweighting each particle using the multiplicative weight  computed using the encoding likelihood model. Mathematical details for the resampling particle filter, including justification for the weight update rule, are provided in the Supplement. An initial guess for the image y(0) was reconstructed by assuming fixed eye position at the origin and performing ten alternating iterations of the algorithm used for the flashed reconstructions. At each intermediate timestep i updated estimates of the image y(i) were computed by performing a single encoding optimization step  using unconstrained convex minimization, followed by a single prior optimization step  using a single forward pass of the Gaussian denoiser. To speed computation, images were updated once for every five display frame transitions. Testing on a subset of data indicated that this did not negatively affect reconstruction quality. Reconstruction quality evaluation Reconstruction quality was evaluated using Multi-scale Structural Similarity (MS-SSIM), a widely used metric for perceptual similarity. MS-SSIM was calculated over the valid region of the image (described above), ignoring non-informative regions of the stimulus for which no RGCs were recorded. For the jittered reconstructions, the absolute position of the reconstructed image was arbitrary (having been jointly estimated from many jittered input samples), and MS-SSIM was computed for a range of pixel-wise shifts of the reconstructed image, and the best value over all shifts was used. The results in the paper were also confirmed using the Learned Perceptual Image Patch Similarity (LPIPS), an alternative measure of perceptual distance computed using pre-trained neural network classifiers. LPIPS has different working principles than MS-SSIM, and has been shown to align with human perceptual judgements. Only pixels within the valid region (described above) were used to compute LPIPS. Cross-validation data rotation for eye movements analysis Five-fold data rotation was used to maximize the number of stimulus images available for determining the effect of jitter eye movements on reconstructed image quality. Five different sets of LNBRCs were fitted, each corresponding to distinct and non-overlapping test and held out partitions, such that test-quality reconstructions could be produced for nearly every stimulus image presentation in the recorded dataset. Cell-type-specific reconstruction analysis The cell-type-specific analysis was performed by reconstructing the jittered eye movements stimulus using joint-LNBRC-dCNN. For simplicity, the LNBRC models used for this analysis only modeled homotypic correlations, differing from the models used elsewhere in the work. Five-fold data rotation was used for this analysis. Spike time perturbation analysis The spike time perturbation analysis tested the temporal precision of the retinal code by shifting recorded spike times by random amounts drawn from a zero-mean Gaussian, with standard deviations of 1 ms, 2 ms, 5 ms, 10 ms, 20 ms, and 40 ms. To ensure optimal reconstruction at each level of perturbation, the LNBRCs were refitted to each condition. Images were reconstructed using the time-perturbed data and the time-perturbed LNBRCs using the algorithms described above. Optimal hyperparameters were found separately for each time perturbation condition by performing grid searches. Uncoupled (LNBR) model correlations analysis The LNBR (uncoupled) model removes the neighboring cell coupling filters of the LNBRC model, thus losing the ability to represent correlated firing between nearby RGCs. The LNBR is parameterized by a linear spatio-temporal stimulus filter, a recursive feedback filter, and a bias. Using the same notation as in the fully-coupled case, the generator signal for cell i in the uncoupled model is written as  The LNBRs were fitted with the same 1 ms time bins, sigmoidal nonlinearity, and Bernoulli random spiking model as the LNBRCs. Space-time separability of the stimulus filters was assumed, and the same alternating optimization procedure for fitting was used as in the LNBRC case. An L1 penalty was used to regularize the spatial component of the stimulus filters, and the optimal value of the corresponding hyperparameter was found using a grid search. Image reconstruction with LNBRs was done in an identical manner as with the LNBRCs. Reconstruction hyperparameters were found using a grid search. Noise correlations shuffled repeats analysis Noise correlations between RGCs were characterized using responses to repeated presentations of the same stimulus. Shuffled responses were constructed by randomly reordering recorded spike trains for each cell across the repeated trials, eliminating noise correlations while preserving single-cell spiking statistics and stimulus-induced correlations. Images were reconstructed for both the real (unshuffled) trials as well as the shuffled trials using LNBRCs fitted to the unshuffled data, using the reconstruction algorithms described above. The change in reconstructed image quality due to shuffling was then computed by taking the mean reconstruction quality across repeats of the same stimulus, and then subtracting the values computed for the shuffled repeats from the values computed for the data repeats. Cross-correlogram computation Cross-correlograms between cells were computed using repeat stimulus presentations by constructing histograms for the differences in spike times of the cells (with 1 ms bins), and taking the mean over all presentations of the same stimulus. Because the stimulus onset and offset frame transitions in the flashed stimuli and transitions between distinct images for the jittered eye movements stimuli induced simultaneous firing of all cells independent of connectivity and shared input structure, a shift predictor correction to the cross-correlograms was applied. This was done by shifting the spike times for the second cell such that the spike trains for that cell corresponded to the response to a different stimulus image, constructing the histogram for the differences in spike times for the cells, and then subtracting said histogram from the original raw cross-correlogram. This removed the component of the cross-correlogram that could be predicted by the trial structure alone, independent of either the spatial content of the stimulus or of noise correlations. Additional References for Methods","To characterize the visual signals evoked by natural images, we recorded light responses of RGCs in isolated macaque retina with a large-scale multi-electrode array. This method captured the activity of nearly complete populations of several hundred RGCs of the four numerically dominant types (ON midget, OFF midget, ON parasol, OFF parasol), which comprise roughly 70% of RGC axons projecting to the brain. Spatiotemporal white noise stimuli were used to identify cells and map their receptive fields [, Rhoades 2019]. Bayesian reconstruction of flashed images We first examined reconstruction of images presented in brief flashes to the retina. Although the dynamics of the flashed stimulus differ markedly from natural vision, the simplicity of the stimulus enabled evaluation of the image reconstruction approach and comparison to previous methods. Thousands of grayscale photographic images from the ImageNet database were presented, for a duration of 100 ms with consecutive trials separated by 400 ms of uniform gray screen (Fig. 1a, also see Methods). Flashed natural images were reconstructed from evoked RGC activity using a Bayesian approximate maximum a posteriori (MAP) algorithm (see). The posterior density (probability of an image given observed spikes) is the product of two separately defined and estimated components: (1) a likelihood model of the natural image stimulus y evoking the measured spiking response s, p( s ∣ y ), computed using a probabilistic encoding model of RGC spiking in response to natural image stimuli; (2) a prior model of natural images, p(y), obtained implicitly from a Gaussian-denoising neural network (Fig. 1c). The likelihood was computed from an encoding model that summed the effects of the visual input, spike history, and spike trains of nearby neurons (to capture spike train temporal structure and cell-to-cell correlations) and then transformed the output with an instantaneous sigmoidal nonlinearity to provide a firing probability for a Bernoulli spike generator (Fig. 1b). This model generalizes the commonly-used linear-nonlinear-Poisson (LNP) cascade model, replacing Poisson spiking with Bernoulli spiking (equivalent at fine time scales) and incorporating recursive feedback and coupling filters – we refer to it as the Linear-Nonlinear-Bernoulli with Recursive Coupling (LNBRC) model. Model parameters (stimulus, feedback, and coupling filters, and an additive constant) were jointly fitted to recorded RGC data by maximizing the likelihood of observed spikes given the stimulus, augmented with regularization terms to induce sparsity in the filter weights (see Methods). Separately, an implicit image prior was obtained by training a denoising convolutional neural network (dCNN) to remove additive Gaussian noise from a large collection of natural images. Such priors underlie the “diffusion models” that represent the current state-of-the-art in machine learning for image synthesis and inference [Kadkhodaie 2020, ]. With these two components, the reconstruction procedure maximized the posterior by alternating between an encoding likelihood optimization step (solved with unconstrained convex minimization) and a prior optimization step (solved with a single forward pass of the denoiser) (Fig. 1d,e, see Methods), yielding an estimate of the most probable image given the RGC spikes and natural image statistics. The performance of the MAP reconstruction algorithm was characterized qualitatively with visual image comparison and quantitatively with MS-SSIM, a commonly used measure of perceptual image quality. Example reconstructions are shown in Fig. 1f. Reconstruction performance was qualitatively and quantitatively more accurate than that obtained using linear reconstruction (mean MS-SSIM of 0.685, 0.652, 0.660, and 0.652 for LNBRC-dCNN MAP reconstructions in the four preparations tested, compared to 0.624, 0.616, 0.578, and 0.575 for linear reconstruction). Performance was comparable to state-of-the-art neural networks trained to nonlinearly recover the high spatial frequency components of images [Kim 2020] (mean MS-SSIM of 0.689, 0.683, 0.651, and 0.653, respectively). In addition to reconstruction quality, the MAP approach provided greater interpretability by separating the likelihood and prior components of estimation, and broader usability with limited retinal data (the retinal encoding models contained ~1.5 million parameters, in comparison with ~240 million parameters for the benchmark direct neural network method). To examine the importance of the encoding and prior models, MAP reconstruction performance with the full model (labeled LNBRC-dCNN) was compared to that achieved with a simpler spectral Gaussian image prior (LNBRC-1F) or with a likelihood corresponding to a simpler LNP encoding model (LNP-dCNN). Images reconstructed using the full approach had sharper and more detailed image structure (edges, contours, textures) than those reconstructed using the 1/F prior, and contained more fine spatial detail than those reconstructed using the LNP encoding model (Fig. 1f). Quantitatively, reconstructions produced using LNBRC-dCNN exhibited greater similarity to the original image than those produced with the simpler 1/F prior or the simpler LNP encoding model (mean MS-SSIM of 0.685, 0.652, 0.660, and 0.652 across preparations using LNBRC-dCNN, in comparison with 0.612, 0.573, 0.577, and 0.565 using LNBRC-1/F, and 0.635, 0.613, 0.597, and 0.603 using LNP-dCNN). Thus, both the dCNN image prior and the LNBRC encoding model contribute substantially to producing high-quality natural image reconstructions. Bayesian reconstruction of images displayed with fixational eye movements Fixational jitter (drift), the small but incessant eye movements that occur when fixating a visual target, is a fundamental component of natural vision in primates. These eye movements have been hypothesized to enhance visual resolution by sampling the image at many spatial phases relative to the lattice of RGC receptive fields, and/or by modulating high frequency spatial details into the temporal domain. However, psychophysical studies suggest that the visual system may not have precise knowledge of the eye position, opening the possibility that positional uncertainty could instead degrade the retinal signal (but see). Although simulation studies have explored the possibility of using the retinal signal alone to compensate for fixational eye movements, it remains uncertain whether unknown eye jitter enhances or degrades the retinal representation. We directly characterized the effects of jitter eye movements by reconstructing images from the experimentally-recorded responses of RGCs to jittered natural stimuli. We measured RGC responses to movies consisting of images from the ImageNet database, presented with randomly jittered spatial offsets in each frame to emulate fixational eye movements. Images were displayed for 500 ms, with each 8.33 ms frame spatially shifted relative to the previous frame according to a discretized sample from a 2D Gaussian distribution with a standard deviation of 10 μm (Fig 2ab), approximately matching the diffusion constant for fixational jitter eye movements in humans and macaques [Z.M. Hafed and R.J. Krauzlis, personal communication, June 2008]. The LNBRC model was fitted to RGC responses to jittered stimuli by maximizing likelihood. Model fit quality was assessed by comparing the model-simulated spikes with recorded data (Fig. 2c), and by computing the fraction of response variance explained by the model. Although some small systematic deviations from the data were observed (Fig. 2c), in general the LNBRC model effectively captured responses to natural stimuli with fixational eye movements (Supplemental Information Fig. S1). The fitted LNBRC was combined with the dCNN natural image prior for simultaneous estimation of the stimulus image and eye position using a modified approximate MAP procedure. To avoid marginalization over the eye movement trajectories, an expectation-maximization (EM) algorithm was used to alternate between reconstructing the intermediate image that maximized the expected log posterior over an estimated distribution of eye movement trajectories, and using that intermediate image to update the eye movement distribution (Fig. 2d, also Methods and Supplement). The effectiveness of this procedure (labeled joint-LNBRC-dCNN) at compensating for unknown eye movements was evaluated by comparing reconstruction quality to the case in which eye movements were known exactly (known-LNBRC-dCNN), and the case in which eye movements were incorrectly assumed to be zero (zero-LNBRC-dCNN). Reconstruction quality for joint-LNBRC-dCNN exceeded that of zero-LNBRC-dCNN (mean MS-SSIM of 0.677, 0.652, and 0.638 for each preparation for joint-LNBRC-dCNN, in comparison with 0.642, 0.617, and 0.615 for zero-LNBRC-dCNN) and approached that of known-LNBRC-dCNN (mean MS-SSIM of 0.685, 0.656, and 0.646 for the same respective preparations). Notably, this held true for nearly every image evaluated, for every preparation (Fig. 2f–g). Qualitative comparisons (Fig. 2e) revealed that the joint solution recovered substantially more image structure and fine spatial detail than the one that ignored eye movements, and produced reconstructions that were similar in content and quality to those produced with known eye movements. These results demonstrate that compensation for jitter eye movements is critical for recovering fine spatial detail in the visual scene, and that the RGC spikes alone are sufficient to perform this compensation. Fixational eye movements enhance the retinal visual signal To test whether jitter eye movements improve or degrade retinal coding of natural images, reconstruction quality was examined as a function of eye jitter magnitude. In all three preparations, when simultaneously estimating both the image and eye positions, the mean reconstructed image quality increased with the magnitude of jitter over nearly the entire naturalistic range tested (Fig. 3a, solid). The same was true when reconstructing with known eye positions (Fig. 3a, dashed), demonstrating that the improvement was due to an improved retinal signal. Validation with the LPIPS perceptual distance measure yielded similar results (Supplemental Information Fig. S4). Thus, fixational jitter eye movements enhance, rather than degrade, the retinal representation. The benefits of fixational jitter could in principle arise from an overall increase in spike rates, because RGCs are responsive to intensity changes over time, which are increased in the presence of jitter. Indeed, the mean number of spikes increased with increasing eye movement magnitude: Pearson correlation coefficients were 0.940, 0.988, and 0.884 for three experimental preparations. Thus, at least some of the improvement in reconstructed image quality could be attributed to increased RGC firing. Image reconstruction could also potentially be improved by more accurate estimation of the jitter trajectory with larger eye movements. This did not appear to be the case: the accuracy of eye trajectory reconstruction declined with increasing magnitude of eye position jitter, albeit much more slowly than for the model that assumed zero movement (Fig. 3b). Thus, the improved image reconstruction with increasing magnitude of eye movements was attributable to a more faithful encoding of the stimulus in RGC spikes rather than a more precise implicit signal about eye position. The potentially distinct impacts of fixational jitter eye movements on each of the parasol and midget RGC representations of the stimulus were examined by reconstructing with one population at a time. Midget-only reconstructions had systematically higher quality than parasol-only reconstructions and contained greater fine spatial detail (Fig. 3c, also Supplemental Information Fig. S5), demonstrating that midget cells encoded a greater fraction of the stimulus than parasol cells. Reconstruction quality improved with increasing jitter magnitude for both the parasol-only and midget-only reconstructions, demonstrating that jitter eye movements tended to improve representations of the stimulus in both populations. Also, for both populations, the error in estimated eye position increased much more slowly than if eye movements were ignored (Fig. 3d), showing that both cell groups were informative of the eye movement trajectory. However, the position error was substantially smaller in the midget-only case, suggesting that the midget RGCs were largely responsible for encoding fine eye movements. Fixational eye movements evoke more precisely timed spikes Previous work in the turtle retina has revealed greater temporal precision of RGC spikes in the presence of simulated fixational eye movements [Greschner 2002]. To test whether this could enhance natural image reconstruction, the observed RGC spikes were randomly perturbed in time according to Gaussian distributions with increasing standard deviation (0, 1, 2, 5, 10, 20, and 40 ms), and reconstruction was performed with the perturbed spikes. To ensure optimal reconstruction with the perturbed spikes, the LNBRC models used for estimating likelihood were refitted to perturbed data. Spike time perturbation had two effects on the retinal signal. First, it disrupted the spike train temporal structure, resulting in reduced strength of the fitted LNBRC feedback filter (not shown). Second, because the spike times of each cell were shifted independently, it reduced the spiking synchrony between neighboring cells, resulting in reduced strengths of the LNBRC coupling filters (not shown). For the flashed stimuli, reconstruction quality declined slowly with spike time perturbations up to about 10 ms, and then declined more sharply for larger perturbations, indicating that spike time structure finer than 10 ms was relatively unimportant (Fig. 4a). However, for jittered stimuli, reconstruction quality deteriorated more rapidly as a function of spike time perturbation, and was affected more than the flashed reconstructions by perturbations on the order of 5 ms (see Discussion). This was true regardless of whether eye movements were jointly estimated (Fig. 4b, solid lines) or known a priori (Fig. 4b, dashed lines). Repeating the analysis with the LPIPS perceptual distance measure yielded similar results (Supplemental Information Fig. S7). Thus, eye movements encode the spatial structure of natural images into the fine temporal structure of spikes, and exploiting this encoding enhances decoding. Correlated firing between RGCs contributes to reconstructed image quality Although previous work [, Ruda 2020] has demonstrated that correlated firing of RGCs affects the visual information transmitted by the retina for simple stimuli, the importance of such correlations in retinal representations of naturalistic stimuli is less certain, as are the distinct roles of stimulus-dependent (signal) and stimulus-independent (noise) correlations. To better understand the role of correlations in visual signaling by the retina, natural image reconstruction was performed with a readout that ignored all correlations, or with data shuffled to eliminate noise correlations. To probe the overall role of correlations, LNBR (“uncoupled”) encoding models were fitted to the experimental data, and the resulting natural image reconstructions were compared to the results obtained with the full LNBRC (“coupled”) model, similar to previous analyses for white noise stimuli. The uncoupled models lacked the ability to represent correlated firing between RGCs beyond linear filtering of the shared visual stimulus, and were fitted and used to compute reconstructions in an identical manner to the coupled models. For both the flashed and jittered stimuli, the reconstructions computed using the coupled models were significantly more accurate than those computed using the uncoupled models. For the flashed stimuli (Fig. 5a), the mean MS-SSIM differences between coupled and uncoupled reconstructions were 0.023, 0.024, 0.037, and 0.023, (p-values < 1·10−10, coupled > uncoupled, Wilcoxon signed rank test, N=1500, N=1750, N=750, and N=750, respectively), and for the jittered stimuli (Fig. 5b) the differences were 0.019, 0.010, and 0.039 (p-values < 1·10−10, coupled > uncoupled, Wilcoxon signed rank test, N=1992 for each) . Thus, for naturalistic stimuli, knowledge of correlated firing properties of RGCs beyond that which could be explained by linear filtering of the shared stimulus was necessary to effectively decode image content. The impact of correlated firing on natural image reconstruction could not be attributed to noise correlations alone, in contrast to what was seen in prior work using white noise stimuli. While the cross-correlograms simulated with the coupled LNBRC model (Fig. 5e, red) accurately matched both real data (black) and data shuffled across repeats to remove noise correlations (blue), the cross-correlograms simulated with the uncoupled LNBR model (green) often differed markedly from both. This indicates that the coupled model better represented signal correlations in RGC firing than the uncoupled model. The coupled model also explained a systematically greater fraction of firing variation than the uncoupled model (Fig. 5f). To probe whether noise correlations contributed significantly to the retinal signal, images were reconstructed from synthetic data created by shuffling the recorded responses of each cell across repeated presentations of the same stimulus. Shuffling removed trial-specific noise correlations between cells, but preserved the firing properties of each cell and stimulus-driven signal correlations between cells. Using the LNBRC fitted to the unshuffled data (i.e. with full knowledge of noise correlations), reconstructions were obtained for both the real (unshuffled) repeats as well as the shuffled data. For the flashed stimuli, the reconstructions computed from unshuffled spikes were marginally more accurate than those computed from the shuffled spikes, for all preparations tested, with mean difference values of 7.4·10−3, 6.6·10−3, 5.1·10−3, and 3.4·10−3 (p-values < 1·10−10, data > shuffled, Wilcoxon signed rank test, N=150 for all) respectively. For the jittered stimuli, the effect was similar: the difference was significant for two of the three preparations tested, with mean values 5.9·10−5, 9.5·10−4, and 7.6·10−3, (p-values 0.45, 0.017, and < 1·10−10, data > shuffled, Wilcoxan signed rank test, N=149 for all). While statistically significant, the effect was substantially smaller than that of removing the coupling filters, suggesting that the contributions of noise correlations to the retinal representation of natural stimuli were modest. Analysis using the LPIPS perceptual distance measure yielded similar results (Supplemental Information Fig. S7). Furthermore, comparison of the raw and shuffled repeat cross-correlograms (black and blue lines in Fig. 5e for data and shuffled, respectively) and cross-correlogram peak height (Fig. 5h) showed that noise correlations were substantially smaller than signal correlations. These striking differences compared to reconstruction performed previously using white noise stimuli highlight the importance of understanding visual encoding of naturalistic scenes with eye movements.","We have presented a Bayesian method to invert the retinal code, reconstructing visual images from the spiking responses of a population of RGCs. This reconstruction is not intended as a description of how the brain processes visual images, but as a tool for making explicit the content of the retinal signal in the form of an image, providing insight into the sensory content that is available in neural activity and the way this content is represented. These analyses relied on both the performance and interpretability of the reconstruction method, leveraging both the sophistication of and separation between the likelihood and prior models. The likelihood, obtained from an LNBRC encoding model, effectively captured RGC responses to naturalistic stimuli with modular components that represented stimulus dependency, spike history dependence, and spike time correlations. Although it is not matched to the details of biological circuitry or cellular biophysics, it is convex in its parameters, and thus reliably fit to spiking data and computationally feasible to use for the MAP reconstruction problem. Separately, natural image structure was captured using the prior implicit in a neural network trained to denoise images. Such implicit priors, related to the “score-based generative models” or “diffusion models” that have recently emerged in the machine learning community, offer unprecedented power for capturing image properties while requiring relatively modest amounts of training data. Most importantly, the likelihood and prior components together provide a Bayesian formulation, which offers enhanced interpretability because the components can be independently altered to evaluate their contributions to the retinal representation. The reconstruction approach reveals that the retinal signal alone is sufficient for accurately decoding visual stimuli in the presence of unknown fixational eye movements, consistent with previous theories and psychophysical studies. Though previous computational investigations have explored this possibility in simulation with simplified stimuli, the present work tested it empirically with efficient reconstruction of naturalistic stimuli using recorded RGC responses. Of course, the present findings do not exclude the possibility of additional extra-retinal signals that could help to compensate for fixational eye movements, as has been reported previously [Zhang 2023]. Indeed, the small gap in quality between images reconstructed by the joint algorithm and those reconstructed with full knowledge of the eye position suggests possible benefits of incorporating extra-retinal signals. Increased fixational jitter was found to improve reconstruction quality. This provides additional evidence in support of the theory that fixational eye movements serve a useful function in visual processing, modulating high frequency spatial detail into time domain and/or enabling super-resolution imaging. Furthermore, because this finding held even when the eye movements were unknown a priori, it demonstrates that jitter eye movements specifically improve the fidelity of the retinal representation of natural images. Precisely timed spikes were shown to play an important role in the retinal representation of jitter eye movements. Though RGCs can spike with temporal precision on the order of 1 ms, previous studies have shown that longer integration times (~10 ms) provide the highest-fidelity readout of steady visual motion from RGCs. Consistent with these studies, and with previous flashed natural image reconstruction [Kim 2020, ], we found that flashed image reconstruction was robust to spike train temporal perturbations up to 10 ms. However, in the presence of jitter eye movements, finer temporal precision (2–5 ms) was required for optimal reconstruction. This is consistent with work suggesting that the spike train temporal structure induced by fixational eye movements encodes high-frequency spatial detail [Greschner 2002, ]. As in previous work on reconstruction of white noise stimuli [, Ruda 2020], correlated RGC firing was critical for reconstructing jittered natural images (but see). Surprisingly, however, the effect for naturalistic stimuli was primarily attributable to stimulus-driven correlations rather than the noise correlations that dominated the results in the prior work. The weak role of noise correlations in the present data matched the results obtained by reconstructing flashed natural images using more limited approaches [, Kim 2020] and results from decoding dynamically-varying artificial movies [Botella-Soler 2018]. Future work could extend the Bayesian reconstruction framework to characterize the function of spatio-temporal nonlinearities in the retinal representation of naturalistic stimuli. Though recent work with subunit [Freeman 2015,, Shah 2019] and neural network [McIntosh 2016] encoding models has demonstrated substantial improvements in accounting for RGC spiking, the roles of the spatio-temporal nonlinearities contained in these models for visual signaling remain unclear. Combining such encoding models with denoising image priors to draw samples from the posterior could more deeply probe the interplay between retinal coding and natural image statistics.",10.1101/2023.08.12.552902
PMC10614961,37904967,Trait Reward Sensitivity Modulates Connectivity with the Temporoparietal Junction and Anterior Insula during Strategic Decision Making,"Many decisions happen in social contexts such as negotiations, yet little is understood about how people balance fairness versus selfishness. Past investigations found that activation in brain areas involved in executive function and reward processing was associated with people offering less with no threat of rejection from their partner, compared to offering more when there was a threat of rejection. However, it remains unclear how trait reward sensitivity may modulate activation and connectivity patterns in these situations. To address this gap, we used task-based fMRI to examine the relation between reward sensitivity and the neural correlates of bargaining choices. Participants (N = 54) completed the Sensitivity to Punishment (SP)/Sensitivity to Reward (SR) Questionnaire and the Behavioral Inhibition System/Behavioral Activation System scales. Participants performed the Ultimatum and Dictator Games as proposers and exhibited strategic decisions by being fair when there was a threat of rejection, but being selfish when there was not a threat of rejection. We found that strategic decisions evoked activation in the Inferior Frontal Gyrus (IFG) and the Anterior Insula (AI). Next, we found elevated IFG connectivity with the Temporoparietal junction (TPJ) during strategic decisions. Finally, we explored whether trait reward sensitivity modulated brain responses while making strategic decisions. We found that people who scored lower in reward sensitivity made less strategic choices when they exhibited higher AI-Angular Gyrus connectivity. Taken together, our results demonstrate how trait reward sensitivity modulates neural responses to strategic decisions, potentially underscoring the importance of this factor within social and decision neuroscience.","Social situations such as negotiations often require people to strategically consider social norms while minimizing their threat of being rejected. It is understood that people act fairly when there is a threat of in the Ultimatum Game (UG) and selfishly when there is not a threat of rejection in the Dictator Game (DG). Thus, people exhibit strategic behavior by making smaller contributions in the DG than in the UG. Past investigations suggested there are relations with strategic behavior and measures of social functioning such as emotional intelligence and Machiavellianism. Strategic decisions were also associated with brain activation in the ventral striatum (VS), dorsal lateral prefrontal cortex (dlPFC), and lateral orbitofrontal cortex). Other work has implicated dorsal anterior cingulate cortex (dACC) and posterior cingulate cortex (PCC) in strategic behavior. Decisions made in social contexts reliably elicited activation in the right temporoparietal junction (rTPJ), and greater contributions in the DG. Finally, stimulating the right dlPFC resulted in fairer offers in the UG versus the DG and disruption of the right dlPFC resulted in higher offers in the DG. In sum, it is known that brain activation can distinguish some strategic decision making in social contexts. Relatively less is known about how task-based brain connectivity modulates strategic decisions. Past research suggests that reward signals related to the receipt of rewards are encoded through corticostriatal connectivity Next, DG decisions modulated VS-TPJ connectivity and dorsal striatum-lateral PFC. Since past findings suggested that the VS responses were related to strategic learning and were elevated with greater strategic decision making corticostriatal connectivity may be modulated by social contexts. Individual differences in trait reward sensitivity (RS) may affect how people make social valuations, possibly moderating neural connectivity in social contexts. RS has been studied as a clinical measure, revealing that people who are hyper and hyposensitive to rewards are at risk for substance use and bipolar or depressive disorders. However, little is known about how corticostriatal connectivity is modulated by RS. For instance, people who are more sensitive to rewards may overvalue their initial endowment in UG and DG may be loath to share it with a stranger. Since the striatal response is sensitive to social valuation, examining how social context is moderated by trait reward sensitivity and striatal responses could help unravel how aberrant reward processing may result in maladaptive decisions that can contribute to substance use, or possibly diminished strategic behavior in social situations. Our aims in this investigation were to assess how brain activity and connectivity are modulated by one’s strategic decisions and trait reward sensitivity. Using functional magnetic resonance imaging (fMRI), we administered Ultimatum and Dictator Games to participants to investigate associations between strategic behavior, reward sensitivity, and brain connectivity while controlling for substance use. We examined activation patterns during both endowment and decision phases, corticostriatal connectivity during the decision phase, and how these patterns were modulated by strategic behavior and reward sensitivity. Our analyses focus on two key questions. First, how do strategic decisions in social situations modulate brain activation and connectivity? Second, how does trait reward sensitivity modulate brain connectivity while making strategic decisions?","Participants Although we were initially aiming to collect imaging data from 100 participants (18–22), we ultimately recruited 59 participants due to constraints imposed by the COVID-19 pandemic. We excluded a total of 5 participants for our neuroimaging analyses based on our pre-registered criteria (https://aspredicted.org/55gd8.pdf) and missing data. Specifically, three participants were excluded due to failure to respond during behavioral tasks, where there were greater than 20% missing responses on a given run. One participant was excluded due to incomplete behavioral data. One participant was excluded due to issues with data collection. Three of the 54 participants had one of the two task runs excluded due to excessive head motion. Our final neuroimaging sample resulted in 54 participants (mean age: 20.95 years, SD: 1.78 years; 24.1% male). Several behavioral analyses related to social functioning had a more limited sample due to missing data. Specifically, 9 participants were missing behavioral data related to social functioning, resulting in a sample of 45 participants (mean age: 20.74 years, SD: 1.54 years; 24.4% male) for several behavioral analyses. All participants were compensated at a rate of $25 per hour inside the scanner and $15 per hour outside the scanner, and received bonuses based on their decisions, resulting in a total payment ranging from $140 to $155. Participants were recruited using Facebook advertisements and fliers posted around the Temple University campuses. We verified that participants were eligible to be scanned using fMRI by the following criteria: a) not being pregnant, b) free of major psychiatric or neurologic illness, and c) not under the influence of substances as evidenced by a breathalyzer test and urine drug screen. All the participants provided written informed consent as approved by the Institutional Review Board of Temple University. Procedure Potential participants were identified based on their responses to an online screener questionnaire using the SONA research platform that assessed reward sensitivity using the Behavioral Activation Subscale (BAS) and the Sensitivity to Reward subscale (SR). Using methods consistent with our prior work (e.g., Alloy, Bender, et al., 2009), we compared results between both SR and BAS to ensure that participants were responding consistently and truthfully by excluding participants with scores that were less than +/−1 quintile on both subscales. Participants also were called on the phone and asked to abstain from alcohol or drug usage for 24 hours prior to the scan. Participants were excluded if they reported that they took any psychoactive medications. Participants attended two appointments, consisting of a battery of psychometric surveys, and a mock scan, followed by a second appointment consisting of the fMRI scan and behavioral tasks. Individual Difference Measures Reward Sensitivity. To measure reward sensitivity, we used the Behavioral Activation Scale (BAS) and the Sensitivity to Punishment/Sensitivity to Reward Questionnaire Reward subscale (SPSRWD)). The BAS is a 20-item self-report questionnaire that measures sensitivity to appetitive motives. The SPSRWD is a 24-item self-report measure that assesses how people feel in response to rewarding stimuli. Substance Use. To measure substance use among healthy adult individuals, we used the Alcohol Use Disorders Identification Test (AUDIT) and the Drug Use Identification Test (DUDIT). The AUDIT is a 10-item self-report measure that assesses frequency of usage over the past year and the self-reported extent to which alcohol use affects the person’s life. The DUDIT scale is an 11-item self-report measure counterpart of the AUDIT that assesses frequency and disruptiveness of non-alcohol related substance use. DUDIT contains references to a wide array of substances, including marijuana, cocaine, and others. Social Functioning. To measure social functioning, we measured trait emotional intelligence and attitudes toward rejection. The trait Emotional Intelligence (EI) questionnaire (TEIQe) is a 30-item self-report measure that assesses individual differences in trait empathy, emotion regulation and perspective taking in emotional contexts. Attitudes toward reciprocity were investigated through the 9-item punishment sub-scale of the Personal Norms of Reciprocity (PNR) measure. Experimental Design We examined bargaining behavior using the Ultimatum (Figure 1) and Dictator Games (Figure 1) (~15 min, counterbalanced across participants). In the Dictator Game (DG), the participant decided how much of an endowed sum ($15–25) to share with their partner. To ensure that participants were deceived into believing that their decisions had a social impact, the participant was told their partner was represented by decisions made by past participants in the study, and that their decisions would be used with future participants. In addition, each decision was made by a different partner, resulting in each trial being a one-shot game. This design is used to minimize the concern for reciprocity, reputation or other motives beyond social preferences for fairness while making each choice. In the Ultimatum Game (UG), participants acted as the proposer in some trials and the responder in other trials. As the proposer, participants chose a split of their endowment; however, they were aware that their counterpart could reject their offer. As a recipient in the UG, participants were presented offers from partners that they could choose to accept or reject. If they chose to reject the offer, neither they nor the proposer made any money for that trial. Although our hypotheses and analyses were not focused on the recipient decisions, we included this condition to make the task more believable by making participants think that their unfair proposals could be rejected. We characterize strategic behavior as behavior that offers lower amounts in DG and generally higher amounts in UG, as this strategy would maximize earnings and minimize the threat of rejection. The experiment consisted of three conditions (Dictator Game-Proposer (DG-P), Ultimatum Game-Proposer (UG-P), Ultimatum Game-Recipient (UG-R)) that were presented in a counterbalanced order. The tasks were administered using PsychoPy across two 7:30 minute runs. Each run consisted of 36 trials, with 12 trials in each condition. On each trial, the participant was endowed with a sum of money between $15–$25. The participant experienced an interstimulus interval (ISI) of 1.5–4.5 seconds, M = 2.42s. This was followed by an indication of the type of trial the participant is playing through a target stimulus. If they were acting as the proposer in the DG, they were presented with a triangle. If they were acting as a proposer in the UG, they were presented with a square. Finally, if they were acting as a recipient in the UG they were presented with a circle. During the decision phase as proposer, participants are presented with the option to select a More or Less split. During the decision phase as a recipient, participants have the choice whether to accept or reject the offer. If a participant missed a trial, the screen indicated that they were too slow and recorded a missed trial in the log. Subsequent to each trial, there was a variable duration intertrial interval of 1–8 seconds; M = 2.7s. Behavioral Analysis To examine whether participants acted strategically through offering more as a Proposer in the Ultimatum Game condition versus the Dictator Game condition, we used a mixed effect linear model. The regressors included the task (UGP or DGP), trial endowment, and the proportion of the endowment the participant offered. To check whether participants rejected unfair offers (i.e., offers with a proportion substantially less than half of the endowment) in the Ultimatum Game as a recipient, we used a logistic linear regression model. Next, we assessed whether there were associations between decisions and measures of social functioning, reward sensitivity, and substance use. Given that both hyper- and hypo-sensitivity to rewards have been linked to substance use, we control for levels of substance use in our data while assessing RS. We used correlations between trait measures with the proportions offered in the UG versus DG (i.e.,). This method of measuring strategic behavior was used rather than pooling hypothetical total earnings (see deviations from pre-registration) as this method avoided inferring earnings and simply used the participants’ decisions. We also conducted exploratory analyses to 1) assess whether there are associations between strategic behavior and reward sensitivity and substance use, and 2) whether there are associations between the individual difference measures and individual conditions (DG-P, UG-P, and UG-R). We conducted analyses on the included self-report measures to ensure that they were correctly operationalized for further analyses. Since the BAS and SR subscale of the SPSRWD were highly correlated r(52) = .71, p < .001, we combined them into a single composite measure of reward sensitivity using their combined z-scores. Reward sensitivity scores were binned into deciles to produce an even distribution for subsequent analysis. Finally, because both hyper- and hypo-sensitivity to rewards have been linked to substance use (e.g.,), we squared the binned composite reward sensitivity scores to create an additional, quadratic measure of aberrant reward sensitivity. In other words, aberrant reward sensitivity explores whether there are consistent patterns across people who are either high or low in reward sensitivity. Next, we found that AUDIT and DUDIT also were correlated r(52) = .32, p = .02. As a result, we operationalized problematic substance use through z-scoring the responses between the measures and combining them into a single composite z-score of problematic substance use using the same method as described for reward sensitivity. Neuroimaging Data Acquisition Functional images were acquired using a 3T Siemens PRISMA MRI scanner. Bold Oxygenation Level-Dependent (BOLD) sensitive functional images were acquired using a gradient echo-planar imaging (EPI) sequence (240 mm in FOV, TR = 1,750 ms, TE = 29 ms, voxel size of 3.0 × 3.0 × 3.0 mm3, flip angle = 74°, interleaved slice acquisition). Each of the two runs included 225 functional volumes. We also collected single-band reference images with each functional run of multi-band data to improve motion correction and registration. To facilitate anatomical localization and co-registration of functional data, a high-resolution structural scan was acquired (sagittal plane) with a T1-weighted magnetization=prepared rapid acquisition gradient echo (MPRAGE) sequence (224 mm in FOV, TR = 2,400 ms, TE = 2.17 ms, voxel size of 1.0 × 1.0 × 1.0 mm3, flip angle 8°). In addition, we also collected a B0 fieldmap to unwarp and undistort functional images (TR: 645 ms; TE1: 4.92 ms; TE2: 7.38 ms; matrix 74×74; voxel size: 2.97×2.97×2.80 mm; 58 slices, with 15% gap; flip angle: 60°). Preprocessing of Neuroimaging Data. Neuroimaging data were converted to the Brain Imaging Data Structure (BIDS) using HeuDiConv version 0.9.0. Results included in this manuscript come from preprocessing performed using fMRIPrep 20.2.3 (Esteban et al., 2018,), which is based on Nipype 1.4.2. The details described below are adapted from the fMRIPrep preprocessing details; extraneous details were omitted for clarity. Head motion was calculated to determine exclusions. Excess head motion was defined where at least one run was a motion outlier, characterized by either fd mean exceeding 1.5 times the interquartile range above the 75th or tsnr values lower than 1.5 times the lower bound minus the 25th percentile per neuroimaging data quality measures from MRIQC). Anatomical data preprocessing. The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with ‘N4BiasFieldCorrection’, distributed with ANTs 2.3.3, and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the ‘antsBrainExtraction.sh’ workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM), and gray-matter (GM) was performed on the brain-extracted T1w using ‘fast’ (FSL 5.0.9). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with ‘antsRegistration’ (ANTs 2.3.3), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c (TemplateFlow ID: MNI152NLin2009cAsym). Functional data preprocessing. For each BOLD run, the following preprocessing steps were performed.. First, a reference volume and its skull-stripped version were generated by aligning and averaging 1 single-band references (SBRefs). A B0-nonuniformity map (or fieldmap) was estimated based on a phase-difference map calculated with a dual-echo GRE (gradient-recall echo) sequence, processed with a custom workflow of SDCFlows inspired by the ‘epidewarp.fsl’ script (http://www.nmr.mgh.harvard.edu/~greve/fbirn/b0/epidewarp.fsl) and further improvements in HCP Pipelines. The fieldmap was then co-registered to the target EPI (echo-planar imaging) reference run and converted to a displacements field map (amenable to registration tools such as ANTs) with FSL’s ‘fugue’ and other SDCflows tools. Based on the estimated susceptibility distortion, a corrected EPI (echo-planar imaging) reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using ‘flirt’ (FSL 5.0.9) with the boundary-based registration cost-function. Co-registration was configured with nine degrees of freedom to account for distortions remaining in the BOLD reference. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using ‘mcflirt`. BOLD runs were slice-time corrected using ‘3dTshift’ from AFNI 20160207. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), and three region-wise global signals. FD was computed using two formulations following Power (absolute sum of relative motions) and Jenkinson (relative root mean square displacement between affines). FD is calculated for each functional run using its implementations in Nipype. The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (CompCor). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for anatomical component correction (aCompCor). For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) are generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, the aCompCor masks are subtracted from a mask of pixels that likely contain a volume fraction of GM. This mask is obtained by thresholding the corresponding partial volume map at 0.05, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components’ time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e., head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using ‘antsApplyTransforms’ (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels. Many internal operations of fMRIPrep use Nilearn 0.6.2, mostly within the functional processing workflow. For more details of the pipeline, see the section corresponding to workflows in fMRIPrep’s documentation (https://fmriprep.readthedocs.io/en/latest/workflows.html). Next, we applied spatial smoothing with a 5mm full-width at half-maximum (FWHM) Gaussian kernel using FEAT (FMRI Expert Analysis Tool) Version 6.00, part of FSL (FMRIB’s Software Library, www.fmrib.ox.ac.uk/fsl). Non-brain removal using BET and grand mean intensity normalization of the entire 4D dataset by a single multiplicative factor were also applied. Neuroimaging Analyses Neuroimaging analyses used FSL version 6.0.4. We used two general linear models. Both types of models were based on a general linear model with local autocorrelation. Our first model focused on the brain activation evoked during the decision phase of the DG-P, UG-P, and UG-R conditions and used a total of thirteen regressors. Six regressors reflected the endowment phase (duration = 1,000 ms) across all three conditions (DG-P, UG-P, and UG-R) for both unmodulated and parametrically modulated analyses. For the parametrically modulated regressors, we calculated the proportion of the endowment proposed as the independent variable. Proportions closer to 0.5 reflected a more even split and were indicative of fairer options, whereas proportions closer to 0 reflected more unfair offers. Six regressors reflected the three task conditions during the decision phase (duration = 3,000 ms). We modelled both unmodulated and parametrically modulated effects for each task. Finally, the thirteenth regressor modelled missed trials. Our second type of model focused on the task-dependent connectivity using the ventral striatum as a seed and areas related to social processing as target regions. To estimate the changes in connectivity resulting from strategic behavior, we used psychophysiological interaction (PPI) analysis. This form of analyzing effective connectivity has been shown through meta-analyses to reveal reliable and specific patterns of task-dependent connectivity. Our PPI analysis focused on effective connectivity using the ventral striatum (VS; Oxford-GSK-Imanova atlas) as a seed. The average time course of activation from this seed region was extracted and used as an additional fourteenth regressor. To construct the PPI model, we used the same model described above and added 14 additional regressors (1 regressor for the VS region and 13 regressors for the interaction between the VS region and the task-based regressors), yielding a total of 25 regressors in each seed-based PPI model. Both activation and connectivity models were then run through a second level analysis that combined the first and second runs. For participants with missing data, or for runs that were excluded due to head motion, we used a participant’s one good L1 run in the group level analyses. Both models also included a common set of confound regressors. We also included additional regressors for the six motion parameters (rotations and translations), the first six aCompCor components explaining the most variance, non-steady state volumes, and the framewise displacement (FD) across time. Finally, we used high-pass filtering (128s cut-off) through a set of discrete cosine basis functions. For all participants and their combined runs, we used a fixed-effects model. These models focused on activation and connectivity patterns and their associations between bargaining behavior, substance use and BOLD responses, independent of RS. Group-level analysis was carried out using FLAME (FMRIB’s Local Analysis of Mixed Effects) Stage 1. Our group-level model focused on comparisons between the Dictator and Ultimatum Games as a Proposer; these comparisons included covariates to account for reward sensitivity, the second-order polynomial expansion of reward sensitivity (which captures effects tied to aberrant reward sensitivity), substance use, strategic behavior, temporal signal to noise ratio (tSNR) and mean framewise displacement (fd mean). We also used two additional models that explore interaction effects. The first interaction model included additional regressors of substance use and reward sensitivity and substance use and aberrant reward sensitivity. The second interaction model included additional regressors of the interaction of strategic behavior and reward sensitivity, and strategic behavior and aberrant reward sensitivity. All pre-registered hypotheses were first tested with a priori regions of interest. We used the Harvard-Oxford Atlas to make masks for the dlPFC, mPFC (ie: paracingulate gyrus), SPL, ACC, and Insula. We used the Oxford-GSK-Imanova and Striatal Connectivity Atlases to define the nucleus accumbens mask. Next, we used the Mars TPJ Atlas for our pTPJ mask. Finally, for the vmPFC mask, we used the Clithero and Rangel, 2013 meta-analysis coordinates (−2, 28, −18) and a 5mm sphere. We followed up our ROI-based analyses with exploratory whole brain analyses to assess activation and seed-based connectivity in brain regions we did not have pre-registered. All z-statistic images were thresholded and corrected for multiple comparisons using an initial cluster-forming threshold of z > 3.1 followed by a whole-brain corrected cluster-extent threshold of p < 0.05, as determined by Gaussian Random Field Theory. Exploratory whole brain analyses reported coordinates using the center of gravity (CoG). Deviations from Pre-Registration Once we began data collection and analyses, we made several adjustments based on four issues that were unspecified in our pre-registration. First, we initially specified that we would use the parametric effect of endowment, but not for decisions. For decisions, we expected to use the actual offers selected (High, Low) in our analyses. However, since many participants selected High more often in the UG condition and Low in the DG condition, these regressors had fewer events for comparison. To address this issue, we modeled strategic decisions as parametric effects of offer amount through the difference in the proportions of the endowments offered between DGP and UGP. Second, we adjusted the covariates in our group level models due to missing data. Although we originally planned to study Machiavellianism, due to an error in data collection, this survey was not completed by our participants. Next, whereas substance use analyses were not mentioned in the pre-registration, we intended to complete them in accordance with the broader aims and hypotheses of the grant, which are also described in the grant report. Third, we used the (−2, 28, −18) meta-analysis vmPFC coordinates for our mask rather than the mask specified in the pre-registration for greater spatial specificity in our analyses. Fourth, we explored group level models that included the interaction of reward sensitivity, substance use and strategic behavior despite not being initially pre-registered. Taken together, these adjustments from the pre-registration have allowed us to analyze the data more robustly, though our results and discussion take greater care to differentiate between confirmatory and exploratory results, especially emphasizing differences in our group level models.","Below, we report results from behavioral analyses, task-based neural activation and connectivity analyses, and exploratory whole brain analyses. We begin by presenting results of the behavioral tasks, assessing whether participants made choices as expected, and their self-reported levels of emotional intelligence, attitudes toward rejection, reward sensitivity, and substance use. Next, we present both pre-registered region of interest and exploratory whole brain analyses activation results during the endowment and decision phases. We then present results of pre-registered and exploratory psychophysiological interaction (PPI) analyses examining strategic choices between the dictator and ultimatum games within reward-related and social neural systems. Finally, we present associations between attitudes toward fairness, reward sensitivity, and brain connectivity. Strategic Behavior If participants made higher offers in the Ultimatum Game compared to the Dictator Game, this would indicate that participants were acting most consistently toward maximizing their earnings, thereby exhibiting strategic behavior. Consistent with our expectations, using a mixed effects model for a random intercept, we found that participants (N=54) made more selfish offers in the DG vs. the UG conditions, (B = −0.43, SE = 0.015, t(2550) = −28.09, p <.001), (see Figure 2). As a manipulation check, we investigated whether participants rejected unfair offers in the recipient condition. A binary logistic regression indicated that participants reject more often with lower offers, (B = 1.72, SE = 0.095, t(1252)= 18.06, p <.001). Next, we explored whether there was a relation between strategic behavior and rejection rate as a function of offer amount as a recipient, finding no significant association, r(52) = −.19, p =.16. Given that there was no relationship of recipient choices to strategic decisions as proposers, we excluded these measures from subsequent analyses. Next, we assessed whether measures of social functioning (N=45) were related to strategic decisions. Consistent with our hypotheses, individuals scoring higher on the Emotional Intelligence (EI) scale made higher offers as a proposer in the Ultimatum Game, r(43) = .35, p = .02. Contrary to our hypotheses, we did not find associations between strategic behavior, emotional intelligence, or attitudes toward rejection that met a p value of less than p=.05. Inasmuch as there was no effect of strategic behavior and our measures of social functioning as we hypothesized, we excluded these measures from further analyses. Although we did not expect relations between strategic behavior and measures of reward sensitivity and substance use, we explored whether there were such associations to contextualize any brain relations we may have found with these respective individual difference measures. We did not find any significant associations between reward sensitivity and substance use, and strategic behavior or individual task conditions (DG-P, UG-P, UG-R) that met a threshold of p < .05. Neural Responses during Endowment Our first goal for our neuroimaging analyses was to examine the response to endowment. We hypothesized that responses in the ventral striatum would increase as a function of the endowment size. We conducted an ROI based analysis, expecting activation in the VS and the vmPFC. We expected that VS and vmPFC responses would vary based on the expectation of how much of the endowment would subsequently be kept in the decision phase, with the greatest amounts in DGP, moderate in UGP, and lowest in UGR. Contrary with our hypotheses, we did not find a significant difference in vmPFC activation during the endowment phase between the DG-P, UG-P, and UG-R conditions, using a one-way ANOVA (2,159) F = 2.40 p = 0.09. Next, we did not find significant activation in the VS during the endowment phase when we used a one-way ANOVA to compare activation between the DG-P, UG-P, and UG-R task conditions F(2,159) = 1.10 p = 0.34 as participants received greater proportions of the endowment. Next, we investigated whether there were associations between reward sensitivity, neural activation, and the levels of endowment presented. We expected that the response in the VS and vmPFC would positively vary based on the endowment. Such an association would reveal if there were moderating variables in VS or vmPFC activation as participants are endowed with higher levels of money when there is a threat of rejection (UG) versus when there is not (DG). We did not find an association between the level of activation in the vmPFC or VS, amount of endowment, and substance use or reward sensitivity as none survived the multiple comparisons correction. Neural Responses while Making Strategic Decisions To examine the effect of bargaining decisions, we investigated how the brain responds as people exhibit strategic behavior through offering fairer offers in the Ultimatum Game (UG) versus the Dictator Game (DG). We hypothesized that we would find activation in areas previously examined, and brain areas associated with social processing, specifically the Anterior Cingulate Cortex (ACC), Superior Parietal Lobule (SPL), and posterior temporoparietal junction (pTPJ). We conducted a ROI analysis in each of these regions and we did not find any significant activation that met p = .05 or lower. Nonetheless, we followed up with exploratory whole brain analyses with appropriate adjustments for multiple comparisons to investigate whether there are other regions that may reflect strategic decision making. When assessing how people chose to be selfish versus fair in the contrast between the DG and UG as a proposer, we found significant clusters in the Inferior Frontal Gyrus (IFG) (MNIxyz = 51, 24, 24; cluster = 20 voxels, p=.035) and a cluster spanning the Anterior Insula (AI), extending into the Orbitofrontal Cortex (OFC) (MNIxyz = 33, 27, −4; cluster = 54 voxels, p<.001). We did not find significant activation in the vlPFC or the VS. In the contrast between UG and DG (i.e., choosing to be fair versus unfair), we found a significant cluster in cerebellum (MNIxyz = 30, −82, −36; cluster = 37, p<.001). In sum, some of our results successfully replicated past investigations of strategic behavior. Strategic Behavior and Neural Connectivity Beyond activation patterns, we examined whether task-dependent connectivity patterns related to reward sensitivity and strategic decisions made in the Dictator and Ultimatum games. First, we examined our pre-registered ROI-based hypotheses using the ventral striatum as a seed. During the decision phase for the proposer conditions, we expected to find that ventral striatal responses to situations requiring strategic thinking (UG-P > DG-P) would be associated with enhanced connectivity with regions modulated by social information. To test this hypothesis, we conducted a PPI analysis using the VS as a seed region, and we focused on connectivity with a priori target regions (vmPFC, mPFC as defined by paracingulate gyrus, pTPJ, and the SPL). Contrary to our hypotheses, we did not find any regions of interest that survived multiple comparisons. Next, we conducted a whole-brain exploratory analysis to assess relations of other possible target regions to reward sensitivity and strategic behavior. We included the IFG and AI as seeds because they were derived from the activation of DG versus UG contrast in our data. Our group level analyses employed several covariates, including motion-based nuisance regressors, reward sensitivity, substance use, and strategic behavior. We also used two additional models that investigated the interactions of reward sensitivity, strategic behavior, and substance use respectively. First, we wanted to explore if strategic behavior as measured by the choices our participants made was associated with brain connectivity. Using the IFG as a seed (MNIxyz = 52, 16, 22), we found that enhanced connectivity with a left pTPJ target region extending into the SMG (MNIxyz = 50, −68, 35; cluster = 22 voxels, p = .008) was modulated by strategic behavior in the Dictator versus Ultimatum Game (see Figure 4). That is to say, selfish participants (i.e.: by making lower proposals in the DG versus UG conditions) experienced enhanced IFG-pTPJ connectivity contingent on whether or not there was a threat of rejection. Our results suggest that enhanced IFG-pTPJ connectivity may facilitate the social processing associated with strategic decisions in social contexts. We also examined if connectivity from an AI seed related to strategic situations was modulated by strategic behavior. Using the AI seed (MNIxyz = 33, 27, −4), we found that attenuated connectivity with the neighboring insular cortex (MNIxyz = 50, 6, −1; cluster = 26 voxels, p = .003) was modulated by strategic behavior in UG versus DG condition. That is to say, participants who were more selfish when there was no threat of rejection exhibited lower AI-Insula connectivity. Our results suggest that attenuated co-activation of the insular cortex may contribute to making more selfish choices in social contexts. The Association Between Connectivity with the Anterior Insula and Trait Reward Sensitivity is Moderated by Strategic Behavior Next, we examined how the interaction of reward sensitivity and substance use may moderate brain connectivity patterns associated with strategic thinking in bargaining situations. Investigating how a trait like reward sensitivity may moderate brain responses can reveal an important factor for understanding both behavior and brain relationships. Specifically, we used a model that included interaction covariates of strategic thinking with reward sensitivity and aberrant reward sensitivity. The model also controlled the main effects of strategic behavior, reward sensitivity, aberrant reward sensitivity, and substance use. We included substance use as a controlling variable due to its known relationships with reward sensitivity in psychopathology. We found that the interaction of reward sensitivity and strategic behavior moderated AI-Angular Gyrus connectivity in the UG versus DG condition (Figure 5). That is to say, participants with higher reward sensitivity and attenuated AI-Angular Gyrus connectivity tended to make more strategic choices when there was a threat of rejection relative to when there was not. Moreover, participants with lower reward sensitivity and enhanced AI-Angular Gyrus connectivity tended to make more strategic choices when there was a threat of rejection compared to when there was not. These results suggest that the combination of strategic decisions and a person’s trait reward sensitivity together may modulate connectivity patterns in social situations requiring strategic thinking. Discussion This study investigated how relations between strategic behavior in bargaining situations and reward responses correspond to patterns of brain activation and connectivity. First, our behavioral results are consistent with past work suggesting that participants act strategically in bargaining situations through acting fairly when there is a threat of rejection (e.g., Ultimatum Game; UG) while keeping more for themselves when there is not a threat of rejection (Dictator Game; DG). Second, we found that strategic behavior between the Dictator and Ultimatum Games evoked activation in the inferior frontal gyrus (IFG) and Anterior Insula (AI), results that were consistent with past investigations (i.e.,). Our analyses also indicated that elevated IFG-rTPJ connectivity was related to enhanced strategic behavior and attenuated AI-Angular Gyrus connectivity was modulated by the interaction of reward sensitivity and strategic behavior. Our work fits in with past literature suggesting that norm compliance is regulated by cortical activation. Although we did not find activation during UG versus DG in our pre-registered regions of interest, our whole brain analyses revealed activation in the right IFG and AI as participants made strategic decisions, replicating previous work. Next, both IFG and AI activation has been observed in other decision-making contexts. For example, FeldmanHall and colleagues reported AI activation during moral decision making. In addition, other work has shown that increased activation in the anterior insula in a trust task is associated with inequity aversion. Further, our results are consistent with stimulation-based research that found elevated right dlPFC area activation corresponded to more strategic behavior and inhibition of dlPFC activity diminished strategic choices. In sum, our findings are consistent with the IFG and AI being involved in norm compliance decisions. Our work extends past literature through investigating how reward processes and cortical connectivity interact with strategic behavior. Our results indicate that elevated IFG-pTPJ connectivity is associated with increased strategic behavior, whereas attenuated AI-Angular Gyrus connectivity is modulated by the interaction of RS and strategic behavior. Although recent work has shown that the dlPFC and pTPJ regulate norm compliance in the UG and DG games ( and that the rTPJ does not necessarily yield greater generosity, our results indicate that the connectivity between these brain regions modulates strategic decisions in social situations. Understanding how connectivity modulates strategic decisions is a critical component of characterizing how the TPJ and dlPFC may be regulated during decision making. Nonetheless, when including RS as a covariate, we find that this relationship changes as people with low RS are more strategic with decreasing AI-Angular Gyrus connectivity. As such, our results suggest a nuanced view of AI-Angular Gyrus and IFG-TPJ coupling, indicating that these brain regions do not necessarily reflect altruistic choice on their own, but may modulate cognitive reward processes while making social decisions. We speculate that our results suggest the downregulation of bilateral TPJ activation and AI deactivation interacts with trait reward sensitivity. Although our work has found that strategic behavior is modulated by both AI-Angular Gyrus and IFG connectivity with the TPJ, and reward sensitivity, we acknowledge that our study has several limitations that merit discussion. First, although we found relations with bilateral TPJ connectivity and strategic behavior, we do not infer specificity in lateralization. Past investigations suggest mixed findings as to the roles of the right and left TPJ, and we judged that exploring these results further was beyond the scope of our paper. Second, we note that relations with social context, reward sensitivity, and brain connectivity could be studied more extensively in a clinical population to assess how these relations are modulated by substance use and manic-depressive symptoms. Whereas we were able to control for levels of substance use to account for RS effects, we had too limited variability in substance use to make inferences about how substance use may contribute to maladaptive strategic decisions. Third, we acknowledge that connectivity analyses are not causal or directional with respect to inference despite identifying the IFG and AI as seeds and the temporoparietal junction as target. Another possible future direction includes evaluating AI-Angular Gyrus and IFG-TPJ connectivity patterns, associations with reward sensitivity, and their relations with recipient decisions in the Ultimatum Game. Fourth, since strategic behavior as a proposer was not related to recipient choices, we judged that these results are beyond the scope of this investigation. Finally, while we assessed strategic behavior, we did not assess it in a dynamic context. As social contexts increase exploration and obtained rewards, a fruitful future direction could investigate how brain responses to changes over time reflect social decisions. Despite the limitations, our findings indicate that strategic decisions in social contexts are associated with AI-Angular Gyrus and IFG-TPJ connectivity and are modulated by trait reward sensitivity. These results provide greater insights into how reward processes interact with social decisions, involving brain processes that appraise the roles of other people while making choices. Since aberrant reward sensitivity is a major mechanism in substance use and depressive and bipolar disorders, investigating how reward sensitivity modulates brain processes during social contexts could provide considerably more understanding into how people make maladaptive decisions resulting in substance use. Such work could help identify people at risk for substance use disorders and help develop interventions for people with aberrant reward patterns. Conflict of interest statement The authors declare no conflicts of interest.",,10.1101/2023.10.19.563125
PMC10508778,37732280,CBGTPy: An extensible cortico-basal ganglia-thalamic framework for modeling biological decision making,"Here we introduce CBGTPy, a virtual environment for designing and testing goal-directed agents with internal dynamics that are modeled on the cortico-basal-ganglia-thalamic (CBGT) pathways in the mammalian brain. CBGTPy enables researchers to investigate the internal dynamics of the CBGT system during a variety of tasks, allowing for the formation of testable predictions about animal behavior and neural activity. The framework has been designed around the principle of flexibility, such that many experimental parameters in a decision making paradigm can be easily defined and modified. Here we demonstrate the capabilities of CBGTPy across a range of single and multi-choice tasks, highlighting the ease of set up and the biologically realistic behavior that it produces. We show that CBGTPy is extensible enough to apply to a range of experimental protocols and to allow for the implementation of model extensions with minimal developmental effort. Author summary We introduce a toolbox for producing biologically realistic simulations of the cortico-basal ganglia-thalamic dynamics during a variety of experimental tasks. The purpose is to foster the theory-experiment cycle, offering a tool for generating testable predictions of behavioral and neural responses that can be validated experimentally, in a framework that allows for simple updating as new experimental evidence emerges. We outline how our toolbox works and demonstrate its performance on a set of normative cognitive tasks.","With the rise of fields like cognitive computational neuroscience, there has been a resurgence of interest in building biologically realistic models of neural systems that capture prior observations of biological substrates and generate novel predictions at the cellular, systems, and cognitive levels. In many cases, researchers rely on off-the-shelf machine learning models that use abstracted approximations of biological systems (e.g., rate-based activity and rectified linear unit gating, among others) to simulate properties of neural circuits. For researchers interested primarily in cortical sensory pathways, these systems work well enough at making behavioral and macroscopic network predictions, but they often fail to provide biologically realistic predictions about underlying cellular dynamics that can be tested in vivo. Although there are a wealth of biologically realistic simulations of cortical and non-cortical pathways that have helped to significantly advance our understanding of BG function, these are often designed to address very narrow behaviors and lack flexibility for testing predictions across multiple experimental contexts. Here we present a scientifically-oriented tool for creating model systems that emulate the control of information streams during decision making in mammalian brains. Specifically, our approach mimics how cortico-basal ganglia-thalamic (CBGT) networks are hypothesized to regulate the evidence accumulation process as agents evaluate response options. The goal of this tool, called CBGTPy, is to provide a simple and easy-to-use spiking neural network simulator that reproduces the structural and functional properties of CBGT circuits in a variety of experimental environments. The core aim of CBGTPy is to enable researchers to derive neurophysiologically-realistic predictions about basal ganglia dynamics under hypothesis-driven manipulations of experimental conditions. A key advantage of our CBGTPy framework is that it separates most properties of the behaving agent from the parameters of the environment, such that experimental parameters can be tuned independently of the agent properties and vice versa. We explicitly distinguish the agent (Section 2.3.1) from the environment (Section 2.3.2). The agent generates two behavioral features – action choice and decision time – that match the behavioral data typically collected in relevant experiments and affords users the opportunity to analyze the simultaneous activity of all CBGT nuclei under experimental conditions. The flexibility of the environment component in CBGTPy allows for the simulation of both simple and complex experimental paradigms, including learning tasks with complex feedback properties, such as volatility in action-outcome contingencies and probabilistic reward scenarios, as well as rapid action control tasks (e.g., the stop signal task). On the biological side, CBGTPy incorporates biologically-based aspects of the underlying network pathways and dynamics, a dopamine-dependent plasticity rule, and the capacity to mimic targeted stimulation of specific CBGT nuclei (e.g., optogenetic stimulation). CBGTPy also allows the easy addition of novel pathways, as well as modification of network and synaptic parameters, so as to enable modeling new developments in the CBGT anatomy as they emerge in the literature. After a brief review of the CBGT pathways in the next subsection, in Section 2 we provide a full description of the structure, use, and input parameters of CBGTPy. In Section 3, we go on to present examples of its usage on a variety of standard cognitive tasks, before turning to a discussion in Section 4. Various appendices (S1.2, S2 Appendix, S4 Appendix, S5 Appendix) present additional details about the CBGT model and CBGTPy toolbox, including the implementation of synaptic plasticity and a guide for CBGTPy installation. Recent findings have suggested that the simple concepts of rigidly parallel feedforward basal ganglia (BG) pathways may be outdated 1, and part of the motivation for CBGTPy is to provide a tool for developing and exploring more nuanced, updated theories of CBGT dynamics as new discoveries are made. Indeed, achieving a full understanding of CBGT circuit-level computations requires the development of theoretical models that can adapt with and complement the rapidly expanding empirical evidence on CBGT pathways. The fundamental goal of the CBGTPy toolbox is to provide a framework for this rapid theoretical development, which balances biological realism with computational flexibility and extensibility. The toolbox The core of the CBGTPy toolbox comprises an implementation of a spiking model CBGT network tuned to match known neuronal firing rates and connection patterns that have been previously used to study various aspects of basal ganglia function in cognitive tasks. The CBGT network model is composed of 6 different regions/nuclei shown in Figure 1: a cortical component, segregated into excitatory (Cx) and inhibitory (CxI) subpopulations; striatum, containing two subpopulations of spiny projection neuron (dSPNs involved in the so-called direct pathway, and iSPNs, involved in the indirect pathway) and also fast-spiking interneurons (FSI); external globus pallidus (GPe), which is divided into prototypical (GPeP) and arkypallidal (GPeA) subpopulations; subthalamic nucleus (STN); internal segment of globus pallidus (GPi); and a pallidal-receiving thalamic component (Th), which receives input from GPi and Cx and projects to cortical and striatal units. Within each region, we model a collection of spiking point neurons, modeled in a variant of the integrate-and-fire framework to include the spiking needed for synaptic plasticity while still maintaining computational efficiency. Numerical integration is performed via custom Cython code, rather than relying on existing frameworks, such as NEURON, BRIAN, or NetPyNE, a design choice which simplified the overall software stack. The core strengths of these frameworks are in the simulation of multi-scale or multi-compartment models, whereas one of the strengths of the CBGTPy model is the high level of direct control that can be exerted over the neural parameters throughout the interactions between the network and its environment (see Section 2.1). The integration is performed in a partially-vectorized manner, in which each variable is represented as a list of Numpy arrays, one array per neural population. Further details of the implementation of this network, including all relevant equations and parameter values, are provided in S1.2. CBGTPy allows for the simulation of two general types of tasks that cover a variety of behavioral experiments used in neuroscience research. The first of these tasks is a discrete decision-making paradigm (n-choice task) in which the activity of the CBGT network results in the selection of one choice among a set of options (see Section 3.1). If plasticity is turned on during simulations, phasic dopamine, reflecting a reward prediction error, is released at the corticostriatal synapses and can modify their efficacy, biasing future decisions. We note that the inclusion of a biologically-realistic, dopamine-based learning mechanism, in contrast to the error gradient and backpropagation schemes present in standard artificial agents, represents an important feature of the model in CBGTPy. We present the details of this learning mechanism in S2 Appendix. The second of these tasks is a stop signal paradigm (stop signal task), where the network must control the execution or suppression of an action, following the onset of an imperative cue (see Section 3.2). Here activity of the indirect and pallidostriatal pathways, along with simulated hyperdirect pathway control, determines whether a decision is made within a pre-specified time window. The probability of stop and the relevant RT distributions can be recorded across different values of parameters related to the stop signal. Agent-Environment Paradigm We have adopted an environment-agent implementation architecture, where the internal properties of the CBGT network (the agent) are largely separated from the external properties of the experiment (the environment). Interaction between the agent and environment is limited in scope, as shown in Figure 2, and occurs only at key time points in the model simulation. The core functionality of the agent is the mapping from stimuli to decisions and the implementation of post-decision changes (e.g., synaptic strength updates) to the CBGT network, while the environment serves to present stimuli, cues, and rewards. CBGTPy uses a data-flow programming paradigm, in which the specification of computing steps is separated from the execution of those steps. Internally, the initialization and simulation of the agent-environment system is divided into a large number of specialized functions, each addressing specific tasks. These code blocks are then organized into sequences, referred to as pipelines. Only after a pipeline is constructed is it executed, transforming any input data into output data. The use of pipelines allows for individual code blocks to be rearranged, reused, and modified as necessary, leading to efficient code reuse. One of the main benefits of the data-flow design is its synergy with the Ray multiprocessing library for Python. Ray operates on a client-server model and allows for the easy distribution of tasks and worker processes based on the available resources. While the sequence of steps for running a simulation can be constructed locally, those same steps can be distributed and performed remotely on the Ray server. As a result, CBGTPy directly supports running on any system that can support a Ray server, which includes high-core-count computing clusters, while maintaining the exact same interface and ease-of-use as running simulations on a local machine. The user, however, can choose to run the model without any multiprocessing library or an alternative multiprocessing library to Ray. These options are explained in detail in the Section 2.2. In the following subsections, we explain all the details of the toolbox by separately describing the agent and environmental components that can be changed by the user. The CBGTPy toolbox can be found in the Github repository https://github.com/CoAxLab/CBGTPy/tree/main. The instructions to install it and the list of functions contained in the toolbox can be found in S3 Appendix and S4 Appendix, respectively. Setting up a simulation One of the objectives of CBGTPy is to enable end users to easily run simulations with default experimental setups. Furthermore, users can specify parameter adjustments with minimal effort, specifically through use of a configuration variable, which we describe in greater detail in the following sections. The following list contains a mandatory set of instructions to be executed in order to implement the entire process associated with running a simulation. These instructions will proceed with a default set of parameters. We also provide two example notebooks (n-choice task and stop signal task), which can be found in the repository and include these steps and commands. Import all relevant functions. Create the main pipeline. Import the relevant paramfile for the selected experiment type. Create configuration dictionary with default values. Run the simulation, specifying which multiprocessing library to use. Extract relevant data frames (e.g., firing rates, reaction times, performance). Save variables of interest as pickle files. Plot variables of interest (e.g., firing rates and reward data frames). We explain each of these steps in detail. Note that if Ray multiprocessing is being used, changing the local IP node (e.g., when the underlying Wi-Fi/LAN network has changed) requires stopping the previous instance of the Ray server and restarting it with the newly assigned IP. We also explain how to shut down the Ray server at the end of this section. Import relevant functions. All the relevant imports can be implemented with the following commands: Create the main pipeline. Here the user can choose to run either the n-choice task or the stop signal task by assigning a variable experiment_choice2. Depending on the choice of this variable a relevant pipeline is created. A pipeline consists of all the modules required to run a task and returns a pipeline object that can be used. For a basic n-choice task, the experiment_choice needs to be set as while for a basic stop signal task, it is set as In both cases, the user also should decide how many choices or action channels the current instance of CBGTPy should create and run, using the variable number_of_choices: While a common version of this decision making task is run with number_of_choices = 2, it can also be run for an arbitrary number of choices (i.e., number_of_choices ≥ 1). The change in the number of choices and corresponding action channels requires scaling of some of the parameters in order to ensure maintenance of the same amount of input to certain shared CBGT nuclei irrespective of the number of action channels. We explain these scaling schemata in S5 Appendix. Please note that some parameters have to be appropriately updated in the configuration variables (e.g., Q data frame, channel names) according to the number of choices selected. We explicitly mention which parameters should be updated with the number_of_choices as we describe them below, and we include example notebooks in the repository for different cases. The pipeline is created with commands Import the relevant paramfile for the selected experiment type. The paramfile contains dictionaries of default parameter values for the neural populations and plasticity model based on the choice of the experiment. The imported attributes, which can be listed out using dir(paramfile), can be modified according to the user’s preferences. For example, setting the cellular capacitance value to 0.5 is accomplished with Create configuration dictionary with default values. The configuration variable is a dictionary in which some parameters take internally set default values, whereas others need to be assigned values to run a simulation. A minimal configuration variable consists of the following parameters, the details of which are described in S2 Table, S3 Table, S4 Table, S5 Table, S6 Table, S7 Table and explained separately in Section 2.3. Note that the parameter corticostriatal_plasticity_present does not have to be introduced in the configuration dictionary when running the stop signal task (see reference on the example notebook). Additionally, it is important to include the stop signal parameters within the configuration dictionary when executing the stop signal task. More details will be provided in the corresponding sections. For reference, you can find an example notebook on github. Run the simulation At this stage, the user can choose the number of cores to be used (num_cores) and the number of parallel simulations that should be executed with the same configuration variable but a different random seed (num_sims). Moreover, the user can optionally specify one of the two supported multiprocessing libraries, Ray and Pathos, to use to run the simulation. Ray is a library providing a compute layer for parallel processing. To start the Ray server, on the command line, run Ray server to execute the head node and obtain the local IP node, in the following way: This command should list a local IP along with a port number. Hence, to initiate a Ray client that connects to the server started above, the user will have to substitute the local IP node, obtained from the previous command line, in place of <local ip node> in Any port number that is free to use in the machine can be used. Here port number 6379 is used, which is the default port number for Ray. To use Ray, the last step consists of setting the variable use_library in the notebook to ‘ray’. As an alternative to Ray, Pathos is a library that distributes processing across multiple nodes and provides other convenient improvements over Python’s built-in tools. To use Pathos, no additional setup is required beyond setting the variable use_library to ‘pathos’. If the user does not want to use any of the above-mentioned libraries, this should be specified by setting the variable use_library to ‘none’. The simulation is performed by filling in values for these variables in the following command and executing it: To ensure the simulation results are both reproducible and robust to minor changes in initial conditions, CBGTPy offers control over the pseudorandom number generator seed. The random seed controls the initial conditions of the network, including precisely which neurons connect together, under the constraint of the given connection probabilities and the baseline background activity levels of the CBGT nuclei. The simulation returns a results object, which is a dictionary containing all the data produced by the model, typically organized into data frames. Extract relevant data frames Once the results object has been returned, specific variables and tables of interest can be extracted (see S1 Fig). All the variables available can be listed by accessing the keys of the variable results, which is done by executing the following command: All environmental variables passed to the simulation can also be accessed here for cross-checking. Some additional data frames related to the simulation are also returned. One of these data frames is results[0][""popfreqs""], which returns the population firing rate traces of all nuclei, with each neuronal subpopulation as a column and each time bin of simulated time as a row (see S2 Fig). This data frame can be addressed directly by executing its name in a command line. Another relevant data frame is datatables[0], which contains a list of chosen actions, optimal actions, reward outcomes, and decision times for all of the trials in the simulation (see S3 Fig). When running multiple simulations in parallel (i.e., num_sims > 1), datatables[i] is returned, where i indicates the corresponding thread. This data frame can be extracted by first executing the command and next typing datatables[i] on the command line, to access the results of the ith simulation. As part of the model’s tuning of dopamine release and associated dopamine-dependent corticostriatal synaptic plasticity, the model maintains Q-values for each action, updated according to the Q-learning rule (details in S2 Appendix). These values are available in results[""Q df""], where each column corresponds to one of the possible choices (see S4 Fig). These data frames are designed for easy interpretation and use in later data processing steps. It should be however noted that while Q-values are updated and maintained by CBGTPy, the q-values do not influence the selection of decision choice. The selection of decision choice is solely dependent on the corticostriatal weights. CBGTPy also provides a function to extract some of these data frames in a more processed form. The specific command for the n-choice task is given by  where firing_rates provides a stacked up (pandas command melt) version of the results[0][""popfreqs""] that can be used in the seaborn.catplot() plotting function, reward_q_df compiles data frames for reward and q-values, performance returns the percentage of each decision choice, rt_dist returns the reaction time distribution for the simulation, and total_performance compares the decision and correctdecision in datatables[i] and calculates the performance of the agent. Depending on the experiment choice, the function returns relevant data frames. Note that for the stop signal task, the data frames returned are just firing_rates and rt_dist. The time-dependent values of the recorded variables can also be extracted for both the n-choice task and the stop signal task using the following command: Presently, for the n-choice task, CBGTPy only allows recording the variable weight or optogenetic_input. The former can be used to track the evolution of corticostriatal weights during a n-choice experiment. The variable optogenetic_input can be recorded and plotted to check if the optogenetic input was applied as intended to the target nuclei. The list of variables to be recorded should be specified in the configuration variable. In the example of the configuration variable used above, both weight and optogenetic_input are recorded.: An example of plotting these data frames is included in the example python notebook in the GitHub repository. In addition, for the stop signal task, CBGTPy also allows the recording of the variable ""stop_input"", which can be used to check if the stop signal inputs were applied correctly to the target nuclei. Save variables of interest as pickle files All the relevant variables can be compiled together and saved in a single pickle file. Pickle files provide a method for saving complex Python data structures in a compact, binary format. The following command saves the object results with additional data frames of popfreqs and popdata into a pickle file network_data in the current directory: Basic plotting functions (plot firing rates and reward data frame) CBGTPy provides some basic plotting functions. The firing_rates data frame from the above functions can be passed to function plot_fr, which returns a figure handle that can be saved (see Figure 3), as follows: In addition to the firing_rates data frame, the plotting function plot_fr requires the datatables (also extracted along with firing_rates data frame), the original results variable, experiment_choice and display_stim. The experiment_choice ensures that relevant nuclei are plotted and the display_stim is a boolean variable that can be set to True/False. When set to True, the stimulation information (e.g., optogenetic or stop signal) is indicated over all trials during which the stimulation was applied. Note that, for a longer simulation, this may slow the plotting function, because the function checks if a stimulation is applied on every trial before indicating the result in the figure. The stop signal application is indicated as a bright horizontal red bar above firing traces of the stimulated nuclei (e.g., Fig 7). The optogenetic stimulation is indicated as a blue bar for excitatory stimulation and yellow for inhibitory stimulation (e.g., Fig 8). The reward and Q-values data frame can be plotted with the function plot_reward_Q_df as follows; note that a figure is shown in the example notebook: Shut down Ray server In case the Ray server was selected as the multiprocessing library to use to run the simulation, when the user is done working with the network, the Ray server can be shut down via the terminal with the command or, if the processes have not all been deactivated, with the command User level modifications CBGTPy allows for modifications of several parameters that a user can easily perform. All the parameters in the configuration variable can be modified. A modified value is usually in the form of a data frame or a dictionary. The underlying function (ModifyViaSelector) in frontendhelpers.py iterates through all the features listed in the data frame/dictionary and updates the default values with the new values passed to the configuration variable. If the user wants to use the default value of a parameter, it is essential to specify the parameter value as None, as also shown in Section 2.2. We can subdivide the parameters that can be modified into two major classes, which we will discuss separately: a) parameters related to the agent, which we call the agent parameters (Section 2.3.1); b) parameters related to the experimental environment, which we call the environmental parameters (Section 2.3.2). Through the adjustment of the appropriate parameters, the user can adapt the model to study a variety of important scientific questions. For example, one could introduce new neural pathways and vary their connectivity to study the effects on the system’s dynamics and behavior (e.g., addition of a cortico-pallidal pathway, which may complement stopping mechanisms in the BG pathways). Alternatively, the user could study the effects of specific neural parameters on decision-making and stop signal tasks. Additionally, by introducing optogenetic stimulation to different populations, such as dSPN and iSPN striatal populations, the user could model how the timing and intensity of this stimulation influences the plasticity and learning processes. For example, it has been shown that inhibition of dSPNs during learning impairs performance in a goal-directed learning task. CBGTPy allows stimulation of multiple populations at different phases of a task, which enhances the options for exploring possible functional pathways and their roles in task performance. Lastly, CBGTPy allows many environmental parameters to be modified for an n-choice or stop-signal task while simulating the activity of the CBGT network. For example, one could test the idea that the slowing down of decision times in healthy humans when there is a high conflict or similarity between choices is related to increased activity in the STN. Taken together, the large number of both network and environmental parameters available for the end user to control is a strength of the CBGTPy framework and greatly increases its ultimate scientific utility. A detailed list of features of the CBGTPy package that can be easily modified by the user can be found in table S1 Table. Agent parameters General neuron parameters The parameters common to all neurons can be modified using the params field in the dictionary configuration. A complete list of editable neuronal parameters is listed in S2 Table. For example, the following dictionary entry can be modified to change the capacitance (C) of all neurons: Population-specific neuronal parameters The neuronal parameters of a specific population can be modified using the field pops in the dictionary configuration. These parameters will override the default values set by the params field. A complete list of editable parameters is given in S3 Table. In the following example, the membrane time constant (Taum) of the neurons in the FSI population is specified: Synaptic parameters The parameters of the synapses (GABA, AMPA or NMDA) can be modified through the field receps in the configuration variable. A complete list of editable synaptic parameters is given in S4 Table. In the following example, the membrane time constants of AMPA and GABA synapses (Tau_AMPA, Tau_GABA) are specified: Population-specific baseline input parameters Each neuron receives a background input from a random Gaussian process with a specified mean frequency and variance equal to 1. Depending on the nature of the background input, the Gaussian process can be excitatory (AMPA and NMDA) or inhibitory (GABA). The user can specify the mean frequency, the efficacy, and the number of connections from the background Gaussian process to the neurons in the population with the dictionary base. A complete list of editable input parameters is given in S5 Table. In the following example, the frequency of the external AMPA inputs (FreqExt_AMPA) applied to FSI neurons is specified: Dopamine-specific parameters The dopamine-related parameters can be modified via the dpmns parameter, which takes as its input a data frame containing the field name and the names of the parameters to be updated. A complete list of these editable parameters is given in S6 Table. In the following example, the dopamine decay rate (dpmn_tauDOP) is specified: SPN-specific dopaminergic parameters The SPN-specific dopaminergic parameters for corticostriatal projections to dSPNs and iSPNs can be modified via d1 and d2, respectively, which take as their input a data frame containing the field name and the parameters to be updated. The complete list of these editable parameters is given in S7 Table. In the following example, the learning rate (dpmn_alphaw) and the maximal value for the corticostriatal weights (dpmn_wmax) are specified: New pathways The parameters of a specific pathway can be changed by using the variable newpathways. This variable can also be used to add new connections. This takes as its input a data frame that lists the following features of the pathway: source population (src), destination population (dest), receptor type (receptor), channel-specific or common (type), connection probability (con), synaptic efficacy (eff) and the type of the connection (plastic), which can be plastic (True) or static (False). An example of a cortico-pallidal pathway involving AMPA synapses with 50% connection probability, synaptic strength 0.01, and no plasticity is presented in the following dictionary entry: If the user wishes to change multiple pathways at once, then the variable can be given a list of data frames as input. Q-learning process CBGTPy uses Q-learning to track the internal representations of the values of the possible choices, which depend on the rewards received from the environment. The parameters of this process can be modified via the variable Q_support_params. The two parameters that can be modified are C_scale and q_alpha. The former controls the scaling between the change in phasic dopamine and the change in weights, and the latter controls the change in choice-specific q-values with reward feedback from the environment. An example of how to specify these two parameters is presented in the following dictionary entry: The equations showing the roles of these parameters are described in detail in S2 Appendix. Q-values data frame The choices available to the agent can be initialized with identical values (e.g., 0.5) representing an unbiased initial condition. Alternatively, non-default values for the Q-values data frame (such as values biased towards one choice) can be initialized using the variable Q_df. An example of how to specify this variable is presented in the following dictionary entry: Note that this parameter should be updated according to the number of choices specified in the variable number_of_choices. The above example shows an initialization for a 2-choice task. Cortical activity In addition to the background inputs that generate the baseline activity of all of the CBGT nuclei, the cortical component provides a ramping input to the striatal and thalamic populations, representing the presence of some stimulus or internal process that drives the consideration of possible choices. The maximum level of this input can be defined by the parameter maxstim and specified using the following dictionary line: Corticostriatal plasticity The corticostriatal plasticity in the n-choice experiment can be switched on and off using the corticostriatal_plasticity_present parameter. When it is set to True, the corticostriatal weights change based on rewards and dopaminergic signals (for more details see S2 Appendix). When it is set to False, the simulation proceeds without any update in the corticostriatal weights and the Q-values. The value of this parameter is set to True using the following dictionary line: Sustained activation to the action channel for the selected choice In order to resolve the temporal credit assignment problem, we rely on post-decision sustained activation to keep the selected channel active during the phasic dopaminergic activity. After the choice has been made, following the onset of a trial (decision phase), the cortical component of the action channel associated with the selected choice continues to receive inputs, while the unselected channels do not (consolidation phase); see Figure 3. This phase may also represent the movement time of the agent. The assumption here is that this activation provides an opportunity for corticostriatal plasticity that strengthens the selected choice. The parameter sustainedfraction is the fraction of input stimulus maintained during the consolidation phase in the cortical channel corresponding to the action selected by the agent, and it can be specified using the following dictionary entry: Once the dopamine signal has been delivered at the end of the consolidation phase, all cortical inputs are turned off for the inter-trial interval. See Section 3.1 and Fig 4 for more details on the trial phases. Thalamic threshold In the default set-up, when the thalamic firing rate of either choice reaches 30 Hz, that choice is selected. This threshold can be specified by the user by setting the parameter thalamic_threshold in the following way: Environment parameters Experiment choice The parameter experiment_choice is set at the beginning of the simulation (see Section 2.2). It also needs to be sent as a configuration variable, so that the specific functions and network components relevant to the appropriate experiment are imported. Inter-trial interval The parameter inter_trial_interval allows the user to specify the inter-trial interval duration. The inter-trial interval also corresponds to the duration of the inter-trial-interval phase of the simulation, where the network receives no external input and shows spontaneous activity. When no value is specified (None), a default value of 600 ms is used. The user can set the value of this parameter using the following dictionary entry: Movement time After a choice is made, the chosen action channel receives sustained activation at some fraction (with a default value of 70%) of the initial cortical input strength. As noted in the previous section, this phase of the simulation (consolidation phase) represents the movement time, which is distinct from the reaction time, provides a key window for corticostriatal synaptic plasticity to occur, and remains unaffected by the selected choice. The length of this phase can be controlled with the parameter movement_time. The default value of the movement time (when this parameter is set to None) is sampled from a normal distribution . However, the user can choose to set it to a constant value by passing a list [""constant"", N], where N represents the constant value of movement time for all trials. The other option is to sample from a normal distribution of specified mean N using [""mean"", N]. The movement time can be set to a fixed value as follows: Choice time out The parameter choice_timeout controls the duration of the time interval in which a choice can be made. The default value of this parameter (when it is set to None) is 1000 ms. This parameter can be changed as follows: Choice labels The data frame channels allows the labels for the action channels to be changed. The new labels can be used to access information about the action channels. An example is shown below: Note that this parameter should be updated according to the number of choices specified in the variable number_of_choices. The above example shows an initialization for a 2-choice task. Number of trials The n_trials parameter sets the number of trials to be run within a simulation. Note that this number should be greater than the volatility parameter (described in the following paragraph). However, if only 1 trial is to be simulated, then volatility parameter should be set to None. Examples of how to set this parameter are as follows: For more details about setting volatility parameter, please refer to the following paragraph. Volatility The parameter volatility indicates the average number of trials after which the reward contingencies switch between the two choices. The volatility parameter is a list consisting of two values, [λ, ‘option’], where option can be set as exact or poisson. The λ parameter generates a reward data frame where the reward contingency changes after an average of λ trials. The option exact ensures that the reward contingency changes exactly after λ trials whereas the option poisson samples the change points from a Poisson distribution with parameter λ. However, note that this parameter cannot be 0 or the total number of trials. To perform a simulation in which the reward contingencies do not change until the end of the simulation, set this parameter to n_trials–1 and drop the last trial from the analysis. An example of how to define and specify the volatility is shown in the following command line: Note that for a 1-choice task or stop signal task, the volatility parameter is not applicable and hence should be defined differently; specifically, the parameter λ should be set to None as follows: Reward probability The parameter conflict represents the reward probability of the reward data frame and is defined as a tuple of reward probabilities for the n choices. In the following example, for a 2-choice task, the first reward probability corresponds to the first choice listed in the channels parameter (e.g., ""left""). The reward probabilities for the choices are independent, thereby allowing reward structure to be set in the format (p1, p2) as in the two following examples, representing unequal and equal reward probabilities, respectively: Note that this parameter should be updated according to the number of choices specified in variable number_of_choices. The above example shows an initialization for a 2-choice task. For example, for a 3-choice task the reward probabilities can be defined as: Reward parameters The trial-by-trial reward size is generated by a random Gaussian process, with a mean (reward_mu) and a standard deviation (reward_std) that can be assigned. To simulate binary rewards, choose mean = 1 and standard deviation = 0, as follows: Optogenetic signal If the experiment requires an optogenetic signal to be applied, then this should be indicated in the configuration variable by setting opt_signal_present to True, with each boolean variable corresponding to each nucleus specified in the list of populations to be stimulated, as shown below: Optogenetic signal probability The opt_signal_probability parameter accepts either a float or a list. The float represents the probability of the optogenetic signal being applied in any given trial. For example, the user wants all the trials in the simulation to be optogenetically stimulated, i.e opt_signal_probability = 1.0, it should be defined as shown below: Alternatively, a specific list of trial numbers during which optogenetic stimulation should be applied can also be passed. An example is shown below: In the above example, a list of trial numbers [0, 1] indicates that the optogentic stimulation is applied to trial numbers 0 and 1. Please note that if more than one nucleus will be stimulated, the opt_signal_probability expects a list of floats (probabilities) or list of list (list of trial numbers). For example, as mentioned in an example above, say the user wants to stimulate two populations (dSPN and iSPN) at trial numbers [0,1] and [1,2] respectively: Optogenetic signal amplitude The amplitude of the optogenetic signal can be passed as a list of floats to the parameter opt_signal_amplitude. A positive value represents an excitatory optogenetic signal, whereas a negative value represents an inhibitory optogenetic signal. An example of excitation is shown below: If we want to send different amplitudes of optogenetic signals to different populations, for example, an excitatory (0.3) to dSPNs and inhibitory (−0.25) to iSPNs, then the amplitudes should be specified as : "" opt_signal_present "": [ True , True ], .. "" opt_signal_population "": ["" dSPN "", "" iSPN ""], "" opt_signal_amplitude "": [0.3 , −0.25], Optogenetic signal onset The opt_signal_onset parameter sets the onset time for the optogenetic signal. The onset time is measured relative to the start of the decision phase. For example, to specify that optogenetic stimulation will start 20 ms after the decision phase begins, the appropriate command is: Optogenetic signal duration The duration for which the optogenetic signal is applied can be controlled by the parameter opt_signal_duration. This parameter accepts a numerical value in ms as well as phase names as strings. For example, to apply the optogenetic stimulation for 1000 ms after the signal onset, the command is: However, in order to apply optogenetic stimulation during the whole decision phase, the duration variable should be the string “phase 0” as shown below: This allows the user to specifically target decision (“phase 0”), consolidation (“phase 1”) and inter-trial interval (""phase 2"") phases with optogenetic stimulation. Optogenetic signal channel The user can also control whether the optogenetic signal is applied globally to all action channels (all), to a randomly selected action channel (any), or to a specific action channel (for instance, left). To do so, the parameter opt_signal_channel needs to be specified as in the example below: Optogenetic signal population The optogenetic stimulation can be applied to a single or multiple populations in the same simulation. In either case, the population names should be defined as a list. The target population can be specified using the parameter opt_signal_population. In the example below, the dSPN population is set as the target population: Although the optogenetic-related parameters can be used to mimic the effect of a stop signal manifested as the application of a step input current to a population, the network also has a number of modifiable parameters that are specific to injecting the stop signal to target nuclei via a box-shaped current. Recorded variables CBGTPy allows recording of time-dependent values of both the corticostriatal weights and any optogenetic inputs to any CBGT nuclei that are being stimulated by using the parameter record_variables. The first component of record_variables can be used to track the evolution of the weights from cortex to dSPNs or iSPNs during an n-choice experiment. The latter component records the optogenetic input applied to the target population and is especially useful for debugging purposes. Both of these variables can be extracted as a data frame by calling the function extract_recording_variables as described in Section 2.2. Note that, for the parameter weight, cortical weights to both dSPNs and iSPNs for all choice representations (channels) are recorded. In addition, when running the stop signal task, the stop signal inputs can be recorded as well. Here, we can see an example of how to extract the weights and the optogenetic input: In addition, for the stop signal task, CBGTPy also allows the recording of the variable ""stop_input"", which can be used to check if the stop signal inputs were applied correctly to the target nuclei. Stop signal If the experiment requires the stop signal to be applied, then this option should be selected in the configuration variable by setting stop_signal_present to True. Different stop signals can be applied to different target populations during the same execution. For these, the stop_signal_present variable is defined as a list whose length depends on the number of target populations. For example, if we apply stop signals to two different populations, we have to set this variable as follows: Note that the user can apply the stop signals to as many nuclei as desired. Stop signal populations The target populations for the stop signal can be specified using the parameter stop_signal_population. In the example below, the STN and GPeA populations are set as the target populations: All the examples presented below corresponding to the stop signal task are designed taking into account that the stop signal is injected into these two populations. Stop signal probability The stop signal probability can be specified using the parameter stop_signal_probability, which is a list whose entries can take a float or a list as input. If a float (between 0 and 1) is introduced, then this value represents the probability to which the stop signal is applied. These trials are picked randomly from the total number of trials. Alternatively, if a sublist is specified, then it must contain the numbers of those trials where the user wants the stop signal to be applied. Note that the entries in the main list refer to the populations specified in the same order as in the variable stop_signal_populations. For example, within the statement  the first float value represents a 100% probability of applying the “first” stop signal to the first specified nucleus (STN), while the subsequent list of values represents the numbers of the trials on which the “second” stop signal will be applied to the other target region (GPeA). Stop signal amplitude The amplitude of the stop signal can be passed as a float to the parameter stop_signal_amplitude. Note that this parameter is a list and that every value refers to the corresponding population. The order should follow the order of the populations. For example, Stop signal onset The parameter stop_signal_onset sets the times when the stop signals are injected into the target nuclei. The onset time is measured with respect to the start of the decision phase. In the example proposed below, the stop signal stimulation at the STN starts 30 ms after the decision phase begins, while that applied to the GPeA starts 60 ms after the decision phase begins: Stop signal duration How long each stop signal is maintained can be controlled using the parameters stop_signal_duration. As in the previous cases, this is a list whose order must follow that of the target populations. In the following example, a stop signal lasting 100 ms is applied to the STN while another with duration 160 ms is applied to the GPeA: In order to apply the stop signal throughout an entire phase, the duration variable should be set to a string containing the name of the phase when the user wants to apply the stop signal, as shown below: This allows the user to specifically target decision (“phase 0”), consolidation (“phase 1”) and inter-trial interval (“phase 2”) phases with persistent stop signal stimulation. Stop signal channel The user can also control if the stop signal applied to a specific population is presented to all action channels (all), to a uniformly and randomly picked action channel (any), or to a specific action representation (for instance, left). These can be specified using the parameter stop_signal_channel. In the following example, one stop signal is presented to the STN populations of all of the action channels, while the second stop signal is applied only to the GPeA population corresponding to the “left” action channel:","In this section, we present some details about examples of the two primary experiments that CBGTPy is designed to implement. An n-choice task in an uncertain environment This task requires the agent to select between n choices (e.g., left/right in a 2-choice task). The selection of each choice leads to a reward with a certain probability. Moreover, the reward probability associated with each choice can be abruptly changed as part of the experiment. Thus, there are two forms of environmental uncertainty associated with this task: a) conflict, or the degree of similarity between reward probabilities; and b) volatility, or the frequency of changes in reward contingencies. Higher conflict would represent a situation where the reward probabilities are more similar across choices, making detection of the optimal choice difficult, whereas a lower conflict represents highly disparate values of reward probabilities and easier detection of the optimal choice. Conflict is not specified directly in CBGTPy; rather, the reward probabilities are explicitly set by the user (Section 2.3.2). An environment with high (low) volatility corresponds to frequent (rare) switches in the reward contingencies. The volatility can be set by the parameter λ, which determines the number of trials before reward probabilities switch. The user can choose between whether the trials are switched after exactly λ trials or whether switches are determined probabilistically, in which case λ represents the rate parameter of a Poisson distribution that determines the number of trials before reward probabilities switch (Section 2.3.2). Using the reward probabilities and volatility, the backend code generates a reward data frame that the agent encounters during the learning simulation. The reward data frame is used in calculating the reward prediction error and the corresponding dopaminergic signals, which modulate the plasticity of the corticostriatal projections. At the beginning of the simulation, the CGBT network is in a resting phase during which all CBGT nuclei produce their baseline firing rates. When a stimulus is presented, the network enters a new phase, which we call phase 0 or decision phase. We assume that at the start of this stage a stimulus (i.e., an external stimulus, an internal process, or a combination of the two) is introduced that drives the cortical activity above baseline. Cortical projections to the striatal populations initiate ramping dynamics there, which in turn impacts activity downstream in the rest of the BG and Th. We also assume that when the mean firing rate of a thalamic population exceeds a designated threshold value of 30 spikes per second (the so-called decision boundary), the CBGT network, and hence the agent, has made a choice. This event designates the end of phase 0, and the duration of this phase is what we call the reaction time. If a decision is not made within a time window Δmax ms after the start of the phase, then we say that none of the available choices have been selected and the decision is recorded as “none”. Such trials can be excluded from further analysis depending on the hypothesis being investigated. To allow for selection between n different choices we instantiate n copies of all CBGT populations except FSI and CxI. This replication sets up action channels representing the available choices that can influence each other indirectly through the shared populations and otherwise remain separate over the whole CBGT loop. To distinguish between the firing ratse of the populations within channels, we will call them Popi, where Pop refers to the corresponding CBGT region and i refers to the channel name (e.g., Cxleft, Cxright for n = 2). The presentation of a stimulus to the cortical population is simulated by increasing the external input frequency in all copies of the cortical Cx populations that ramp to a target firing rate . The ramping current  is calculated as  where  is the integrator time step, and the external input frequency also changes according to  After a Th population reaches the threshold and hence a decision is made, the ramping input to Cx is extinguished and a subsequent period that we call phase 1 or consolidation phase begins, which by default has duration sampled from a normal distribution  but can also be fixed to a given duration. This phase represents the period of the motor implementation of the decision. Activity during phase 1 strongly depends on what happened in phase 0 such that, if a decision i occurred at the end of phase 0, then Cxi will be induced to exhibit sustained activity during phase 1 (see S2 Appendix), while in any non-selected action channels, the cortical activity returns to baseline. If no decision has been made by the network (within a time window Δmax ms, with a default value of 1000 ms), then no sustained activity is introduced in the cortex (see Figure 3, 3rd trial, the top left subplot showing cortical activity). Finally, each trial ends with a reset phase of duration 600 ms (although this can be adjusted by the user), which we call phase 2 or the inter-trial interval phase, when the external input is removed and the network model is allowed to return to its baseline activity, akin to an inter-trial interval. A visualization of the decision phases is shown in Figure 4, where two different options, right and left, are considered. The blue trace represents the thalamic activity for the right channel, Thright, while the orange trace represents that for the left channel, Thleft. At the end of phase 0, we can see that Thleft reaches the decision threshold of 30 spikes per second before Thright has done so, resulting in a left choice being made. During phase 1, the Thleft activity is maintained around 30 spikes per second by the sustained activity in Cxleft. In this task, critical attention should be paid to phase 0, as this represents the process of evidence accumulation where the cortical input and striatal activity of both channels ramp until one of the thalamic populations’ firing rates reaches the threshold of 30 Hz. To be largely consistent with commonly used experimental paradigms, the maximal duration of this phase is considered to be Δmax = 1000 ms such that, if the agent makes no decision within 1000 ms, the trial times out and the decision is marked as “none”. These trials can be conveniently removed from the recorded data before analysis. If a decision is made, then the simulation proceeds as though a reward is delivered at the end of phase 1 – that is, at the end of the motor sensory response – such that phase 1 represents the plasticity phase, where the choice selected in phase 0 is reinforced with a dopaminergic signal. During this phase, the cortical population of the selected channel receives 70% of the maximum cortical stimulus applied during the ramping phase, although the user can change this percentage. This induces sustained activity that promotes dopamine- and activity-dependent plasticity as described in S2 Appendix. The activity-dependent plasticity rule strengthens (weakens) the corticostriatal weight to dSPNs (iSPNs) of the selected channel when dopamine rises above its baseline level. CBGTPy allows for the specification of other parameters such as learning rate, maximum weight values for the corticostriatal projections and dopamine-related parameters (see details with examples in Section 2.3). At the beginning of the simulation, with baseline network parameters, the selection probabilities are at chance level (i.e., 50% for a 2-choice task). If the network experiences rewards, however, the dopamine-dependent plasticity strengthens the corticostriatal projection to the dSPN population of each rewarded choice, thereby increasing the likelihood that it will be selected in the future. CBGTPy allows for probabilistic reward delivery associated with each option, as well as switching of these probabilities between the two actions (Figure 5). When such a change point occurs, the previously learned action now elicits a negative reward prediction error, forcing the network to unlearn the previously learned choice and learn the new reward contingency. A stop signal task The stop signal task represents a common paradigm used in cognitive psychology and cognitive neuroscience for the study of reactive inhibition. In this task, participants are trained to respond as fast as possible after the presentation of a “Go” cue. Sometimes the “Go” cue is followed by the presentation of a “Stop” cue, which instructs subjects to withhold their decision and hence, if successful, prevents any corresponding movement before it begins. Imaging and electrophysiological studies in humans, rodents, and monkeys agree in reporting that STN neurons become activated in response to a stop signal, providing a fast, non-selective pause mechanism that contributes to action suppression through the activation of the cortical hyperdirect pathway. However, this mechanism, by itself, mostly fails to inhibit locomotion, appearing to be not selective and long-lasting enough to prevent a late resurgence of the evidence accumulation process as needed to guarantee a complete cancellation of the execution of the motor response. A complementary slower but selective mechanism is thought to be provided by the activation of arkypallidal neurons in the GPe in response to an external stimulus that instructs the network to brake the ongoing motor planning process. According to this idea, a long-lasting action inhibition results from the activation of pallidostriatal GABAergic projections. For more details on how this mechanism takes place see. To reproduce these mechanisms and to simulate the interruption of the action selection process, we inject two independent, external, excitatory currents directly into STN and GPeA neurons during a typical CBGT simulation. This choice is based on the findings of Mallet et. al. (2016). The stop signal is excitatory and hence is simulated by up regulating the baseline input frequency to the AMPA receptors  where stop_amplitude defines the magnitude of the stop signal stimulation. The currents injected as a step function cause an increase in the firing rates of the target nuclei. Both of these external currents are defined using parameters that can be modified in an easy and user-friendly way, without requiring any familiarity or advanced knowledge of the details of the implementation (see Section 2.3 for further details and examples). The following list of parameters characterizes each one of these currents: amplitude, specifying the magnitude of the stimulation applied; population, specifying the CBGT region or sub-population being stimulated. onset, specifying the onset time of the stimulation, with respect to the trial onset time (i.e., the beginning of phase 0); duration, defining the duration of the stimulation. Since the length of phase 0 is not fixed and is dependent on how long it takes for the thalamic firing rates to reach the decision threshold (30Hz), this parameter should be set carefully; probability, determining the fraction of trials on which the stop signal should be introduced; channel, defining which action channels are stimulated. A more detailed description of all of these parameters can be found in Section 2.3.2. The details of the stop parameters used to reproduce Figure 7 are included in S9 Table. The characterization of the different network phases described in Section 2 slightly changes when performing the stop signal task (see Figure 6). During the decision phase (phase 0, which in this task lasts for a maximum of 300 ms) the stop signals are directly presented to the target populations by injecting independent external currents. The user can choose the moment of the injection by manipulating the variable stop_signal_onset_time. These signals are kept active for a period equal to stop_signal_duration, with a magnitude equal to the stop_signal_amplitude. These values do not need to be the same for all of the stop signals used. At this stage, two possible outcomes follow: (a) despite the presentation of the stop signals, the network still manages to choose an action; or, (b) the network is not able to make an action after the presentation of the stop signals, and phase 0 ends with no action triggered, which represents a stop outcome. The former option could arise for various reasons; the strength of the stop signal may not be sufficient to prevent the network from triggering an action or the evaluation process may still have enough time to recover, after the stop signal ends, to allow the thalamic firing rates to reach the decision threshold (e.g., 30 Hz) within the permitted decision window. In Figure 7 we show an example of stop signal stimulation applied to STN and GPeA populations, independently, in a 1-choice task. The onset of the stimulation applied to STN occurred at 30 ms while that for the stimulation of GPeA was set to 60 ms; both signals were applied for a duration of 145 ms. Both stimulations were applied in both of the trials shown. Note that trial number 1 corresponds to a correct stop trial (no decision was made within phase 0 ), whereas the following trial corresponds to a failed stop trial. These outcomes can be inferred from the activity of various populations at the end of the decision making windows: the thalamic firing rate reaches a higher level there and the firing traces in GPi decrease more for the failed stop trial than for correct stop, while cortical activity is sustained beyond this time specifically when stop fails. Optogenetic stimulation CBGTPy also allows for simulation of optogenetic stimulation of CBGT nuclei while an agent performs the available tasks (stop signal or n-choice). Optogenetic stimulation is implemented by setting a conductance value for one of two opsins dependant upon the mode of stimulation, channelrhodopsin-2 for excitation and halorhodopsin for inhibition. The excitatory or inhibitory optogenetic input is applied as a current  added to the inward current  of all neurons in a nucleus or subpopulation during a typical CBGT simulation such that  where the conductance  is a signed value entered via the configuration variable in the notebooks. The reversal potential of channelrhodopsin () was considered to be 0 mV and halorhodopsin () was considered to be −400 mV. The stimulation paradigm includes the following parameters: amplitude, the sign of which specifies the nature (positive→excitatory / negative→inhibitory) and the absolute value of which specifies the magnitude of the conductance applied; population, specifying the CBGT region or sub-population being stimulated; onset, specifying the onset time of the stimulation; duration, defining the duration of the stimulation; probability, indicating the fraction of trials or a list of trial numbers to include stimulation; channel, specifying which action channels are stimulated. The parameter population should be entered as a list of the subpopulations to be stimulated. The parameter onset is calculated from the beginning of phase 0 ; for example, if this parameter is 10, then the optogenetic stimulation starts 10 ms after phase 0 starts. The parameter duration controls the duration of the optogenetic stimulation. This parameter either accepts a numeric value in ms or a string specifying which phase should be stimulated. The numeric value stipulates that the list of selected populations will be stimulated from the specified onset time for the specified time duration. The string (e.g., “phase 0”) stipulates that the stimulation should be applied throughout the specified phase, thereby allowing the user to specifically target the decision, consolidation or inter-trial interval phase. If an optogenetic configuration results in extending the duration of a phase (e.g., strongly inhibiting dSPN may extend phase 0 ), a time out is specified for every phase to prevent a failure to terminate the phase. The default timeouts for phase 0, 1 and 2 are 1000 ms, 300 ms and 600 ms respectively unless specified by the user. The parameter probability offers the flexibility of either assigning a number that determines the fraction of trials (randomly sampled from the full collection of trials) on which the stimulation is to be delivered or else entering a list of specific trial numbers. Lastly, the parameter channel specifies the name of the action channel, such as “left”, onto which the stimulation should be applied. This parameter also accepts two additional options, “all” or “any”, the former of which leads to the application of a global stimulation to the same population in all channels and the latter of which randomly selects a channel for stimulation on each trial. The details of the optogenetic input is included in S10 Table. We show an example of optogenetic stimulation applied to a list of iSPN and dSPN populations in a 3-choice task (Figure 8). The excitatory stimulation (shown as thick blue bar), with an amplitude of 0.1, was applied to the iSPN populations of all the channels (namely A, B, and C) in the first trial, for the duration of phase 0. An increased activity in the iSPN population (activation of the indirect pathway) caused a choice time out on this trial. In the subsequent second trial, an inhibitory stimulation with an amplitude of −0.5 was applied to the dSPNs (shown as yellow bar) for 400 ms. This resulted in brief but strong inhibition of dSPNs (direct pathway), thereby delaying the action selection. This can be observed by comparing the durations of the decision phase between the second and third trials, where no such manipulation was imposed.",,"Here we introduce CBGTPy, an extensible generative modeling package that simulates responses in a variety of experimental testing environments with agent behavior driven by dynamics of the CBGT pathways of the mammalian brain. A primary strength of this package is the separation of the agent and environment components, such that modifications in the environmental paradigm can be made independent of the modifications in the CBGT network. This allows the user to derive predictions about network function and behavior in a variety of experimental contexts, which can be vetted against empirical observations. Moreover, various changes in the parameters of the network, as well as the experimental paradigm, can be made through the higher-level configuration variable that is sent as an argument in running the simulation, thereby avoiding a considerable coding effort on the part of the user. CBGTPy also returns behavioral outcomes (e.g., choices made and decision times) and “recordings” of neuronal outputs (instantaneous firing rates) for all of the CBGT nuclei in the form of easily usable and readable data frames. Overall, CBGTPy allows for theorists and experimentalists alike to develop and test novel theories of the biological function of these critical pathways. The individual components of CBGTPy are all designed to enable maximum flexibility. The basal ganglia model is constructed in an organized series of steps, beginning with high-level descriptions of the model and gradually providing more fine-grained details. Developing a modification to the network becomes a matter of inserting or modifying the appropriate components or steps, allowing high-level redesigns to be implemented as easily as more precise low-level modifications. CBGTPy’s high degree of extensibility can, in large part, be attributed to its use of a data-flow programming paradigm. Neural pathways between major populations, for example, can be specified at a very high level, requiring only a single entry in the pathway table to describe how each subpopulation is connected. If the connectivity of a particular subpopulation, or even a particular neuron, needs adjustment, then the later steps in network construction can be adjusted to implement those changes. CBGTPy was designed with this degree of flexibility to ensure that in the future, more complex biological models of the CBGT network can be developed and implemented in an efficient manner. Of course, CBGTPy is not the only neural network model of these cortical-subcortical networks. Many other models exist that describe the circuit-level dynamics of CBGT pathways as either a spiking or a rate-based system. CBGTPy has some limitations worth noting, such as not being as computationally efficient as rate-based models in generating macroscale dynamics, including those observed using fMRI or EEG, and associated predictions. Also, the properties of the cortical systems modeled in CBGTPy are quite simple and do not capture the nuanced connectivity and representational structure of real cortical systems. For these sorts of questions, there are many other modeling packages that would be better suited for generating hypotheses (e.g.,). Where CBGTPy excels is in its a) biologically realistic description of subcortical pathways, b) scalability of adding in new pathways or network properties as they are discovered, c) flexibility at emulating a variety of typical neuroscientific testing environments, and d) ease of use for individuals with relatively limited programming experience. These benefits should make CBGTPy an ideal tool for many researchers interested in basal ganglia and thalamic pathways during behavior. One issue that has been left unresolved in our toolbox is the problem of parameter fitting. Spiking network models like those used in CBGTPy have an immense number of free parameters. The nature of both the scale and variety of parameters in spiking neural networks makes the fitting problem substantially more complex than that faced by more abstracted neural network models, such as those used in deep learning and modern artificial intelligence. This is particularly true when the goal is to constrain both the neural and behavioral properties of the network. Models like CBGTPy can be tuned to prioritize matching cellular level properties observed empirically (for example see) or to emphasize matching task performances to humans or non-human participants (see). We view this as a weighted cost function between network dynamics and behavioral performance whose balance depends largely on the goals of the study. To the best of our knowledge, there is no established solution to simultaneously fitting both constraints together in these sorts of networks. Therefore, CBGTPy is designed to be flexible to a wide variety of tuning approaches depending on the goal of the user, rather than constrain to a single fitting method. Because our focus is on matching neural and behavioral constraints based on experimental observations, CBGTPy’s environment was designed to emulate the sorts of task paradigms used in systems and cognitive neuroscience research. We purposefully constructed the environment interface to accommodate a wide variety of traditional and current experimental behavioral tasks. These tasks are often simpler in design than the more complex and naturalistic paradigms used in artificial intelligence and, to an increasing degree, cognitive science. Nonetheless, a long-term goal of CBGTPy development is to interface with environments like OpenAI’s Gym in order to provide not only a mechanistic link towards more naturalistic behavior, but also a framework to test hypotheses about the underlying mechanisms of more dynamic and naturalistic behaviors. In summary, CBGTPy offers a simple way to start generating predictions about CBGT pathways in hypothesis-driven research. This tool enables researchers to run virtual experiments in parallel with in vivo experiments in both humans and non-human animals. The extensible nature of the tool makes it easy to introduce updates or expansions in complexity as new observations come to light, positioning it as a potentially important and highly useful tool for understanding these pathways.",10.1101/2023.09.05.556301
PMC10312527,37398321,Visualizing scRNA-Seq Data at Population Scale with GloScope,"Increasingly scRNA-Seq studies explore the heterogeneity of cell populations across different samples and its effect on an organism’s phenotype. However, relatively few bioinformatic methods have been developed which adequately address the variation between samples for such population-level analyses. We propose a framework for representing the entire single-cell profile of a sample, which we call its GloScope representation. We implement GloScope on scRNA-Seq datasets from study designs ranging from 12 to over 300 samples. These examples demonstrate how GloScope allows researchers to perform essential bioinformatic tasks at the sample-level, in particular visualization and quality control assessment.","Single-cell sequencing data has the potential to considerably enhance our comprehension of human health demonstrating how individual cell differences affect disease outcomes. Initially, single-cell sequencing studies examined the scope of cell diversity found in biological systems, including large projects such as the Human Cell Atlas Project. Such studies generally obtain large numbers of cells from few individual donors and focus on the shared cell type diversity. However, an increasing number of scRNA-Seq investigations target patient populations and emphasize the impact of single-cell variation on human health outcomes. These population-based scRNA-Seq studies typically involve scRNA-Seq data from larger cohorts of individuals who are selected from populations exhibiting various health-related phenotypes. Despite the plethora of methodological advancements in scRNA-Seq, most current tools were designed for the goal of understanding the single cell level information and lack appropriate strategies for analyzing scRNA-Seq population studies. Most of the current analyses of population scRNA-Seq data tends to consider the individual cells as the primary data unit. Existing tools that do account for population variability focus on identifying individual genes with differential expression. Beyond differential expression analysis, sample-level analysis that exist are generally limited to comparisons of the relative proportions of different cell-types between groups of samples. We propose an analysis paradigm that uses the entire single-cell profile of a sample instead of focusing on cells as units. We refer to such an approach as a sample-level (or patient-level) analysis. Our proposal is based on representing each sample as a distribution of cells. More specifically, we summarize each sample with a probability distribution describing the distribution of cells and their gene expression within the sample. Such a representation allows us to summarize the entire scRNA-profile of a sample into a single mathematical object. In this way, we synthesize the entire single-cell profile of an individual sample while maintaining information regarding the variability of the single-cells. This global representation, which we call GloScope, can be used in a wide variety of downstream tasks, such as exploratory analysis of data at the sample-level or prediction of sample phenotypes. Moreover, this representation does not require classification of sequenced cells into specific cell-types (e.g. via clustering), and therefore is not sensitive to any auxiliary cell-type identification procedure. We apply the GloScope representation on a variety of published data collected on sample cohorts and demonstrate how the GloScope representation allows for visualization of important biological phenotypes and aids in detection of sample-level batch effects.","The GloScope representation Our GloScope representation consists of representing each sample as a distribution along with a corresponding divergence or distance; we then estimate the distance or divergence between each pair of samples based on their scRNA-Seq data. This representation allows for application of kernel methods common in machine learning, which depend on the calculation of the distance between each pair of samples , . for downstream statistical analysis. To do this, we posit an underlying true distribution of cells  for each sample , which is a continuous probability distribution on , where  is the number of genes. We define a measure of divergence  on the space of probability distributions in . In this work, we fix  as the symmetrized Kullback-Leibler divergence,  which has been used in a similar manner in the case of facial recognition [e.g., ] We do not observe the  directly and must instead estimate that distribution from observed data. The observations from a sample  consists of  sequenced cells; in order to estimate  we will make the simplifying assumption that the sequenced cells are independent and identically distributed (i.i.d) draws from the sample’s full population of cells, . Even with this assumption, density estimation is complicated in this setting. For scRNA-Seq datasets,  is often in the range of 2,000–8,000 (the number of detectable genes given the sequencing depth). The number of cells per sample, , can vary by experiment, and often  ranges lies in the range of 500 to 10,000 cells per sample. The data from each cell is high dimensional and sparse, a distributional structure known to be impactful in the analysis of scRNA-Seq data. Defining a Latent Space Even with several thousand cells per sample, it is infeasible to estimate the density in such a high dimensional space without the assumption of an underlying lower dimensional latent space. Therefore, for each sample  and cell  we model a latent variable  and a transformation . Then our observed vector  of gene expression counts from a cell is assumed drawn from an appropriate generative model for RNA counts with mean parameter , i.e. . For a sample , we assume that the  for each cell  is distributed as the latent random variable . Instead of estimating  in , GloScope instead estimates  in the lower-dimensional space . In Section 2.1, we denote the estimated distribution as  for conceptual simplicity, but a more precise notation would be  to clearly emphasize that we are estimating the distribution on a lower-dimensional space. Furthermore, we note that our above heuristic states that we observe counts  in cell  drawn from a single distribution ; this ignores cell-specific effects that could result in slightly different distributions for different cells, such as different sequencing depth that varies for each cell . The latent variables , however, are independent of the cell-specific effects due to the technology, which makes estimation of a single distribution, , shared by all cells a coherent mathematical framework. Estimation The GloScope representation estimates  for each sample with a two-stage strategy: 1) estimation of the latent variables  for each cell  in sample  and 2) estimation of the density of  from  and corresponding distances  between samples. An advantage of estimating the latent variable samples before the density is that we can apply one of many existing dimensionality reduction techniques that account for sparse count data, such as ZINBWave or scVI, or techniques that simultaneously remove batch effects and estimate a latent space, such as Harmony or fastMnN. The GloScope representation assumes that the user chooses an appropriate method for the first stage estimation of  (i.e. a dimensionality reduction method) and then offers two approaches for the second stage (estimation of the distances between the ). The first approach applies a Gaussian mixture model to the  to estimate , the density associated with the distribution , and then calculates  as our estimate of . Single cell methods utilizing dimensionality reduction, described above, often include a regularizing assumption that the latent variables . This Gaussian regularization in the model and the fact that many datasets are mixtures of cell type populations, motivates our use of Gaussian mixture models (GMMs). We use the R package mclust to implement the GMM estimation. As there is no closed form expression for the KL divergence between GMM distributions, we use Monte Carlo integration to approximate the KL divergence between two GMM densities; this is based on  samples drawn from the estimated GMM distributions, again using the mclust package. Specifically, for  draws of  from , we have  We also provide a second approach that estimates  directly using a k-nearest neighbor approach without explicitly estimating the density . Denote by  the distance from the th cell in sample  to its kth nearest neighbor in sample . Then the KL divergence can be estimated directly as  where d is the dimension of the latent space. We implement this strategy using the FNN package to estimate the symmetrized KL divergence between sample  and sample . Simulating scRNA-Seq data Simulation Model To simulate population-level scRNA-Seq data with which we benchmark our methodology, we follow the model introduced by the muscat R package. We would note that this is a model for simulating count data for each gene, and unlike our GloScope representation does not assume any latent variable representation in generating the data. The muscat package assumes a simple two-group setting in which each sample  may come from one of two groups, denoted by the variable . The  cells from sample  come from  different cell-types with the proportion of cells from cell-type  given by , where . Thus the gene expression vector  of a cell  from sample  is assumed to follow a negative binomial mixture model :  where  is a CDF on  representing a product distribution of independent negative binomials, i.e. each gene’s expression value is independent and follows a negative binomial distribution with mean given by the  the element of the vector  and dispersion parameter . The vector of gene means for cell  in sample  is parameterized in muscat as  where  is the library size (total number of counts);  is the relative abundance of  genes in cells belonging to sample  and cell-type  is the fold-change for genes in cluster k if the sample belongs to group . Notice, as mentioned above, that because of different sequencing depths per cell, each cell within sample  has a different mean  governed by the sequencing-depth parameter , hence our notation  We make adjustments to the above model in the muscat package to more fully explore sample variability. To explore the effect of library size variation at both the cell and sample level, we introduce the decomposition , where  is the overall (average) library size, and  and  are variations from that due to sample or cell level differences, constrained so that . We also adjusted the model to allow sample-specific proportions vectors , with . We define proportions per treatment group, , for treatments , such that  and randomly generate probability vectors  for sample  from a Dirichlet distribution according to its treatment group, , with sample level variation parameter . Selection of Parameters The muscat package also provides methods for creating these many parameters based on a few input parameters by the user and estimating the other parameters based on reference data provided by the user. We followed their strategy, with the following additions. We chose the group fold change difference per cell-type,  following the schema of muscat, which allows for various types and size of changes between the different groups. Briefly, the simulation of  is controlled by parameters 1) , which is a user-defined average log2 fold change across all DE genes, 2) , which varies the magnitude of gene expression difference for cluster k, and 3) a proportion vector  which is the proportion of genes that follow six different gene expression patterns (see); for simplicity, we allowed only the two most typical gene expression patterns, which are EE (equally expressed) and DE (differentialy expressed) genes for our simulations, resulting in  effectively being a single scalar, the proportion of genes that are differentially expressed. The selection of , the number of cells per sample , also followed the strategy of muscat, where the user provides a value , representing the average number of cells per sample across all samples, and the value of each individual  for each sample is assigned via a multinomial with equal probability and total number of cells across all samples equal to . The parameters , and initial values of  and  were obtained by estimating these parameters from the reference data, following the muscat package: after performing quality control, we used the filtered gene matrix and the edgeR package to estimate the parameters from the reference data. Using our modified parameterization described above,  was then chosen as the average of the  estimated from the reference samples. Sample-level sequencing depth variability  were simulated as . Per-cell variability, , was simulated as . Finally, the selection of  used in our simulation diverged from muscat package strategy. The muscat estimates of  created overly large differences between the treatment groups and samples (Supp. Figure S14); furthermore their strategy recycles the same set of parameters  if the simulated sample sizes are larger than provided reference sample sizes (i.e. the same value of  would be given to multiple simulated samples), resulting in unintended batches of samples. Instead, we estimated  from the reference data using the muscat strategy, and chose a single sample  whose initial estimates  were representative. We then set  and created individual  with variation per sample by adding noise to , where  controled the degree of sample-level variation. Supplementary Figure S15 shows the effect of changing different parameters ( and log-fold change), visualized using UMAP on an illustrative example. Simulation Settings In following the above strategy of selecting parameters, we randomly chosen 5 COVID samples from the COVID-19 PBMC dataset,. After estimating  and  as described above from the reference samples, the values were fixed for all simulations. The value  was chosen as 5,000, which is similar to the average cell per samples in several datasets (e.g.). The default value for  to control the sample level cluster proportion variability was set to be 100, except where explicitly noted, which keeps the variation in cluster proportions to be relatively small among samples (see Figures 5(d)). for each cell-type ,  values of  as described above based on , for each cell-type , a single vector  for the population log-fold-change between groups, based on the parameters , , and , for each sample  a single value  and  values of , one for each of the  cells from each sample. This results in  values of  for each sample. (Note that some simulations set  and/or  to 0 for all  and ). Once these parameters were fixed, the following user-defined parameters were set differently for different simulation settings:  (the number of samples in a single group), the vector group proportions , average library size , and the DE parameters , , and . With these global parameters chosen for a simulation setting, the remaining sample-specific parameters are generated anew in each simulation:  Combining these parameters result in the  needed for each sample in a single simulation, and then the cell-counts for each sample  are simulated from . Supp Table S1–S6 provide the different parameter settings that were run and their resulting power and average ANOSIM values corresponding to the figures shown here. Numerical metrics for evaluating simulations In order to quantify how well our representation was able to differentiate sample groups in different settings, we implemented a simple hypothesis test for comparing the two groups based on our estimated distances from our GloScope representation. We relied on the Analysis of Similarities (ANOSIM) test, which is a non-parametric test based on a metric of dissimilarity, to evaluate whether the between group distance is greater than the within group distance. We used the function anosim in the R package vegan to perform the test. The test statistic is calculated as:  where  is the mean of rank similarities of pairs of samples from different groups,  is the mean of rank similarity of pairs within the same groups, and  is the total number of samples. The test statistics ranges from −1 to 1. Strong positive test statistics means greater between group distances than the within groups; strong negative test statistics means the opposite and may represent wrong group assignments; and test statistics near zero indicate no differences. Finally, p-values are calculated based on a null permutation distribution: the distribution of  recalculated after randomly shuffling the samples’ group assignment. The p values are calculated as the proportion of times that the permuted-derived statistics are larger than the original test statistic. We used the results of ANOSIM to calculate the power in different simulation settings, creating a quantitative metric for evaluating the sensitivity of the GloScope representation in different scenarios. For a choice of input parameters, we repeated the simulation 100 times. For each simulation, we calculated the pairwise distances between all  samples, then used ANOSIM p-values to determine whether we would reject the null hypothesis. Finally, we calculated the power as the proportion of the 100 simulations’ test statistics that have p values smaller than . Data processing procedures This section details the steps undertaken to estimate GloScope representations of samples from publicly available scRNA-Seq data. These steps broadly consisted of ensuring the data we used had quality control matching the corresponding paper, estimating the cells’ latent embeddings, and applying the GloScope methodology. For most datasets we performed the first two steps with data structures and functions from the  package Seurat. For the larger lupus immune cell and mouse brain datasets, we instead utilized the SingleCellExperiment data structure and applied functions from other packages. Code for running these analyses, as well as text files containing data sources and specific processing choices, are available in the following GitHub repository: https://github.com/epurdom/GloScope_analysis. Quality Control Verification. The UMI count data and cell annotations from each sample-level scRNA-Seq study were downloaded from its publicly accessible source (indicated in the code). We checked whether data provided already had the quality control steps described in its respective paper. These steps can include removing cells with extreme expression values and filtering certain gene sets, such as mitochondrial genes. Only the data provided from did not appear to have the stated steps of the manuscript already applied, and we reproduced the cell-wise quality control procedure described in that paper’s Methods section. We also removed genes expressed in less than 10 cells (except for the data from which provided only PCA embeddings that we used directly). Latent Space Estimation. In this paper we present results based on using 10-dimensional latent embeddings, calculated with either scVI or PCA. To calculate scVI embeddings, we used the entire UMI count matrix as input after the aforementioned verification steps. To calculate PCA embeddings, we used a subset of only the 2, 000 most highly variable genes. To select which genes to include, we first log-normalized the counts within each cell; this was implemented with logNormCounts from Seurat or logNormCounts from scuttle for SingleCellExperiment objects. Then we fit a LOESS curve to predict each gene’s log-scale variance from its log-scale mean; that regression was implemented with the vst method of FindVariableFeatures in Seurat and with modelGeneVar from the scran package for SingleCellExperiment objects. The FindVariableFeatures function of Seurat also selects the 2, 000 highly variable genes based on large residuals in the LOESS regression. That exact selection rule is not available for SingleCellExperiment objects, so we instead applied a similar procedure implemented by getTopHVGs from scran. This alternative only differs in a truncation step and is commonly used in other scRNA-Seq analyses. Each of the 2, 000 selected genes was centered and scaled to zero mean and unit variance before running PCA. In Seurat objects this was done via a two-step procedure with calls to ScaleData and RunPCA. However for SingleCellExperiment objects we standardized each gene and ran PCA with a single call to runPCA from scater. Application of GloScope. After obtaining each cell’s latent representation via PCA or scVI, we fit sample-level densities with GMM or kNN and the KL divergence between samples was estimated. This produces the GloScope representations, and these steps are implemented by gloscope function in our GloScope R package, which accompanies this paper and is available in the GitHub repository: https://github.com/epurdom/GloScope. To run GloScope, we first had to determined which cells constituted a single sample in each dataset, based on the provided metadata. In some studies each patient only provided one sample and in others a single patient provided multiple samples, for instance from affected and healthy regions. Based on this we ran GloScope with tissue samples as the unit of analysis. The sole exception to this choice is the PBMC data from lupus patients in; this study processed some tissue samples in multiple processing cohorts, and we used the cross of sample and cohort as our unit of analysis. Before applying GloScope we confirmed that the tissue sample identifier associated with each cell matches the reported study design. The original melanoma data from had duplicate encodings, which we standardized, and one sample with a mislabeled phenotype. For multiple myeloma cells from, we parsed concatenated strings into patient, observation period, and phenotype indicators. Two colorectal tumor samples from were sequenced with two technologies, and we only considered the replicates using the newer technology. We also chose to remove samples with less than 50 cells. This excluded 2 samples from (AB3178 and AB3195) and 1 from (C119N). We noted that one sample from (AB3461), had extreme divergences with other samples. We removed all cells from this sample for the results presented in this paper. Pseudobulk data analysis For datasets where raw count data are available, we performed pseudobulk analysis by summing each sample’s cell entries across each genes using muscat’s aggregateData function. After obtaining the pseudobulk data, we processed it using functions from the Seurat package. We log-normalized the data using NormalizeData and selected the top 2000 highly variable feature using the FindVariableFeatures function with default arguments. Counts from the selected genes were scaled using ScaleData and a PCA embedding was obtained with RunPCA. Prediction of phenotype with GloScope representation After using GloScope to obtain the symmetrized KL divergence matrices of COVID PBMC samples, we obtained their MDS embeddings with 10 dimensions. 60% of the data points were reserved for training and rest 40% were used for testing purpose. We applied SVM to classify sample’s phenotype using the package e1071 (i.e, COVID vs healthy), and 5-fold cross valiadtion to tune the hyperparameters cost and . Finally, we used the test sets to assess the prediction algorithm by counting the prediction accuracy rate. Availability of data and materials Code for running these analyses, generating figures, and text files detailing dataset download source and specific processing choices are available in the following GitHub repository: https://github.com/epurdom/GloScope_analysis. The R package for GloScope is available in the GitHub repository: https://github.com/epurdom/GloScope, and will be submitted to Bioconductor shortly.","Overview of the GloScope Representation If we consider trying to model individual samples, we see that the format of scRNA-Seq data when considered as data on samples (not cells) is non-standard. Most computational strategies assume each sample is measured on a shared set of features. Instead, for each sample  we observe a matrix , containing the gene expression measurements of that sample across all cells ( corresponds to the number of genes and  to the number of cells sequenced from sample ). There is no direct correspondence between the  cells in sample  with the  cells of sample  so there is no immediate way to align data from different samples as input into a statistical model or predictive algorithm. We propose to create a representation of each sample that does not require explicitly aligning individual cells across samples, but leverages the nature of the observed data to represent each sample in a similar space. We consider the gene measurements for each of the  cells to be a sample from the full population of all cells of each sample. The full population of cells defines a probability distribution we designate as  on .  is a representation of the sample’s entire single-cell profile across all cells and importantly is a mathematical object that can be compared across samples. We do not observe , but we do observe  samples from this distribution (the sequenced cells), allowing us to estimate  from the data. Thus, we transform each sample from the matrix  of observed gene expression measurements to an estimate of the sample’s distribution, . However, because gene expression data lie in a high dimensional space, with the number of genes  in the thousands, estimating  directly from the cells is intractable. Thus, we assume that there exists a lower dimensional representation or latent variable in  which governs the gene expression of a sample. We instead estimate the distribution of this latent variable. We do this by first estimating a lower dimensional representation of our all our cells, for example via methods like PCA or scVI applied to all the cells. This results in a matrix of reduced representation  corresponding to the new coordinates of each cell in this reduced space. We then estimate the distribution  from the  cells in this reduced space. Unlike the , which have different, unrelated, dimensions for each sample , the  lie in the space of distributions on  and can be compared. As probability measures, these representations are now familiar mathematical objects and sample-level analysis can be done in the space of probability measures. There are many well-known metrics defined on the space of probability measures, such as the Wasserstein distance, and downstream analysis can be performed after choosing a metric to quantify pairwise sample differences. We call this representation of samples the GloScope representation, and we illustrate this transformation in Figure 1. For our examples, we use the square root of the symmetrized Kullback-Leibler (KL) divergence to quantify the differences between sample distributions; while not a proper metric, this divergence can be effectively used to create a global representation of probability distributions (see Methods for details). For example, the pairwise divergences between GloScope-represented samples can be given as input to canonical divergence analysis methods such as Multidimensional Scaling, which creates coordinate system to represent the samples that capture the pairwise divergences. We will demonstrate that such a visualization enables detection of possible batch effects and exploratory assessment of the strength of phenotypic differences between our samples. We primarily concentrate in this work on assessing the use of the GloScope representation for the purpose of visualization and exploratory data analysis, but our representation can be used for other important downstream tasks, include clustering of samples, global hypothesis tests for differences between sample populations, and prediction of phenotypes (for example via kernel prediction methods, e.g.). Using Cell-type Composition Our GloScope approach to creating a global representation uses the entire gene distribution , which encodes both cell-type composition and gene expression. However, the underlying logic of GloScope could also be applied to compare only cell-type composition. Specifically, if each cell can be classified into one of  subtypes, then we observe for each sample the proportion of cells in each cell-type,  is an estimate of a probability distribution, only now a simpler discrete distribution into  groups. We can use the GloScope strategy in a similar way to globally compare samples, only now restricted to only differences in cell-type composition. Comparison of cell-type composition has been proposed for globally comparing single-cell samples, and there has been some limited work in analysis of data from flow-cytometry using cell-type compositions to globally compare samples which has similarities to using GloScope on the proportions. Unlike a full GloScope representation, applying GloScope on the cluster proportion vector requires classifying cells into subtypes before application of the method. Accurate identification of cells into subtypes is often a manual and time-consuming process, which makes this approach less useful for the exploratory data analysis that is often upstream of the subtype identification step. However, GloScope applied to the clusters can be used for more formal hypothesis testing of significant global differences in cell-type composition. Visualization of patient and sample phenotypes using GloScope representations In this section we demonstrate the utility of the GloScope representation to visualize and evaluate sample-level phenotypic differences. As an initial illustration, we consider two datasets with replicate samples collected for each phenotype, where the phenotypes have well-known biological differences in cell-type structure. These serve as an initial proof-of-concept of the GloScope representation. The first dataset is scRNA-Seq data from the mouse cortex. Here the samples are cells from different regions of the brain with replication in each from three genetically identical mice. This is a dataset where we know the regions have distinct compositions of cell types and gene expressions. When we visualize these samples using the GloScope representation in Figure 2(a), we see these distinctions clearly. The samples from the two main subdivisions of the cortex, isocortex (CTX) and hippocampal formation (HPF), clearly separate. Furthermore, we see that replicate samples from the same region strongly cluster with each other, while different regions are generally well separated. Within the CTX region, we observe blocks of biologically meaningful brain region groups such as the sensory and visual area: primary somatosensory (SSp), posterior parietal association (PTLp), visual area (VIS), and the Somatomotor areas: primary motor (MOp) and secondary motor (MOs). We also observe clustering of physically adjacent brain regions such as temporal association, perirhinal, and ectorhinal areas (TEa-PERI-ECT), agranular insular (AI), prelimbic, infralimbic, orbital area (PL-ILA-ORB) and anterior cingulate (ACA). Next we consider skin cell samples from a study of twelve patients, consisting of nine healthy skin samples from the foreskin, scalp, and trunk alongside three inflamed skin samples collected from truncal psoriatic skin. We expect marked differences between cellular distributions collected at the different locations in the body due to varying proportions of cell types in certain tissues. For instance the authors note different types of main basal keratinocytes and melanocytes dominate in scalp and trunk samples, as compared to foreskin tissues. Our visualization of the GloScope representations of this data in Figure 3 shows a clear clustering of skin samples collected from similar locations on the body, and a separation of both the foreskin and psoriasis samples from scalp and trunk samples, echoing the conclusions of the authors who identified a keratinocyte subpopulation which separates these phenotypes from the scalp and trunk control samples. Next we demonstrate the GloScope representation on additional datasets of patient cohorts where the samples are patients with differing disease phenotypes: 1) COVID lung atlas data from, which contains 27 samples, either diagnosed with COVID-19 or healthy control samples, and 2) Colorectal cancer data with 99 samples (after quality control), grouped into three phenotypes: healthy, mismatch repair-proficient (MMRp) tumors, and mismatch repair-deficient (MMRd) tumors. The use of GloScope on these datasets demonstrates its utility for the visualization of both sample and phenotype variability. For the COVID lung samples (Figure 4(a)), we can easily see the separation between COVID-infected and healthy donors, matching the observation of that lung samples from COVID patients were highly inflamed. For the colorectal cancer data, visualization of the GloScope representation shows healthy samples well separated from the tumor samples (Figure 4(b)). Though the two types of tumors do not separate in this visualization, an Analysis of Similarities (ANOSIM,) test of significance applied to their GloScope divergences between these two groups does find their representations to be significantly different , indicating that the representation is encapsulating systematic differences between the two tumors (see Methods section). Quantitative Evaluation of GloScope via Simulation We use simulation experiments to quantify GloScope’s efficacy at detecting various classes of single-cell differences that might be observed due to differences in samples’ phenotype. We simulate sample-level data where different aspects of the single-cell composition of a sample vary depending on their group assignment; for simplicity we consider only two different phenotypic groups. Count matrices were generated from a pipeline modified from that presented in the R package muscat (see Methods for details). We focus on two basic biological scenarios that could causes phenotypic-based dissimilarity between scRNA-Seq samples which we would want the GloScope representation to accurately reflect: differential cell-type composition and differential gene expression. By cell-type composition, we refer to the proportion of various cell-types found in a sample; for example an inflammatory disease phenotype might result in a higher proportion of immune cells in the patient than in a healthy sample. Cell-type gene expression differences (DE) refers to differences across samples in the marginal gene expression levels within cells of a certain type. For example the IL2 gene has more expression within the T-cells of inflammation tissue samples when compared to the its expression in T-cells of healthy samples. Both types of differences are biologically plausible and can co-exist. We also note that in practice the distinction between these two can blur: many genes exhibiting sufficiently strong differential expression between phenotypes will result in the creation of a novel cell-type for all practical purposes, thereby corresponding to differential cell-type composition and vice versa. In our simulations we evaluate how well these two types of differences are detected by GloScope. We create datasets demonstrating either differentially expressed genes or differential cell-type composition. We see that the average differences between samples in different different phenotype groups, as measured by our GloScope representation, appropriately increase in response to both increased differences in global cell composition (Figure 5(a)) and increased differential gene expression (Figure 5(b)). This indicates that our representation effectively reflects both types of changes. Similarly, when increased sample variability is added, both in global cell composition and gene expression, our GloScope representation correspondingly shows increased within-group variability (Figure 5(c) and (d)). We can use our GloScope representation to compare different choices of the design or analysis of the experiment, based on how well the two phenotypic groups separate in the GloScope representation. To do so, we perform analysis of similarities (ANOSIM), a hypothesis test for differences between groups based on observed pairwise divergences on samples. ANOSIM takes as input divergences between samples and tests whether divergences are significantly larger between samples in different groups compared with those found within groups based on permutation testing (see Methods for more details). Evaluation of ANOSIM over many simulations gives the power of the test in different settings, resulting in a metric to compare choices in our analysis. Using these power computations, we can see that changes in the sample variability and sample size are reflected as expected in these power calculations: increasing all of these sources of variability naturally reduces the power (Supplementary Figure S1). These types of simulations, in conjunction with our GloScope representation, can be used to evaluate design choices at the sample-level, such as the number of samples needed to reach a desired power level. Unsurprisingly, differences in cell-composition in large clusters are more easily detected than similar differences in small clusters (Figure 6(a)), and gene expression differences concentrated in small clusters are harder to detect than those found in large clusters (Figure 6(c)). We can also compare choices in the data analysis pipeline. For example, GloScope relies on a user-provided choice of latent variable representation of the single-cell data. We compare the choice of PCA versus scVI in a wide range of our simulation settings. The most striking difference is in detection of cell-composition differences, where scVI has much less power in detecting differences between the two phenotypic groups than PCA (Supplementary Figure S2). The latent variable representations given by scVI demonstrates much greater variability between samples than the those of PCA (Supplementary Figure S3), potentially resulting in less power to detect the shared phenotypic differences. On the other hand, scVI representations have more power than their PCA counterparts when the source of differences is due to log-fold changes in genes (Supplementary Figure S4), perhaps due to better accounting for sparse low-count data. Finally, we can also consider choices made in implementing GloScope, in particular in the choice of estimation of the density of the latent variables  in each sample. We consider two popular density estimation strategies: parametric Gaussian mixture models (GMMs) and non-parametric -nearest neighbors (kNNs). We do not observe large differences in the power of these methods when varying the level of differential expression (Supp. Figure S4), but kNN is somewhat more powerful in the presence of cell-type composition changes (Supp. Figure S2). Applying both methods on a wide range of datasets (Supp. Figure S5–S6) shows that, on average, the estimates of divergence from the two methods are generally monotone with moderate to strong correlations (Pearson coefficient ranging from 0.36 to 0.95); furthermore, the kNN estimates are systematically lower and appear to saturate when GMM estimates are large. While kNN-density estimation offers an asymptotically unbiased estimator of the symmetric KL divergence, it is known to exhibit downward finite sample bias due to underestimation of density in the tails of a distribution. Due to these considerations, we relied on GMM estimates of density, though none of the results shown qualitatively change if kNN estimates are used instead. GloScope representation for Quality Control Finally, we demonstrate the use of GloScope for exploratory data analysis of relatively large sample cohorts and illustrate the utility of having a sample-level representation of the data for exploratory data analysis. The first dataset is a study of COVID-19 consisting of 143 samples of peripheral blood mononuclear cells (PBMC); samples in the study originated from patients that were either identified as infected with COVID-19 with varying levels of severity (COVID), negative for COVID-19 (Healthy), healthy volunteers with LPS stimulus as a substitute of an acute systemic inflammatory response (LPS), or having other disease phenotypes with similar respiratory symptoms as COVID-19 (non-COVID). Figure 7(a) shows these samples after applying MDS to the pairwise divergences calculated from the GloScope representation for the 143 samples of the study. The visualization shows that both COVID patients and healthy donors are clearly separated from patients with other respiratory conditions (LPS and non-COVID). The other noticeable pattern is that the remaining patients do not show a strong separation between the COVID and Healthy phenotypes, but do appear to separate into at least two groups unrelated to these main phenotypes of interest – an observation that is further strengthened when considering the MDS representation of only the COVID patients and healthy donors (Figure 7(b)). Exploration of the provided sample data from shows that these groups correspond to different sequencing locations, indicating a strong batch effect due to sequencing site, with samples sequenced at the Cambridge site clearly separated from those at the New Castle (Ncl) and Sanger sites. When the individual cells are visualized (Supplementary Figure S11), the distributional differences between these sequencing sites validate these differences, with cells from the Cambridge site lying in quite different spaces from cells of the same cell type from the other sequencing sites. Furthermore, indicates that samples from these different sites underwent different sequencing steps such as cell isolation and library preparations (and the original analysis in corrected for potential batch effects by applying the batch correction method, Harmony). A similar analysis was applied to a Systemic lupus erythematosus (SLE) dataset, with scRNA-Seq data of the PBMC cells of 261 patients; some patients had multiple samples resulting in total 336 samples. Again, our GloScope representation clearly shows that there are distinct patterns among different batch sources, in addition to separation of normal samples from the other conditions (Figure 8(a)). After application of Harmony to this data based on the batch, our GloScope representation shows much greater intermingling of the data from different batches (Figure 8(b)). This type of exploratory analyses of data is a common task in the analysis of scRNA-Seq data, and the GloScope representation provides a meaningful strategy for evaluating these types of processing choices. Batch effects are common concerns with large sets of data, especially in human subject data where the samples are likely to be collected and possibly sequenced at different sites. These examples immediately demonstrates the power of our GloScope representation for exploratory data analysis. Current visualization strategies of scRNA-Seq data generally consist of applying tools such as UMAP or tSNE to create a two-dimensional visualization of the individual cells, as in Supplementary Figure S11. However, batch effects are often due to variables that vary per sample or patient, such as the hospital of collection. Detecting such shared differences based on visualization of the individual cells would generally be quite difficult, particularly for large numbers of samples. Visualization of samples based on our GloScope representation immediately highlights the differences in these samples. Comparison with pseudo-bulk analysis Another potential strategy for sample-level exploratory analysis is using a pseudo-bulk created from the scRNA-Seq data. This is a strategy of aggregating over each sample’s cells to obtain a single observation per sample; the most common is to simply sum the counts. Then standard methods from bulk mRNA-Seq, such as PCA, can be applied at the sample level. We create such a PCA visualization of the pseudo-bulk of several of the datasets mentioned above (Supp Fig S13). For the COVID-19 PMBC samples, for example, the pseudobulk analysis does not clearly separate out the LPS and non-COVID samples, nor is the strong batch effect due to sequencing site as clearly identified. Similarly, for the SLE data, the pseudobulk representation does not identify the strong batch effects seen in our GloScope representation. Furthermore, the pseudo-bulk strategy is based on summarizing across individual genes, usually raw counts. Many public datasets provide other normalized versions of the data (e.g. residuals); similarly many batch-correction methods, like Harmony, provide a batch-corrected latent variable representation. None of these are obvious candidates for a pseudo-bulk approach. Our GloScope representation requires as input only a latent-variable representation of the data and thus is flexible to accommodate all of these types of input. This is important, for example, in evaluating the effect of batch correction methods. With GloScope, we can visualize the data before and after batch correction with the Harmony algorithm (Figure 8), allowing us to confirm that the Harmony algorithm has removed much of the differences between batches.","In this work, we demonstrated the use of GloScope for exploratory analysis, and in particular how the GloScope divergences can be used to create two-dimensional scatter plots of samples, similar to that of PCA plots of bulk mRNA-Seq data. We demonstrated the ability of the GloScope representation to detect important artifacts in the data, as well as assess batch-correction methodologies. While we focus on the utility of the GloScope representation to visualize scRNA-Seq data at the sample level, the representation can be used more broadly with other statistical learning tools. For example, we can use the GloScope divergences between samples as input to a prediction algorithm in order to predict a phenotype. With the COVID-19 data, we apply the SVM algorithm to the GloScope divergences which results in a prediction algorithm that was able to separate the normal from the COVID samples with a 5-fold cross-validated prediction accuracy of around 0.88. This simple example serves as an illustration of the power of a global representation of the entire scRNA-Seq profile. Finally, we note that GloScope can easily be incorporated into existing scRNA-Seq pipelines at multiple stages of analysis to assess the progress. Latent-varible representation, via PCA or scVI is a standard initial step in an analysis, while many popular batch correction methods provide low-dimensional representations of corrected data. Even multimodal integrations usually result in a low-dimensional latent space estimation. The output of all of these tasks can be provided to GloScope for evaluation of sample-level similarities, resulting in a flexible tool for exploratory analysis of the results.",10.1101/2023.05.29.542786
PMC10028745,36945558,Sequence characteristics and an accurate model of abundant hyperactive loci in the human genome,"Enhancers and promoters are classically considered to be bound by a small set of TFs in a sequence-specific manner. This assumption has come under increasing skepticism as the datasets of ChIP-seq assays of TFs have expanded. In particular, high-occupancy target (HOT) loci attract hundreds of TFs with seemingly no detectable correlation between ChIP-seq peaks and DNA-binding motif presence. Here, we used a set of 1,003 TF ChIP-seq datasets (HepG2, K562, H1) to analyze the patterns of ChIP-seq peak co-occurrence in combination with functional genomics datasets. We identified 43,891 HOT loci forming at the promoter (53%) and enhancer (47%) regions. HOT promoters regulate housekeeping genes, whereas HOT enhancers are involved in tissue-specific process regulation. HOT loci form the foundation of human super-enhancers and evolve under strong negative selection, with some of these loci being located in ultraconserved regions. Sequence-based classification analysis of HOT loci suggested that their formation is driven by the sequence features, and the density of mapped ChIP-seq peaks across TF-bound loci correlates with sequence features and the expression level of flanking genes. Based on the affinities to bind to promoters and enhancers we detected 5 distinct clusters of TFs that form the core of the HOT loci. We report an abundance of HOT loci in the human genome and a commitment of 51% of all TF ChIP-seq binding events to HOT locus formation thus challenging the classical model of enhancer activity and propose a model of HOT locus formation based on the existence of large transcriptional condensates.","Tissue-specificity of gene expression is orchestrated by the combination of transcription factors (TFs) that bind to regulatory regions such as promoters, enhancers, and silencers. Classically, an enhancer is thought to be bound by a few TFs that recognize a specific DNA motif at their cognate TF binding site (TFBS) through its DNA-binding domain and recruit other molecules necessary for catalyzing the transcriptional machinery. Based on the arrangements of the TFBSs, also called “motif grammar”, the architecture of enhancers is commonly categorized into “enhanceosome” and “billboard” models. In the enhanceosome model, a rigid grammar of motifs facilitates the formation of a single structure comprising multiple TFs which then activates the target gene. This model requires the presence of all the participating proteins. Under the billboard model, on the other hand, the TFBSs are independent of each other and function in an additive manner. However, as the catalogs of TF ChIP-seq assays have expanded thanks to the major collaborative projects such as ENCODE and modENCODE, this assertion that the TFs interact with DNA through the strictly defined binding motifs has fallen under increasing contradiction with empirically observed patterns of DNA binding regions of TFs. In particular, there have been reported genomic regions that seemingly get bound by a large number of TFs with no apparent DNA sequence specificity. These genomic loci have been dubbed high-occupancy target (HOT) regions and were detected in multiple species. Initially, these regions have been partially attributed to technical and statistical artifacts of the ChIP-seq protocol, resulting in a small list of blacklisted regions which are mostly located in unstructured DNA regions such as repetitive elements and low complexity regions. These blacklisted regions have been later excluded from the analyses and they represent a small fraction of the mapped ChIP-seq peaks. In addition, various studies have proposed the idea that some DNA elements can serve as permissive TF binding platforms such as GC-rich promoters, CpG islands, R-loops, and G-quadruplexes. Other studies have argued that these regions are highly consequential regions enriched in epigenetic signals of active regulatory elements such as histone modification regions and high chromatin accessibility. Early studies of the subject have been limited in scope due to the small number of available TF ChIP-seq assays. There have been numerous studies in recent years with additional TFs across multiple cell lines. For instance, Partridge et al. studied the HOT loci in the context of 208 chromatin-associated proteins. They observed that the composition of the chromatin-associated proteins differs depending on whether the HOT locus is located in an enhancer or promoter. Wreczycka et al. performed a cross-species analysis of HOT loci in the promoters of highly expressed genes, and established that some of the HOT loci correspond to the “hyper-ChIPable” regions. Remarker et al. conducted a comparative study of HOT regions in multiple cell lines and detected putative driver motifs at the core segments of the HOT loci. In this study, we used the most up-to-date set of TF ChIP-seq assays available from the ENCODE project and incorporated functional genomics datasets such as 3D chromatin data (Hi-C), eQTLs, GWAS, and clinical disease variants to characterize and analyze the functional implications of the HOT loci. We report that the HOT loci are one of the prevalent modes of regulatory TF-DNA interactions; they represent active regulatory regions with distinct patterns of bound TFs manifested as clusters of promoter-specific, enhancer-specific, and chromatin-associated proteins. They are active during the embryonic stage and are enriched in disease-associated variants. Finally, we propose a model for the HOT regions based on the idea of the existence of large transcriptional condensates.","Datasets Transcription factor (DAP), histone modification, DNase-I hypersensitivity sites ChIP-seq and ATAC-seq datasets for HepG2, K562, H1-hESC cell lines were batch downloaded from the ENCODE Project. For each DAP of each cell line, if there were multiple datasets, the one with the latest date was selected, prioritizing the ones with the least among of audit errors and warnings (Table S1). The GRCh37/hg19 assembly was used as a reference genome throughout the study. In those cases when ChIP-seq dataset was reported on GRCh38/hg38, the coordinates were converted to hg19 using liftOver. The phastCons evolutionary conservation scores generated from 46 vertebrate species, placental mammals and primates, CpG islands, repeat elements and GENCODE TSS annotations were all obtained from the UCSC genome browser database. Transcribed enhancer regions (eRNAs) were obtained from the FANTOM database. Super-enhancer regions were obtained from (Hnisz et al. 2013). Hi-C datasets were obtained from ENCODE Project. Please refer to Supplemental Methods 1.3 for detailed description of Hi-C data analysis. GC contents were calculated using the “nuc” functionality of the bedtools program. Gene expression data was obtained from the Roadmap Epigenomics project. For analyzing the expression levels of target genes, the gene of the overlapping TSS was used for promoters, whereas for enhancers, the nearest genes were selected using the bedtools closest function. Tissue-specificity metric tau scores for genes were downloaded from (Palmer et al. 2021) which were calculated using the data mined from Gene Expression Omnibus. Definitions The loci were divided into bins according to a two-part scale. The first part is on a linear scale from 1 to 5, the second part is on a logarithmic scale from 5 to the maximum number of DAPs bound to a single locus in that cell line (Table 1). These nominal numbers are used in cases when the distributions are displayed for individual cell lines (such as Fig1A and Fig). When the figures display the distributions for two cell lines in a joint manner (such as Fig3A,B), the edges are converted to the average percentages of the overall scale lengths for each cell line. Regular enhancers were defined as central 400bp regions of DNase-I hypersensitivity sites (DHS) which overlap H3K27ac histone modification regions with promoter and exons removed from them. Promoters were defined as 1.5kbs upstream and 500 downstream regions of the canonical and alternative TSS coordinates were extracted from the knownGenes.txt table obtained from UCSC Genome Browser. All the genomic arithmetic operations were done using the bedtools program. Figures were generated using Matplotlib and Seaborn packages. Statistical and numerical analyses were done using the pandas, NumPy, SciPy and sklearn packages in Python programming language. Genomic repeat regions were extracted from RepeatMasker table obtained from http://www.repeatmasker.org/. CpG islands were extracted from cpgIslandExt table obtained from the UCSC Genome Browser. Protein-protein interaction network information was obtained using the https://string-db.org web interface. Statistical analyses All the statistical significance analyses were done using the SciPy package. Statistical significance of genomic region overlaps was calculated using the “bedtools fisher” command. The p-values too small to be represented by the command line output were represented as <1E-100. Correlation values with the number of bound TFs were calculated using the average of the value for the bins, and the midpoint numbers of the edges of each bin. GWAS analysis NHGRI-EBI GWAS database variants were grouped according to their traits (dataset e0_r2022-11-29). For each GWAS SNP, LD SNPs with r2>0.8 were added using the plink v1.9 program using the parameters --ld-window-r2 0.8 --ld-window-kb 100 --ld-window 1000000. Enrichments of GWAS-trait SNPs were calculated as the ratios of densities of SNPs in each class of regions (eg. HOT enhancers, HOT promoters) to either that of the regular enhancers or the DHS regions. Statistical significance of enrichment was calculated using the binomial test. FDR values were calculated using the Bonferroni correction. Sequence classification analysis Classification tasks were constructed in a binary classification setup. The control regions were used from: a) Randomly selected (10x the size of the HOT loci) merge DHS regions from all the available datasets from Roadmap Epigenomic Project b) using all of the promoter regions as defined above c) regular enhancers as defined above, with the HOT loci subtracted (see Supplemental Methods 1.6.1 for details). Sequence-based classification (CNN): sequences were converted to one-hot encoding and a Convolutional Neural Network was trained using each of the control regions as negative set. The model was built using tensorflow v2.3.1 and trained on NVIDIA k80 GPUs (see Supplemental Methods 1.6.2.1 for details). Sequence-based classification (SVM): SVM models were trained using the LS-GKM package (see Supplemental Methods 1.6.2.2 for details). Feature-based classification: sequences were represented in terms of GC, CpG, GpC contents and overlap percentages with annotated CpG islands. Logistic regression and SVM classifiers were trained using these sequence features (see Supplemental Methods 1.6.3 for details). Variant analysis Common SNPs and INDELs were extracted from the gnomAD r2.1.1 dataset. Variants with PASS filter value and MAF>5% were selected using the “view -f PASS -i ‘MAF[0]>0.05’” options of bcftools program. Loss-of-function variants were downloaded from the gnomAD website under the option “all homozygous LoF curation” section of v2.1.1 database. raQTLs were downloaded from https://sure.nki.nl. Liver and blood eQTLs were extracted from the GTEx v8 dataset (https://www.gtexportal.org/home/datasets). Liver caQTLs were obtained from the supplementary material of.","HOT loci are one of the prevalent modes of TF-DNA interactions To define and analyze the high-occupancy target (HOT) loci, we used the most up-to-date catalog of ChIP-seq datasets (n=1,003) of TFs obtained from the ENCODE Project assayed in HepG2, K562, and H1-hESC (H1) cells (545, 411, and 47 ChIP-seq assays respectively, see Methods for details). While the TFs are defined as sequence-specific DNA-binding proteins that control the transcription of genes, the currently available ChIP-seq datasets include the assays of many other types of transcription-related proteins such as cofactors, coactivators, histone acetyltransferases as well as RNA Polymerase 2 variants. Therefore we collectively call all of these proteins DNA-associated proteins (DAPs). Using the datasets of DAPs, we overlaid all of the ChIP-seq peaks and obtained the densities of DAP binding sites across the human genome using a non-overlapping sliding window of length 400 bp and considered a binding site to be present in a given window if 8 bp centered at the summit of a ChIP-seq peak as overlapping. Given that the analyzed three cell lines contain varying numbers of assayed DAPs, we binned the loci according to the number of overlapping DAPs in a logarithmic scale with 10 intervals and defined HOT loci as those that fall to the highest 4 bins, which translates to those which contain on average >18% of available DAPs for a given cell line (see Methods for a detailed description and justifications). This resulted in 25,928, 15,231, and 2,732 HOT loci in HepG2, K562, and H1 cells respectively. We applied our definition to the Roadmap Epigenomic ChIP-seq datasets and observed that the number of available ChIP-seq datasets significantly affects the resulting HOT loci. However, the HOT loci defined using the Roadmap Epigenomic datasets were almost entirely composed of subsets of the ENCODE-based HOT loci, comprising 50%, 62%, and 15% in HepG2, K562, and H1, respectively (Table S5). Importantly, we note that the distribution of the number of loci is not multimodal, but rather follows a uniform spectrum, and thus, this definition of HOT loci is ad-hoc (Fig 1A, Fig S1). Therefore, in addition to the dichotomous classification of HOT and non-HOT loci, we use all of the DAP-bound loci to extract the correlations with studied metrics with the number of bound DAPs when necessary. Throughout the study, we used the loci from the HepG2 cell line as the primary dataset for analyses and used the K562 and H1 datasets when the comparative analysis was necessary. Although the HOT loci represent only 5% of all the DAP-bound loci in HepG2, they contain 51% of all mapped ChIP-seq peaks. The fraction of the ChIP-seq peaks of each DAP overlapping HOT loci varies from 0% to 91%, with an average of 65% (Fig 1B, y-axis). Among the DAPs that are present in the highest fraction of HOT loci are (Fig 1B, x-axis) SAP130, MAX, ARID4B, ZGPAT, HDAC1, MED1, TFAP4, and SOX6. The abundance of histone deacetylase-related factors mixed with transcriptional activators suggests that the regulatory functions of HOT loci are a complex interplay of activation and repression. RNA Polymerase 2 (POLR2) is present in 42% of HOT loci arguing for active transcription at or in the proximity of HOT loci (including mRNA and eRNA transcription). When the fraction of peaks of individual DAPs overlapping the HOT loci are considered (Fig 1B, y-axis), DAPs with >90% overlap are GMEB2 (essential for replication of parvoviruses), ZHX3 (zinc finger transcriptional repressor), and YEATS2 (subunit of acetyltransferase complex). Whereas the DAPs that are least associated with HOT loci (<5%) are ZNF282 (transcriptional repressor), MAFK, EZH2 (histone methyltransferase), and TRIM22 (ubiquitin ligase). The fact that HOT loci harbor more than half of the ChIP-seq peaks suggests that the HOT loci are one of the prevalent modes of TF-DNA interactions rather than an exceptional case, as has been initially suggested by earlier studies. Around half of the HOT loci (51%) are located in promoter regions (46% in primary promoters and 5% in alternative promoters), 25% in intronic regions, and only 24% are in intergenic regions with 9% being located >50 kbs away from promoters, suggesting that the HOT loci are mainly clustered in vicinities (promoters and introns) of transcription start sites and therefore potentially playing essential roles in the regulation of nearby genes (Fig 1C). When considering the non-promoter HOT loci, we observed that they were universally located in regions of H3K27ac or H3K4me1, indicating that they are active enhancers (Fig S2 A–D). When comparing the definitions of promoters and enhancers based on chromHMM states and ENCODE SCREEN annotations, the composition of HOT loci in relation to promoters and enhancers showed similar fractions (Fig S2E). Both HOT promoters and enhancers are almost entirely located in the chromatin-accessible regions (97% and 93% of the total sequence lengths, respectively, Fig 1D). We compared our definition of the HOT loci to those reported in Remaker et al. 2020 and Boyle et al. 2014. We observed that because these two studies define HOT loci using 2 kb windows, they cover a larger fraction of the genome. Our set of HOT loci largely consisted of subsets of those defined in these two studies, with overlap percentages of 81%, 93%, and 100% in HepG2, K562, and H1, respectively (Fig S3). Further analysis revealed that our set of HOT loci primarily constitutes the “core” and more conserved (Fig S4) regions of HOT loci defined in the mentioned studies, while their composition in terms of promoter, intronic, and intergenic regions is similar (Fig S5), suggesting that the three definitions point to loci with similar characteristics. To further dissect the composition of HOT enhancer loci, we compared them to super-enhancers as defined in the study by White et al. 2013 and a set of regular enhancers (see Methods for definitions). Overall, 31% of HOT enhancers and 16% of HOT promoters are located in super-enhancers, while 97% of all HOT loci overlap H3K27ac or H3K4me1 regions (Fig 1E). While HOT enhancers and promoters appear to provide a critical foundation for super-enhancer formation, they represent only a small fraction of super-enhancer sequences overall accounting for 9% of combined super-enhancer length. A 400 bp HOT locus, on average, harbors 125 DAP peaks in HepG2. However, the peaks of DAPs are not uniformly distributed across HOT loci. There are 68 DAPs with >80% of all of the peaks located in HOT loci (Fig 1B). To analyze the signatures of unique DAPs in HOT loci, we performed a PCA analysis using the overlapping DAP combinations for each HOT locus. This analysis showed that the principal component 1 (PC1) is correlated with the total number of distinct DAPs located at a given HOT locus (Fig S6A). PC2 separates the HOT promoters and HOT enhancers (Fig 2A, FigS6B), and the PC1-PC2 combination also separates the p300-bound HOT loci (Fig S6C). This indicates that the HOT promoters and HOT enhancers must have distinct signatures of DAPs. To test if such signatures exist, we clustered the DAPs according to the fractions of HOT promoter and HOT enhancer loci that they overlap with. This analysis showed that there is a large cluster of DAPs (n=458) which on average overlap only 17% of HOT loci which are likely secondary to the HOT locus formation (Fig S7). We focused on the other, HOT-enriched, cluster of DAPs (n=87) which are present in 53% of HOT loci on average (Fig S7) and consist of four major clusters of DAPs (Fig 2D). Cluster I comprises 4 DAPs ZNF687, ARID4B, MAX, and SAP130 which are present in 75% of HOT loci on average. The three latter of these DAPs form a PPI interaction network (PPI enrichment p-value=0.001) (Fig S8A). We called this cluster of DAPs essential regulators given their widespread presence in both HOT enhancers and HOT promoters. Cluster II comprises 29 DAPs which are present in 47% of the HOT loci and are 1.7x more likely to overlap HOT promoters than HOT enhancers. Among these DAPs are POLR2 subunits, PHF8, GABP1, GATAD1, TAF1 etc. The strongest associated GO molecular function term with the DAPs of this cluster is RNA Polymerase transcription factor initiation activity suggestive of their direct role in transcriptional activity (FigS8B). Cluster III comprises 16 DAPs which are 1.9x more likely to be present in HOT enhancers than in HOT promoters. These are a wide variety of transcriptional regulators among which are those with high expression levels in liver NFIL3, NR2F6, and pioneer factors HNF4A, CEBPA, FOXA1, and FOXA2. The majority (13/16) of DAPs of this cluster form a PPI network (PPI enrichment p-value < 10−16, Fig S8C). Among the strongest associated GO terms of biological processes are those related to cell differentiation (white fat cell differentiation, endocrine pancreas development, dopaminergic neuron differentiation, etc.) suggesting that cluster III HOT enhancers underlie cellular development. Cluster IV comprises 12 DAPs which are equally abundant in both HOT enhancers and HOT promoters (64% and 63% respectively), which form a PPI network (PPI enrichment p-value < 6×10−16, Fig S8D) with HDAC1 (Histone deacetylase 1) being the node with the highest degree, suggesting that the DAPs of the cluster may be involved in chromatin-based transcriptional repression. Lastly, Cluster V comprises 26 DAPs of a wide range of transcriptional regulators, with a 1.3x skew towards the HOT enhancers. While this cluster contains prominent TFs such as TCF7L2, FOXA3, SOX6, FOSL2, etc., the variety of the pathways and interactions they partake in makes it difficult to ascertain the functional patterns from the constituent of DAPs alone. Although this clustering analysis reveals subsets of DAPs that are specific to either HOT enhancers or HOT promoters (clusters II and III), it still does not explain what sorts of interplays take place between these recipes of HOT promoters and HOT enhancers, as well as with the other clusters of DAPs with equal abundance in both the HOT promoters and HOT enhancers. To test the significance of the PPI networks described above, we ran 100 trials for each cluster by randomly selecting an equal number of DAPs reported in PPI networks and calculated the significance of the PPI enrichment p-values. All of the reported PPI enrichment p-values were significantly higher than the randomized trials (p-value < 0.01, one-sample t-test). Notably, PC4 separates HOT loci associated with CTCF (Fig 2C) and Cohesin (Fig S6D). This clear separation of CTCF- and Cohesin-bound HOTs is surprising, given that only relatively small fractions of their peaks (21% and 38% respectively) reside in HOT loci, and present in 36% of the HOT loci, compared to some other DAPs with much higher presence described above, that do not get separated clearly by the PCA analysis. Furthermore, CTCF- and Cohesin-bound HOT enhancer loci are located significantly closer (p-value<10−100; Mann-Whitney U Test) to the nearest genes (Fig S9B), making it more likely that those loci are proximal enhancers. And the total number of overlapping DAPs is significantly higher (p-value< 10−100; Mann-Whitney U Test) in CTCF- and Cohesin-bound loci compared to the rest of the HOT loci (Fig S9C), suggesting that at least a portion of the number of DAPs in HOT loci can be explained by 3D chromatin contacts between the genomic regions mediated by CTCF-Cohesin complex. To comprehensively quantify the 3D chromatin interactions involving the HOT loci, we used Hi-C data with 5 kbs resolution (see Methods). First, we obtained statistically significant chromatin interactions using FitHiChIP tool (see Methods) and observed that HOT loci are enriched in chromatin interactions and 1.66x more likely to engage in chromatin interactions than the regular enhancers (p-value< 10−20, Chi-square test). When all of the DAP-bound loci are considered, the number of chromatin interactions positively correlates with the number of bound DAPs (rho=0.3, p-value<10−100, Spearman correlation). Next, we overlayed the chromatin interactions with the loci binned by the number of bound DAPs. We observed that the loci with high numbers of bound DAPs are more likely to engage in chromatin interactions with other loci harboring large numbers of DAPs, i.e. the HOT loci have the propensity to connect through long-range chromatin interactions with other HOT loci (Fig 3A). To further validate this observation, we obtained frequently interacting regions (FIREs), and observed that the FIREs are 2.89x (p-value<10−230, Chi-square test) enriched HOT loci compared to the regular enhancers (see Methods). Moreover, 66% of HOT loci are located in TAD regions and 21% are located in chromatin loops. In particular, the HOT loci are 2.97x (p-value< 10−230, Mann-Whitney U test) enriched in the chromatin loop anchor regions (11% of the HOT loci) compared to regular enhancers. To investigate further, we analyzed the loop anchor regions harboring HOT loci and observed that the number of multi-way contacts on loop anchors correlates with the number of bound DAPs (rho=0.84 p-value< 10−4; Pearson correlation). The number of multi-way interactions in loop anchor regions varies between 1 and 6, with only one locus, in an extreme case, serving as an anchor for 6 overlapping loops on chromosome 2 (Fig 3B). That locus contains one HOT enhancer harboring 101 DAPs, located 23 kbs away from the LINC02583 gene, which is linked to liver-specific GWAS traits hematocrit, left ventricular diastolic function, and high-density lipoprotein cholesterol, highlighting the important role of the HOT locus. Of the loop anchor regions with >3 overlapping loops, 51% contained at least one HOT locus, suggesting an interplay between chromatin loops and HOT loci (Fig 3B). Overall, 94% of HOT loci are located in regions with at least one chromatin interaction. This observation is consistent with previous reports that much of the long-range 3D chromatin contacts form through the interactions of large protein complexes. While there is a correlation between the HOT loci and chromatin interactions, the causal relation between these two properties of genomic loci is not clear. A set of DAPs stabilizes the interactions of DAPs at HOT loci Next, using the ChIP-seq signal intensity as a proxy for the DAP binding affinity, we sought to analyze the patterns of binding affinities of DAP-DNA of interactions in HOT loci. We observed that the overall binding affinities of DAPs correlate with the total number of colocalizing DAPs (Fig 4A, rho=0.97, p-value<10−10; Spearman correlation). Moreover, even when calculated DAP-wise, the average of the overall signal strength of every DAP correlates with the fraction of HOT loci that the given DAP overlaps with (rho=0.6, p-value<10−29; Spearman correlation. Figure 4B), meaning that the overall average value of the signal intensity of a given DAP is largely driven by the ChIP-seq peaks which are located in HOT loci. While the overall average of the ChIP-seq signal intensity in HOT loci is greater when compared to the rest of the DAP-bound loci, individual DAPs demonstrate different levels of involvement in HOT loci. When sorted by the ratio of the signal intensities in HOT vs. non-HOT loci, among those with the highest HOT-affinities are GATAD1, MAX, NONO as well as POLR2G and Mediator subunit MED1 (Fig 4B,C). Whereas those with the opposite affinity (i.e. those that have the strongest binding sites in non-HOT loci) are REST, RFX5, TP53, etc (Fig 4B,C). By analyzing the signal strengths of DAPs jointly, we observed that a host of DAPs likely has a stabilizing effect on the binding of DAPs in that, when present, the signal strengths of the majority of DAPs are on average 1.7x greater. These DAPs are CREB1, RFX1, ZNF687, RAD51, ZBTB40 and GPBP1L1. So far, we have treated the DAPs under a single category and did not make a distinction based on their known DNA-binding properties. Previous studies have discussed the idea that sequence-specific DAPs (ssDAPs) can serve as anchors, similar to the pioneer TFs, which could facilitate the formation of HOT loci. We asked if ssDAPs yield greater signal strength values than non-sequence-specific DAPs (nssDAPs). To test this hypothesis, we classified the DAPs into those two categories using the definitions provided in the study (Lambert et al. 2018) and categorized the ChIP-seq signal values into these two groups. While statistically significant (p-value< 0.001, Mann-Whitney U test), the differences in the average signals of ssDAPs and nssDAPs in both HOT enhancers and HOT promoters are small (Fig 4D). Moreover, while the average signal values of ssDAPs in HOT enhancers are greater than that of the nssDAPs, in HOT promoters this relation is reversed. At the same time, the average signal strength of the DAPs is 3x greater than the average signal strength of H3K27ac peaks in HOT loci. Based on this, we concluded that the ChIP-seq signal intensities do not seem to be a function of the DNA-binding properties of the DAPs. Sequence features that drive the accumulation of DAPs We next analyzed the sequence features of the HOT loci. For this purpose, we first addressed the evolutionary conservation of the HOT loci using phastCons scores generated using an alignment of 46 vertebrate species. The average conservation scores of the DAP-bound loci are in strong correlation with the number of bound DAPs (rho=0.98, p-value<10−130; Spearman correlation) indicating that the negative selection exerted on HOT loci are proportional to the number of bound DAPs (Fig 5A). With 120 DAPs per locus on average, these HOT regions are 1.7x more conserved than the regular enhancers in HepG2 (Fig 5B). We observed a similar trend of conservation levels when the phastCons scores generated from primates and placental mammals and primates were considered, the HOT loci being 1.45x and x1.1 more conserved than the regular enhancers, respectively (Fig S10). In addition, we observed that the HOT loci of all three cell lines (HepG2, K562, and H1) overlap with 22 ultraconserved regions, among which are the promoter regions of 11 genes including SP5, SOX5, AUTS2, PBX1, ZFPM2, ARID1A, OLA1 and the enhancer regions of (within <50kbs of their TSS) 5S rRNA, MIR563, SOX21, etc. (full list in Table S4). Among them are those which have been linked to diseases and other phenotypes. For example, DNAJC1 and OLA1 (which interacts with BRCA1) have been linked to breast cancer in cancer GWAS studies. Whereas AUTS2 and SOX5 have been linked to predisposition to neurological conditions such as Autism spectrum disorder, intellectual disability, and neurodevelopmental disorder. Of these genes, ARID1A, AUTS2, DNAJC1, OLA1, SOX5, and ZFPM2 have been reported to have strong activities in the Allen Mouse Brain Atlas. CpG islands have been postulated to serve as permissive TF binding platforms and this has been listed as one of the possible reasons for the existence of HOT loci in a previous study. To test this hypothesis, we extracted the overlap rates of all DAP-bound loci with CpG islands (see Methods). While the overall fraction of loci that overlap CpG islands correlates strongly with the number of bound DAPs (rho=0.7, p-value=0.001; Pearson correlation), only 12% of HOT enhancers overlapped CpG island whereas, for the HOT promoters, this fraction was 83%, suggesting that CpG islands alone do not explain HOT enhancer loci despite accounting for the majority of HOT promoters loci (Fig S11A). Similarly, the average GC content is strongly correlated with the number of bound DAPs (rho=0.89, p-value<10−4; Pearson correlation, Fig S11B), with the average GC content of 64% and 51% in HOT promoters and HOT enhancers respectively (p-value<10−100, Mann-Whitney U test), in both HepG2 and K562. In addition, we observed that the average content of repeat elements in the loci strongly and negatively correlates with the number of bound DAPs across the cell lines (rho=−0.9, p-value=<10−5; Pearson, Fig S11C), which is likely the result of the fact that the HOTs are under elevated negative selection and reject insertion of repetitive DNA. Other genomic sequence features that have been considered in the context of HOT loci in previous studies include and are not limited to G-quadruplex, R-loops, methylation patterns, etc., which have concluded that each of them can partially explain the phenomenon of the HOT loci. Still, one of the central questions remains whether the HOT loci are driven by sequence features or they are the result of cellular biology not strictly related to the sequences, such as the proximal accumulation of DAPs in foci due to the biochemical properties of accumulated molecules, or other epigenetic mechanisms. To address this question with a broader approach, we asked whether the HOT loci can be accurately predicted based on their DNA sequences alone, and sequence features, including GC, CpG, GpC contents, and CpG island coverage. For sequence-based classification, we trained a Convolutional Neural Network (CNN) model using one-hot encoded sequences and an SVM classifier trained on gapped k-mers (gkmSVM) (Supplemental Methods 1.6.2, Figure S12). For feature-based classification, we trained logistic regression (LogReg) classifiers and SVM classifiers with linear, polynomial, radial basis function, and sigmoid kernels (See Supplemental Methods 1.6.2.2 for detailed analysis). We carried out the classification experiments using the following control sets: a) randomly selected loci from merged DNaseI Hypersensitivity Sites (DHS) of cell lines in the Roadmap Epigenomics Project, b) promoter regions, and c) regular enhancers (See Supplemental Methods 6.1.1 for definitions). Using the sequence features, we trained separate models using each of the features in addition to one with all of the features combined. We observed that, when averaged across all the methods, GC content value possesses the highest amount of discrimination power (auROC: 0.73), followed by the combination of all features (auROC: 0.70) (Figure S13A,B). When compared across the classification methods, LogReg and SVM with linear kernel outperformed the other non-linear kernels by 20%, suggesting that the features possess linearly combined or largely overlapping effects in encoding the information in HOT loci (Figure S13A). When classified using the sequences directly, CNN yielded the highest performance with auROC of 0.91, while for the gkmSVM it was 0.86 (both averaged over cell lines and control sets), suggesting that CNNs capture the motif grammar of the HOT loci better than gapped k-mers (Figure S13C). When the two classification schemes (sequence- and feature-based) are compared, CNNs outperformed the LogReg and linear SVMs by a factor of 1.3x (or 17%), suggesting that there is additional information that is highly relevant to the DNA-DAP interaction density encoded in the DNA sequences, in addition to the GC, CpG, GpC (Fig 5C). This is in line with the observation mentioned above, that 88% of the HOT enhancers do not overlap with annotated CpG islands. This analysis concluded that the mechanisms of HOT locus formation are likely encoded in their DNA sequences. Of the control regions tested, we observed that CNNs can discriminate with the auROC values of 0.91, 0.89, and 0.87 for DHS, promoters, and regular enhancers respectively (Fig 5C). This observation reflects the fact that HOT loci are themselves located in enhancers and promoters, albeit representing fairly separable and distinct subsets of them. Extending the input regions from 400 bp to 1 kbs for sequence-based classification did not lead to a significant increase in performance, suggesting that the core 400 bp regions contain most of the information associated with DAP density (Fig S14). Highly expressed housekeeping genes are commonly regulated by HOT promoters After characterizing the HOT loci in terms of the DAP composition and sequence features, we sought to analyze the cellular processes they partake in. HOT loci were previously linked to highly expressed genes. In both inspected differentiated cell lines (HepG2 and K562), the number of DAPs positively correlates with the expression level of their target gene (enhancers were assigned to their nearest genes for this analysis; rho=0.56, p-value<10−10; Spearman correlation; Fig 15A). In HepG2, the average expression level of the target genes of promoters with at least one DAP bound is 1.7x higher than that of the target genes of enhancers with at least one DAP bound, whereas when only HOT loci are considered this fold-increase becomes 4.7x. This suggests that the number of bound DAPs of the HOT locus has a direct impact on the level of the target gene expression. Moreover, highly expressed genes (RPKM>50) were 4x more likely to have multiple HOT loci within the 50 kbs of their TSSs than the genes with RPKM<5 (p-value<10−12, chi-square test). In addition, the average distance between HOT enhancer loci and the nearest gene is 4.5x smaller than with the regular enhancers (p-value<10−30, Mann-Whitney U test). Generally, we observed that the distances between the HOT enhancers and the nearest genes are negatively correlated with the number of bound DAPs (rho=−0.9; p-value<10−6; Pearson correlation. Fig S15B), suggesting that the increasing number of bound DAPs makes the regulatory region more likely to be the TSS-proximal regulatory region. To further analyze the distinction in involved biological functions between the HOT promoters and enhancers, we compared the fraction of housekeeping (HK) genes that they regulate, using the list of HK genes reported by (Hounkpe et al. 2021). According to this definition, 64% of HK genes are regulated by a HOT promoter and only 30% are regulated by regular promoters (Fig 6A). The HOT enhancers, on the other hand, flank 21% of the HK genes, which is less than the percentage of HK genes flanked by regular enhancers (38%). For comparison, 22% of the flanking genes of super-enhancers constitute HK genes. The involvement of HOT promoters in the regulation of HK genes is also confirmed in terms of the fraction of loci flanking the HK genes, namely, 21% of the HOT promoters regulate 64% of the HK genes. This fraction is much smaller (<9% on average) for the rest of the mentioned categories of loci (HOT and regular enhancers, regular promoters, and super-enhancers, Fig 6A). We then asked whether the tissue-specificities of the expression levels of target genes of the HOT loci reflect their involvement in the regulation of HK genes. For this purpose, we used the tau metric as reported by (Palmer et al. 2021), where a high tau score (between 0 and 1) indicates a tissue-specific expression of a gene, whereas a low tau score means that the transcript is expressed stably across tissues. We observed that the average tau scores of target genes of HOT enhancers are significantly but by a small margin greater than the regular enhancers (0.66 and 0.63, respectively. p-value<10−18, Mann-Whitney U test), with super-enhancers being equal to regular enhancers (0.63). The difference in the average tau scores of the HOT and regular promoters is stark (0.57 and 0.74 respectively, p-value<10−100, Mann-Whitney U test), representing a 23% increase (Fig 6B). Combined with the involvement in the regulation of HK genes, average tau scores suggest that the HOT promoters are more ubiquitous than the regular promoters whereas HOT enhancers are more tissue-specific than the regular and super-enhancers. Further supporting this, the GO enrichment analysis showed that the GO terms associated with the set of genes regulated by HOT promoters are basic HK cellular functions (such as RNA processing, RNA metabolism, ribosome biogenesis, etc.), whereas HOT enhancers are enriched in GO terms of cellular response to the environment and liver-specific processes (such as response to insulin, oxidative stress, epidermal growth factors, etc.) (Fig 6C). A core set of HOT loci is active during development which expands after differentiation Having observed that the HOT loci are active regions in many other human cell types, we asked if the observations made on the HOT loci of differentiated cell lines also hold true in the embryonic stage. To that end, we analyzed the HOT loci in H1 cells. It is important to note that the number of available DAPs in H1 cells is significantly smaller (n=47) than in HepG2 and K562, due to a much smaller size of the ChIP-seq dataset generated in H1. Therefore, the criterion of having >17% of available DAPs yields n>15 DAPs for the H1, as opposed to 77 and 55 for HepG2 and K562, respectively. However, many of the features of the loci that we’ve analyzed so far demonstrated similar patterns (GC contents, target gene expressions, ChIP-seq signal values etc.) when compared to the DAP-bound loci in HepG2 and K562, suggesting that albeit limited, the distribution of the DAPs in H1 likely reflects the true distribution of HOT loci. To alleviate the difference in available DAPs, in addition to comparing the HOT loci defined using the complete set of DAPs, we also (a) applied the HOT classification routing using a set of DAPs (n=30) available in all three cell lines (b) randomly subselected DAPs in HepG2 and K562 to match the number of DAPs in H1. We observed that, when the complete set of DAPs is used, 85% of the HOT loci of H1 are also HOT loci in either of the other two differentiated cell lines (Fig 7A). However, only <10% of the HOT loci of the two differentiated cell lines overlapped with H1 HOT loci, suggesting that the majority of the HOT loci are acquired after the differentiation. A similar overlap ratio was observed based on DAPs common to all three cell lines (Fig 7B), where 68% of H1 HOT loci overlapped with that of the differentiated cell lines. These overlap levels were much higher than the randomly selected DAPs matching the H1 set (30%, Fig 7C). Average evolutionary conservation scores (phastCons) of the developmental HOT loci are 1.3x higher than K562 and HepG2 HOT loci (p-value<10−10, Mann-Whitney U test, Fig 7D). It is conceivable to hypothesize that the embryonic HOT loci are located mainly in regions with higher conservation regions, and more regulatory regions emerge as HOT loci after the differentiation. Some of these tissue-specific HOT loci could be those that are acquired more recently (compared to the H1 HOT loci), as it is known that the enhancers are often subject to higher rates of evolutionary turnover than the promoters. GO enrichment analysis showed that H1 HOT promoters, similarly to the other cell lines, regulate the basic housekeeping processes (Fig S16) while the HOT enhancers regulate responses to environmental stimuli and processes active during the embryonic stage such as TORC1 signaling and beta-catenin-TCF assembly. This suggests that the main processes that the HOT promoters are involved in during the development remain relatively unchanged after the differentiation (in terms of associated GO terms, and due to being the same loci as the HOT promoters in differentiated cell lines), whereas the scope of the cellular activities regulated by HOT enhancers gets expanded after differentiation to be more exclusively tissue-specific. HOT loci are enriched in causal variants After establishing the expression and tissue-specificities of the HOT loci, we next analyzed the polymorphic variability in HOT loci and whether these loci are enriched in phenotypically causal variants. First, we analyzed the density of common variants extracted from the gnomAD database (filtered with MAF>5%). We observed that HOT enhancers and HOT promoters are depleted in INDELs (4.7 and 4.1 variants per 1 kbs, respectively), compared to the regular enhancers and regular promoters (5.5 and 6.2 variants per 1 kbs, p-value<10−4 and <10−100, respectively, Mann-Whitney U test; Fig 8A). Contradicting the pattern of conservation scores described above, the distribution of common SNPs is elevated in HOT enhancers and HOT promoters compared to regular enhancers and regular promoters (1.14x and 1.07x fold-enrichment, p-values <10−20 and <10−100, respectively, Mann-Whitney U test; Fig 8B). This elevation of common variants in HOT loci, despite being located in conserved loci has been reported in a previous study in which the binding motifs of TFs were observed to colocalize in regions where the density of common variants was higher than average. The eQTLs, on the other hand, are 2.0x enriched in HOT promoters compared to the regular promoters (p-value<10−21, Mann-Whitney U test), while HOT enhancers are only moderately enriched in eQTLs compared to the regular enhancers (1.15x, p-value>0.05, Mann-Whitney U test; Fig 8C). eQTL enrichment in HOT promoters and regular promoters (compared to HOT and regular enhancers respectively) is in line with the known characteristics of the eQTL dataset, that the eQTLs most commonly reflect TSS-proximal gene-variant relationships, and therefore are enriched in promoter regions since the TSS-distal eQTLs are hard to detect due to the burden of multiple tests. Unlike the eQTL analysis, we observed that the chromatin accessibility QTLs (caQTLs) are dramatically enriched in the overall enhancer regions (HOT and regular) compared to the promoters (HOT and regular) (4.1x, p-value<10−100; Mann-Whitney U test, Fig 8D). This observation confirms the findings of the study which reported the caQTL dataset in HepG2 cells, which reported that the likely causal caQTLs are predominantly the variants disrupting the binding motifs of liver-expressed TFs enriched in liver enhancers. However, within the promoters regions, the HOT promoters are 3.0x enriched in caQTLs compared to the regular promoters (p-value=0.001; Mann-Whitney U test), whereas the fold enrichment in HOT enhancers is insignificant (1.2x, p-value=0.22, Mann-Whitney U test). A similar enrichment pattern displays the reporter array QTLs (raQTLs), with respect to the overall (HOT and regular) promoter and enhancer regions, with 3.3x enrichment in enhancers (p-value<10−10, Mann-Whitney U test, Fig 8E). But, within-promoters and within-enhancers enrichments show that the enrichment in HOT promoters is more pronounced than the HOT enhancers (3.6x and 1.8x, p-values<0.01 and <10−11, respectively, Mann-Whitney U test). The enrichment of the raQTLs in enhancers over the promoters likely reflects the fact that the SNP-containing loci are first filtered for raQTL detection according to their capacities to function as enhancers in the reporter array. Combined, all three QTL datasets show a pronounced enrichment in HOT promoters compared to the regular promoters, whereas only the raQTLs show significant enrichment in HOT enhancers. This suggests that the individual DAP ChIP-seq peaks in HOT promoters are more likely to have consequential effects on promoter activity if altered, while HOT enhancers are less susceptible to mutations. Additionally, it is noteworthy that only the raQTLs are the causal variants, whereas e/caQTLs are correlative quantities subject to the effects of LD. Finally, we used the GWAS SNPs combined with the LD SNPs (r2>0.8) and observed that the HOT promoters are significantly enriched in GWAS variants (1.8x, p-value<10−100) whereas the HOT enhancers show no significant enrichment over regular enhancers (p-value>0.1, Mann-Whitney U test) (Fig 8F). We then calculated the fold-enrichment levels GWAS traits SNPs using the combined DHS regions of Roadmap Epigenome cell lines as a background (see Methods). Filtering the traits with significant enrichment in HOT loci (p-value<10−3, Binomial test, Bonferroni corrected, see Methods) left 7 traits, of which all are definitively related to the liver functions (Fig 8G). Of the seven traits, only one (Blood protein level) was significantly enriched in regular promoters. While the regular enhancers are enriched in most of the (6 of 7) traits, the overall enrichment values in HOT enhancers are 1.3x greater compared to the regular enhancers. The fold-increase is even greater (1.5x) between the HOT and DHS regions. When the enrichment significance levels are selected using unadjusted p-values, we obtained 24 GWAS traits, of which 22 are related to liver functions (Fig S17). This analysis demonstrated that the HOT loci are important for phenotypic homeostasis.","HOT loci have been noticed and studied in different species since the early years of the advent of the ChIP-seq datasets. Up until recently, most of the studies have extensively studied the reasons through which the ChIP-seq peaks appeared to be binding to HOT loci with no apparent sequence specificity and characterized certain sequence features of the HOT loci which could enable elevated read mapping rates. As the number of assayed DAPs in multiple human cell types and model organisms has increased, however, the assumption of the HOT loci being exceptional cases and results of false positives in ChIP-seq protocols have given way to the acceptance that the HOT loci, with exorbitant numbers of mapped TFBSs, are indeed hyperactive loci with distinct features characteristic of active regulatory regions. In this study, we studied the HOT loci in multiple complementary aspects to the previous works and expanded the scope of characterization extensively using the functional genomics datasets. We used the two most extensively characterized differentiated cell lines of the ENCODE Project; HepG2 and K562. We also included the H1-hESC human stem cells to study the activities of HOT loci during the embryonic stage. The number of assayed DAPs in these cell lines is far from complete, therefore it is important to note that as the sizes of the assayed DAP ChIP-seq datasets increase, our understanding of the mechanisms of HOT loci will certainly improve. However, the core principles can already be inferred using the currently available datasets. Previous studies have used different metrics to define the HOT loci. For example, Wreczycka et al. 2019 used the 99th percentile of the density of TFBSs for a 500 bp sliding window, Remaker et al. 2020 used the window length of 2 kb and required >25% of TFs to be mapped, Partridge et al. 2020 used loci with >70 chromatin-associated proteins in 2 kb window. These heterogeneous definitions, however, fail to appreciate that the histogram of loci binned by the number of harbored TFBSs represents an exponential distribution (Fig 1A, Fig S1). We, therefore, applied our analyses both to the binarily defined HOT and non-HOT loci, as well as to the overall spectrum of loci in the context of TFBS density. This approach allowed us to better understand the correlations of characteristics of loci with the TF activity. Noticeably, this approach showed us that the HOT loci have their propensities to engage in long-range chromatin contacts with other equally or more DAP-bound loci than less active ones, making it more clear that the HOT loci are located in 3D hubs and FIREs (Fig 3A). Using the datasets generated in H1 we established that only <10% of the HOT loci in two differentiated cell lines overlap with the HOT loci of stem cells. This points to the high tissue-specificity of the HOT loci. Previous studies have also concluded that the HOT loci are not constitutive by nature, and are established in a dynamic manner after the differentiation. Of note, we used the datasets related to H1 cells in order to study the developmental aspects of the HOT phenomenon, and due to the much smaller sizes of the available datasets, we did not include the H1 in other parts of the analyses. We conducted our analyses using a more comprehensive set of datasets of functional genomics including Hi-C data, eQTLs, raQTLs, etc. Our approach of splitting the HOT loci into enhancer and promoter regions allowed us to detect distinct patterns characteristic of these two categories. While the HOT promoters and enhancers share some sequence features, they are bound by a distinct set of DAPs and possess different biases in enrichments of different types of QTLs. We have analyzed the patterns of DAPs in HOT loci using PCA in a similar way described in Partridge et al. 2020, which was conducted only on chromatin-associated proteins in HepG2 since we asked if the findings of the study conducted only on chromatin-associated proteins hold true for the HOT loci defined using an unbiased set of DAPs, and we observed that the chromatin-associated DAPs can be distinctly separated from the other transcription-related DAPs. Previous studies have carried out extensive mapping of the binding motifs of TFs to the HOT loci and identified a small set of “anchor” binding motifs of a few key tissue-specific TFs, and proposed that perhaps these driver TFs initiated the formation of HOT loci, similar to how the pioneer factors function. More importantly, more studies have come to the conclusion that the overwhelming majority of the peaks do not contain the corresponding motifs and that most of the mapped peaks represent indirect binding through TF-TF interactions. We relied on the conclusions of these studies in making the assumption that the inexplicably high density of DAPs could not be explained by the direct binding events and did not carry out the analyses based on DNA-binding motifs. Interestingly, the high prediction accuracy of our deep learning model is in agreement with the notion of the existence of shared motifs among the HOT loci but also implies that the indirectly bound loci also carry shared sequence features, perhaps other than the binding motifs or weak motifs which are not detected using the traditional PWM-based tools of motif detection. More studies are needed to further categorize the HOT loci along with the binding affinities of TFs. Another model that has been increasingly attributed to the formation and maintenance of long-range 3D chromatin interactions involves phase-separated condensates. Some enhancers (dubbed MegaTrans enhancers) were shown to drive the formation of large chromosomal assemblies involving a high concentration of TFs. In general, it has been increasingly appreciated that condensates ubiquitously attract and activate enhancers. The property of the condensates, which is of special interest to this study, is the capacity to serve as a “storage” of factors and co-factors inside the phase-separated droplets. For instance, the condensates can store hundreds of p300 molecules at active enhancers such that their catalytic histone acetyltransferase activity is decreased while in the phase-separated state, essentially kept in dormant mode until released. The detection of condensates relies on low-throughput live cell imaging methods such as FISH, which often involves only a few tagged molecules. Therefore, currently, there are no datasets of condensate formation with large numbers of molecules simultaneously that we could use to make statistical inferences. However, there is already an increasing body of research reporting that many transcriptional activities are driven by the formation of condensates, where each of them studies individual proteins in their contexts. Based on all this, we postulate that the HOT loci might be the loci where transcriptional condensates form. Once the condensates of sufficient size form, the kinetic trap that it creates can facilitate the accumulation of a soup of DAPs, which then can undergo high-intensity protein-protein and protein-DNA interactions, many constituents of which then get mapped to the involved DNA regions upon ChIP-seq experiments. Condensates formed at different foci in the nucleus have been shown to acquire physiochemical properties depending on their functions. For instance, the sizes of the transcriptional condensates have been shown to be regulated by the concentration of RNA molecules contained in them. Initially, RNA molecules serve as scaffolds to form the condensates, however, once the concentration of nascent RNA starts to increase due to transcription the condensates dissolve, providing a regulatory feedback loop for the condensates, thus explaining the phenomenon of transcriptional bursts. Another aspect that this RNA-based condensate regulation explains is the enrichment of transcribed RNAs in the active enhancers. Indeed, we observed extreme enrichment of eRNAs in HOT enhancers (Fig S18), further supporting the condensate hypothesis of the HOT loci. With the condensates assumed, the HOT loci become all the more explainable since ChIP-seq extracts the reads from populations of millions of cells, amounting to an average of many underlying protein-protein and protein-DNA interactions. With the advent of more precise protocols such as CUT&RUN, micro-C, and single-cell versions of ChIP-seq, ATAC-seq combined with bigger databases of experimentally verified condensate studies, we will have a better understanding of how the HOT loci form and gain insights into the causal relations between the high concentrations of DAPs and the transcriptional condensates.",10.1101/2023.02.05.527203
PMC10723289,38106203,Imputing Single-Cell Protein Abundance in Multiplex Tissue Imaging,"Multiplex tissue imaging are a collection of increasingly popular single-cell spatial proteomics and transcriptomics assays for characterizing biological tissues both compositionally and spatially. However, several technical issues limit the utility of multiplex tissue imaging, including the limited number of RNAs and proteins that can be assayed, tissue loss, and protein probe failure. In this work, we demonstrate how machine learning methods can address these limitations by imputing protein abundance at the single-cell level using multiplex tissue imaging datasets from a breast cancer cohort. We first compared machine learning methods’ strengths and weaknesses for imputing single-cell protein abundance. Machine learning methods used in this work include regularized linear regression, gradient-boosted regression trees, and deep learning autoencoders. We also incorporated cellular spatial information to improve imputation performance. Using machine learning, single-cell protein expression can be imputed with mean absolute error ranging between 0.05–0.3 on a [0,1] scale. Our results demonstrate (1) the feasibility of imputing single-cell abundance levels for many proteins using machine learning to overcome the technical constraints of multiplex tissue imaging and (2) how including cellular spatial information can substantially enhance imputation results.","Multiplex tissue imaging (MTI) are a set of single-cell spatial proteomics and transcriptomics assays for highly detailed profiling of biological tissues. With MTI, single-cell abundance levels and spatial distribution of 10–150 of proteins and/or 500–2000 RNAs can be quantified simultaneously. MTI enables characterization of individual cells as well as tissue organization, and MTI has been used in studies of healthy tissue, COVID, cancer, and other diseases. There are many MTI platforms, including cyclic immunofluorescence (CycIF), CO-Detection by indEXing (CODEX), CosMx, Xenium and multiplex immunohistochemistry. MTI has been used to generate large datasets in NIH consortia such as the NIH Human BioMolecular Atlas Program (HuBMAP) and the NCI Cancer Moonshot Human Tumor Atlas Network. MTI is also an increasingly common assay in cancer, where it has proven important for quantifying tumor spatial organization and microenvironment heterogeneity and connecting these features to cancer subtypes, prognosis, and therapy response. However, several key factors limit the usefulness of multiplex tissue imaging. Only 10–150 proteins and/or several thousand RNAs can be assayed in a single experiment, and hence the information obtained from a single experiment is bounded. Further, MTI assays can suffer from several technical issues that reduces the information obtained, including tissue loss or folding, probe failure, illumination artifacts, or errors in downstream image processing. These limitations have a profound impact on MTI data quality and substantially reduce the overall utility of MTI. To mitigate these limitations and improve utility of MTI, machine learning and deep learning approaches can be used to computationally increase the numbers of proteins/RNAs available from MTI and mitigate assay failures. Computationally increasing—or imputing—additional data by filling in missing data with predicted values is already common in other molecular assays, such as single cell RNA sequencing (scRNA), bulk genomics, and bulk transcriptomics. While imputation has been applied to MTI images, to the best of our knowledge imputation on MTI single-cell datasets has not been explored. Imputing single-cell data is especially valuable because single-cell datasets require fewer computational resources to process than images and can be readily integrated with other molecular datasets. In this study, we applied machine learning (ML) and deep learning (DL) methods to impute protein abundance in tissue-based cyclic immunofluorescence (t-cycIF) datasets obtained from breast cancer tissues. We evaluated the performance of ML and DL methods to predict protein abundance levels in t-cycIF single-cell datasets that included 20 proteins. Three distinct ML/DL approaches—regularized linear regression, gradient-boosted trees, and auto encoders—were used and evaluated for in-patient and across-patient imputation. Spatial information was introduced as well to improve imputation results. Overall, our results indicate that accurate imputation is possible for many proteins and that spatial information significantly improves imputation results.","Experimental Setup The BOND RX Automated IHC/ISH Stainer was used to bake FFPE slides at 60°C for 30 minutes, to dewax the sections using the Bond Dewax solution at 72°C, and for antigen retrieval using Epitope Retrieval 1 (Leica™) solution at 100°C for 20 minutes. Slides underwent multiple cycles of antibody incubation, imaging, and fluorophore inactivation. All antibodies were incubated overnight at 4°C in the dark. Slides were stained with Hoechst 33342 for 10 minutes at room temperature in the dark following antibody incubation in every cycle. Coverslips were wet-mounted using 200 μL of 10% Glycerol in PBS prior to imaging. Images were acquired using a 20x objective (0.75 NA) on a CyteFinder slide scanning fluorescence microscope (RareCyte Inc. Seattle WA). Fluorophores were inactivated using a 4.5% H2O2, 24 mM NaOH/PBS solution and an LED light source for 1 hour. The detailed protocol is available in protocols.io (dx.doi.org/10.17504/protocols.io.bjiukkew). Data Preparation The original source files include X and Y spatial coordinates and bio-morphological information (orientation, area, extent, etc.) for each cell. These features are removed for the initial imputation experiments, which solely rely on protein information. To prepare the available data for the machine learning models and deep learning networks, we used Min-Max Scaling to scale features to be in the [0,1] range. Statistical Validity: For robust statistical validity, we conducted more than 30 experiments (n > 30) for each protein imputation and each model. Elastic Net To setup an experiment using the Elastic Net, the scikit-learn library was used and within this library the ElasticNetCV, which automatically performs cross validation of error values. To support our results with statistical significance as well as a high enough numbers of trials, we performed multiple experiments n > 30. Each run was performed using a different random seed. Light GBM To setup a training and evaluation pipeline for our Light GBM model, we used the Ludwig platform, which enables “End-to-end machine learning pipelines” in a low code environment. To setup a Ludwig network only a config file specifying the features, in this work the proteins, and the target to be imputed is required. We automated this process by using a combination of shell and make scripts. Each run was assigned a different random seed to ensure reproducibility. Auto Encoder The Auto Encoder imputation was setup to make use of an iterative approach. As a first step, the pre-processed source data was loaded. To impute a specific protein, the protein values were replaced with a mean value calculated by using all available values for the protein in question. After the dataset was prepared and the protein in question replaced, the data was used as input for the auto encoder. A full encode and decode process was performed (Fig. 2), and the output was stored as an intermediate result. From this intermediate result, the imputed protein values were taken and used as a replacement of the mean protein values created in the preparation step. After this, another round of imputation was performed. This process was preformed 10 times, which resulted in a 10-step iterative imputation process. Reported MAE and RMSE values are calculated by using the last 5 iterative decoding’s, calculating the mean of the decoding’s for the protein and then calculate the MAE and RMSE. For each model and network, we performed a multitude of experiments, with a minimum of at least 30 experiments. To create reproducible but different results, each experiment used a unique random seed.","Study Cohort and Analysis Overview: The multiplexed tissue imaging single-cell datasets used in this study were generated using a 20-plex t-CycIF protein panel applied to a cohort of hormone receptor-positive (HR+), HER-2 negative metastatic breast cancer biopsies. The tissue biopsies and datasets are part of the NCI Cancer Moonshot Human Tumor Atlas Network and have detailed associated clinical metadata. Our dataset includes a total of 8 biopsies derived from 4 patients (Fig. 1a) that received a CDK4/6 inhibitor in combination with endocrine therapy, which is a common combination therapy in metastatic HR+ breast cancer. Each patient contributed a pair of biopsies, a pre-treatment biopsy and a biopsy taken at the time of tumor progression. Image stacks collected from t-CycIF were processed using the MCMICRO image analysis pipeline to generate single-cell feature tables (Fig. 1b). Each row in the table is a single cell identified in the image, and the table columns are the protein abundance levels calculated via mean pixel intensity per cell. In total 475359 single cells were identified across all biopsies, with an average of 59400 cells per biopsy. To perform in-patient evaluation, either the pretreatment or the posttreatment biopsy was used for training a machine learning model while the remaining biopsy was used for testing model performance. Biopsy timing was not used in this study. In total 16 proteins were shared between all biopsies, including 8 proteins for identifying cell types (lineage proteins) and 8 proteins for characterizing cellular functional states (functional proteins) (Table 1). The imputation task in this study was to predict protein abundance levels for a withheld protein or set of proteins. For each machine learning experiment, one or more proteins was withheld and used as the target variable(s) for the predictive model, and the remaining protein abundances were used as input features for the model. This task simulates the key application for imputation in multiplexed tissue imaging: computationally increasing proteins not originally included in an MTI assay or inferring protein levels where the assay failed (Fig. 1c). Three machine learning methods were used for imputation: elastic-net (EN) regularized linear regression, light gradient-boosting machine (LGBM), and neural network autoencoders (AE). Each model has advantages and disadvantages. As a linear model, EN is constrained to learning linear relationships between model inputs and outputs whereas LGMB and AE can learn non-linear relationships. EN and LGBM are easy to set up and minimal adjustments are required to get started. However, EN and LGBM can impute only one protein per model, resulting in the need to train a model for each protein of interest. This in turn requires a time-consuming process of training many models. In contrast, a single AE model can be trained once and then used to impute any single protein or even multiple proteins at once. However, due to the compression and reconstruction techniques applied in AEs, they sometimes approximate data with some loss of precision, which may lead to reduced performance compared to EN and LGBM approaches. Imputation model training and evaluation was performed both within patients and across patients (Fig. 1d). For the in-patient (IP) strategy, a model was trained on a single patient biopsy, and then model performance was measured using the other biopsy of the same patient. For the across-patient (AP) strategy, a model was trained on all biopsies except those from a single patient, and then model performance was evaluated using the biopsies of the single patient held out of training. The IP approach was anticipated to produce optimal performance because inter-patient variability is not present to reduce accuracy, whereas the AP approach is likely to be less accurate due to patient variability. Importantly, the AP approach approximates real-world usage. We evaluated the model performance using the mean absolute error (MAE). Preprocessing was performed to remove all columns from the datasets except protein intensities, followed by a min-max scaling approach, which maps values between 0 and 1. Thus, model error is in the range [0,1] where lower error represents better performance. Protein abundance imputation with elastic net and light gradient-boosting machines We used the elastic net (EN) regularized linear regression approach as a baseline model because it is simple, robust, and widely used in modeling high-dimensional omics datasets. EN imputation accuracy ranged from 0.05 and 0.20 MAE in both IP and AP experiments for twelve of sixteen proteins (Fig. 2a), demonstrating that it is possible to accurately impute many protein abundance levels. Proteins CK17 and Ki67 were most accurately imputed with MAE of 0.05. Proteins where imputation MAE was worse than 0.2 included CK19, ER, CK14, and PR. CK19 (1.28) and ER (0.32) protein expression exhibited some of the highest variances (Table S3). These markers are likely to exhibit patient-specific patterns that may be difficult for ML models to learn. In addition, for these proteins the AP MAE was notably higher than the IP setting. Reduced imputation performance for these proteins can be explained by noting that HR+ breast cancers are enriched in these proteins, and patient-specific patterns in these proteins are common. Wide-spread abundance levels of these proteins and patient-specific patterns prevent accurate imputation (Fig. 2b). Unexpectedly, imputation accuracy for proteins EGFR, Ki67, and Vimentin was higher in AP experiments compared to the IP experiments. Using Light Gradient Boosting Machine (LGBM) yielded improved imputation accuracy compared to EN. Imputation accuracy differences between IP and AP analyses using the LGBM were less pronounced relative to EN, especially for proteins CK19 and ER (Fig. 2c). Like the EN, LGBM performance for the same twelve of sixteen proteins was between 0.05 and 0.20 MAE in IP and AP settings. LGBM imputation accuracy for CK19 and ER are like the EN and greater than 0.2 MAE. Overall, LGBM displayed more accurate imputation results than EN both within and across patients (Table 2). Of particular significance were imputation accuracy for Ki67, pERK, EGFR, and AR in the AP context, which show improved performance compared to the IP setting. The difference between IP and AP performance of the CK19 protein is also much smaller when using LGBM compared to the EN. Importantly, LGBM imputation accuracy in the AP experiments is comparable to that in IP experiments, and mean accuracy as well as the standard deviation across all proteins was improved relative to the EN model. Protein abundance imputation using autoencoders and all model comparisons: Auto Encoders (AE) are deep learning neural networks for accurate reconstruction of high-dimensional data that include two distinct components: (1) an encoder network that maps a high-dimensional input to a lower-dimensional representation in a latent space and (2) a decoder network that reconstructs the original high-dimensional input from the low-dimensional latent space representation. The goal of an AE is to perform information-preserving dimensionality reduction of its input to the latent space so that it can then accurately reconstruct the input from the latent representation. AEs have been successfully used for imputation in various biological domains, including single-cell RNA and genomics and more. Unlike LGBM and EN models, AEs can impute multiple features simultaneously because their output is a full reconstruction of the entire input. Leveraging this capability, we performed both single-protein and multi-protein imputation experiments. Multi-protein imputation was performed based on the order in which the proteins were assayed during t-CycIF’s multiple imaging rounds. After training the AEs, we used a standard iterative approach for imputing cellular protein abundances using three steps: (i) for each target protein to be predicted, start by replacing the target(s) abundance with 0 or the mean protein abundance from the training dataset; (ii) repeatedly use the AE to predict protein abundances and then use these predictions as new inputs to the AE; and lastly (iii) use the last AE prediction for each protein as the final AE prediction (Fig. 3). AEs accurately imputed proteins in both single and multi-protein experiments (Fig. 4a, Fig. 4b). Imputation accuracy within patients is better than across patients, but the difference in accuracy for most proteins is less than 0.05 MAE. Imputation accuracy of CK19 levels is between 0.15–0.35 MAE, while imputation of the best performing proteins, CK17 and p21, is between 0.05 and 0.10 MAE. Like the EN and LGBM models, imputation performance is worst for the proteins with the most variable abundance levels in our breast cancer cohort, including CK19, ER, and PR. Performance differences between the imputation of single proteins and that of multi-proteins are minimal (Fig. 4c). We next compared performance for all three machine learning models used for imputation. Overall, LGBM performed best, followed by the EN and the AEs. These performance differences are consistent in both IP and AP settings (Fig. 4c). However, performance differences between the models are relatively modest, with the LGBM achieving a mean accuracy of 0.10 MAE in the AP experiments, followed by the EN with a mean accuracy of 0.11 MAE, and the AEs with a mean accuracy of 0.13 MAE. Using cellular spatial information to improve imputation: A key advantage of multiplexed tissue imaging datasets is that the spatial coordinates of each cell are known, making it possible to quantify spatial information around individual cells. We hypothesized that the spatial information available in t-CycIF could be used to improve imputation performance. To test this hypothesis, we quantified the spatial cellular context surrounding a target cell by calculating the mean protein abundance of neighboring cells. Average abundance levels of all proteins in neighboring cells were then added to our prior set of input features to create a feature set that includes both single-cell protein abundances plus average neighbor abundances (Fig. 5a). When no neighboring cells were detected, a value of 0 was assigned for neighbors’ protein abundances. Radii of 15, 30, 60, 90 and 120 micrometers were used to identify neighboring cells and assess the impact of using different sizes of radii on imputation performance. Only features for one radius setting were used for training a model, and hence a single set of spatial features was included as input for a predictive model. A radius of 15 micrometers captures most of the immediate neighbors of a cell, whereas larger radii capture the extended neighborhood of a cell. In these imputation experiments, only LGBM and AEs were used due to LGBM’s superior performance compared to ENs and AE’s multi-protein imputation capability. Using spatial information improved overall imputation accuracy. Importantly, imputation accuracy for the proteins that had proven difficult to impute due to their very high levels of variance (Supplements) - CK19, ER, and PR - was improved significantly with spatial information (Fig. 5b). LGBM performance also improved for other proteins such as CK17, CD45, Ecad, ASMA and p21. Performance of the AE achieved improvements in single-protein imputation for most markers, with CK19 showing the greatest improvement (Fig. 6a). Multi-protein imputation also benefited from spatial information integration (Fig. 6b), however performance gains were not as pronounced compared to the single protein imputation model. Aligned with prior research, imputation accuracy generally improves up to a certain neighborhood radius and then plateaus or declines (Table 3, S1, and S2). However, the LGBM does not show the same improvement up until a certain radius, but instead remains largely steady, with a peak performance observed using 60μm (Table 3, S1, and S2). Performance in both IP and AP imputation is improved by incorporating spatial information (Table S1 and S2).","In this study, we used machine learning models to accurately impute single-cell protein abundance levels using datasets obtained from the t-CycIF multiplexed tissue imaging assay. Our datasets included eight biopsies from a cohort of four metastatic breast cancer patients, making it possible to train and evaluate ML models for both in-patient (IP) and across patient (AP) imputation. Within a range of [0,1], imputation performance ranged from 0.05–0.15 mean absolute error for most proteins. Proteins with particularly high variance in our cohort, such as CK19 and ER, were more difficult to impute and imputation MAE for these proteins were 0.15–0.35. The LGBM model, a gradient-boosted regression tree approach, was the best performing model amongst all tested ML algorithms. Making spatial features available to ML models, in the form of neighboring cells’ protein abundance levels, improved their performance with a reduction of 0.02 in MAE on average. Improved performance was especially notable for proteins with high variance that were otherwise difficult to impute. This result complements recent research indicates that cell communication may vary and requires careful evaluation using multiple cellular neighborhoods. Our results are concordant with this observation as they show a similar pattern of improved protein performance using a diverse set of radii. While the LGBM shows the overall best performance, there are tradeoffs to consider when choosing a machine learning model for imputation for multiplexed tissue imaging datasets. Traditional ML models such as LGBM and EN can only impute one protein per model, which requires training and storing a model for each protein to be imputed. Using a single model for each protein is time and cost inefficient. In contrast, an AE is capable of imputing multiple proteins at once and can also impute all proteins included in their training data, requiring only a single training session and model. While AEs perform marginally worse than LGBM and EN for protein imputation, their capability for multi-protein imputation offers an advantage in reduced training time and cost. Multi-protein imputation, as opposed to sequential imputation, also models inter-protein relationships, and potentially yields more biologically pertinent relationships to explore. Limitations of this work include the small number of proteins used for imputation, the cohort composition of metastatic breast cancers, and the small sample size. This analysis used the sixteen proteins that were shared amongst all biopsies, and it is uncertain if other proteins can be imputed as accurately as these sixteen. This analysis also focused on breast cancer biopsies and diseased tissue, and imputation results may be different in healthy tissue or in other diseases. Furthermore, this study uses data derived from four patients. Data from these patients may not be representative of a larger patient cohort, and hence it is difficult to understand how well our results generalize. A study like ours would benefit from using a larger and more diverse cohort, potentially with more proteins and different multiplexed tissue imaging assays. In summary, this study demonstrates that machine learning can be used to accurately impute single-cell protein abundance levels within and across patients using multiplexed tissue imaging datasets. Three diverse ML models were all able to impute single-cell protein abundance levels well. Finally, incorporating spatial information significantly improves imputation performance, suggesting that local cellular neighborhoods are important determinants of cellular protein abundance levels.",10.1101/2023.12.05.570058
PMC10634784,37961168,Forecasting dominance of SARS-CoV-2 lineages by anomaly detection using deep AutoEncoders,"The COVID-19 pandemic exemplified the need for a rapid, effective genomic-based surveillance system to predict emerging SARS-CoV-2 variants and lineages. Traditional molecular epidemiology methods, which leverage public health surveillance or integrated sequence data repositories, are able to characterize the evolutionary history of infection waves and genetic evolution but fall short in predicting future outlooks in promptly anticipating viral genetic alterations. To bridge this gap, we introduce a novel Deep learning, autoencoder-based method for anomaly detection in SARS-CoV-2 (DeepAutoCov). Trained and updated on the public global SARS-CoV-2 GISAID database. DeepAutoCov identifies Future Dominant Lineages (FDLs), defined as lineages comprising at least 25% of SARS-CoV-2 genomes added on a given week, on a weekly basis, using the Spike (S) protein. Our algorithm is grounded on anomaly detection via an unsupervised approach, which is necessary given that FDLs can be known only a posteriori (i.e., after they have become dominant). We developed two concurrent approaches (a linear unsupervised and a posteriori supervised) to evaluate DeepAutoCoV performance. DeepAutoCoV identifies FDL, using the spike (S) protein, with a median lead time of 31 weeks on global data and achieves a positive predictive value ~7x better and 23% higher than the other approaches. Furthermore, it predicts vaccine related FDLs up to 17 months in advance. Finally, DeepAutoCoV is not only predictive but also interpretable, since it can pinpoint specific mutations within FDLs, generating hypotheses on the potential increases in virulence or transmissibility of a lineage. By integrating genomic surveillance with artificial intelligence, our work marks a transformative step that may provide valuable insights for the optimization of public health prevention and intervention strategies.","The COVID-19 pandemic has exerted an unprecedented impact on global health, economy, and daily life causing nearly 7 million deaths between December 2019 and October 2023. Since its first introduction in our species, the original SARS-CoV-2 Wuhan strain has rapidly diversified in distinct lineages classified by specific genotypic mutations in the Spike (S) glycoprotein. The Centers for Disease Control and Prevention (CDC) has also designated several Variants Being Monitored (VBM) and Variants of Concern (VOC) – e.g., Alpha (B.1.1.7), Beta (B.1.351), Gamma (P.1), Delta (B.1.617.2), or the currently circulating Omicron (B.1.1.529) and their lineages – associated with increased transmissibility, more severe disease (e.g., increased hospitalizations or deaths), significant reduction in neutralization by antibodies generated during previous infection or vaccination, or reduced effectiveness of treatments or vaccines. SARS-CoV-2 variants and lineages have been attributed to the occurrence of secondary, tertiary, quaternary, and quinary COVID-19 epidemic waves. As hundreds, if not thousands, of new spike (S) protein variants continue to emerge, it is unfeasible to conduct experimental risk assessments whenever a new variant or lineage is identified. Therefore, one significant public health challenge is the need to develop early warning systems capable of quickly evaluating emerging strains in terms of their epidemic or pandemic threat potential. The World Health Organization has outlined specific criteria to select vaccine reference strains in the context of circulating strains. Unfortunately, the impossibility to predict which virus variant will gain dominance in any area of the world and how long that variant will remain dominant is a practical limit as to how often vaccine composition changes can be implemented either regionally or globally. Artificial Intelligence (AI), an especially Deep Learning methods, has been used to predict SARS-CoV-2 mutations at specific amino acid sites of the S protein or characterize drivers of emerging strains evolution by calculating antibody escape and binding affinity of the protein to ACE2, which allows viral entry in host cells. In the present study, we propose DeepAutoCoV: a novel approach based on an AutoEncoder model to predict future dominant lineages (FDLs) of SARS-CoV-2. We define an FDL as any lineage (including its sublineages) that will become predominant in the lineages landscape, i.e., a lineage that at any moment in time will constitute a sizable percentage (e.g., >25%, or any other percentage, depending on user-defined stringency criteria) of all the sequenced genomes in a specific time period. DeepAutoCoV is used to detect anomalies, i.e., data that do not fit learned pattern distributions. Our assumption is that, given a steady pathogenic genomic landscape, represented by all the recently sequenced SARS-CoV-2 genomes, FDLs are detectable as anomalies carrying specific genomic differences compared to the circulating ones, with such differences being the factor making them fitter and, therefore, dominant in the future. The key advantage of DeepAutoCoV is that it does not require any a priori assumption on which S protein sites or domains are more likely to mutate to generate FDLs.","Data sets We downloaded from GISAID all S protein sequences, along with their Pango Lineages, sampled between December 24th, 2019, and February 4th, 2023, and filtered the data by country to assure high quality, by retaining S protein sequences without missing or unrecognized amino acids, and with lengths in the range between the median length of the proteins of their lineage plus or minus 30 amino acids, consistent with S expected lengths (1241 to 1301). After filtering, we generated five distinct datasets: four representing, respectively, each nation of interest, and one including the remaining sequences of all other countries (Table S1). Feature extraction and AutoEncoder Deep Learning model We focused on the S protein as the target of antibody-mediated immunity and the primary antigen in current vaccines. We converted protein sequences into fixed-length numeric features using k-mers as previously described. The AutoEncoder, which is designed to handle k-mers in the input layer, consists of: an encoding layer, followed by a series of dense layers, and a dropout layer for reducing overfitting; the central part includes a dense layer with Gaussian noise and 32 neurons; the decoder mirrors the encoder’s structure but with an increasing number of neurons, and the output layer reconstructs the input data (see Supplementary Methods for details). The AutoEncoder is trained over ten epochs with a batch size of 256, utilizing the mean square error (MSE) loss function and Adam optimizer, with TensorFlow and Keras Python libraries. Anomalies are identified as data points deviating significantly from learned reconstructions by the MSE plus 1.5 times its standard deviation. The initial training is based on the sequences uploaded on dataset of each nation during the week when sequenced S proteins are available for the first time (Table S8). In the first week, each sequence belongs to the lineage that represents the steady-state virus landscape. To reduce the number of features (k-mers), we kept only k-mers that are present in at least 25% of the sequences in the first training set. In the following weeks, the initially trained AutoEncoder to predict if there are outliers (i.e., new dominant variants that do not belong to the steady state) among the newly introduced sequences. If so, we dynamically update the model by retraining it (see Supplementary Methods for details). For each dataset, the model is evaluated in terms of 1) the ability to detect at least one true new dominant lineage before it reaches 25% of frequency in respective dataset of nation, 2) the positive predictive value (PPV), i.e., the fraction of new dominant lineages that the model accurately recognizes (see Supplementary Methods for details). Model comparison We compared the performance of the AutoEncoder model by developing a supervised approach using Logistic Regression (LR) and a linear distance-based (DB) model i.e., an anomaly detection using linear distance instead of the features extracted by the AutoEncoder. The LR model establishes a relationship between a series of independent variables and a binary dependent variable, providing as output the probability that a particular observation belongs to one of the two categories (in our case Anomaly and non-Anomaly). We used the Logistic Regression function of the Scikit-Learn library. Briefly, each week following the baseline data is considered a test set, with FDL and non-FDL sequences. When an FDL reaches the 25% threshold, its sequences are added to the training set and the class is switched to non-FDL, i.e., the lineage is no longer an anomaly to be detected. The DB model ranks sequences according to the sum of k-mers. Sequences whose sum of k-mers deviates from the sum of k-mers in the training set are defined as anomalies. As with the AutoEncoder model, we considered the top 100 ranked by week to be marked as FDLs. The dummy method was implemented by assigning the FDL class to randomly sampled sequences (See Supplementary Methods for details).","Predicting FDLs is different from predicting existing lineages or mutations The problem of SARS-CoV-2 lineage and mutation prediction has been thoroughly studied. The goal of previous works has been focused, however, on predicting existing lineages, that is, given a new sequence, to label it according to a list of known lineages (a supervised multi-class problem); or predicting which mutations will likely arise or their effects. Our proposed task is a fundamentally different binary anomaly detection problem, i.e., given a new sequence, we aim to predict whether it belongs to new (unknown) lineages that in the future will become dominant, reaching the aforementioned 25% threshold. FDLs: assumptions and definition Ideally, DeepAutoCoV is designed to serve as part of a global or national genomic surveillance system generating and inputting new sequences into DeepAutoCoV in near-real-time. The model would, then, flag SARS-CoV-2 genomes as FDLs. Such genomes could be considered for further analyses and/or uses, e.g., as potential vaccine reference strains (Figure S1). To test its performance in successfully detecting FDLs within a given country/region, we simulated epidemic monitoring programs not only at a global scale, but also at the national level. To this end, we downloaded 5,728,544 S protein sequences annotated with their Pango Lineages available in the Global Initiative on Sharing All Influenza Data (GISAID) database from December 24th, 2019, to February 4th, 2023 (Table S1). We assume that collection dates correspond to submission dates. In other words, when a S protein is sequenced, it is assumed that it is immediately uploaded to the GISAID database. Based on the difference between the collection and upload dates, GISAID usually occur a few days to a few weeks from the sampling date. While in a real-time applications such a limitation would be significant, the assumption has no impact for training and validation steps. However, currently available sequence data in GISAID are, mostly, convenience samples with over- or under-representation of different countries that may not reflect actual diffusion of different lineages. Therefore, we considered a data set including sequences from all other countries (Global data set), as well as four country-based subsets (Table S1): United States of America (USA), United Kingdom (UK), Denmark, and France data sets. The four nations were selected to assess the robustness of our AI model’s ability of FDLs early identification, in settings with both high (USA, UK) and low (France, Denmark) sequence volume. We pre-processed each data set to ensure data quality and consistency (see Methods). We define dominant lineages independently for each dataset as the top 15 lineages in terms of fraction of total sequences belonging to the lineage over the whole timeline, which led to different dominant lineages for each dataset. Here we assume a 25% fraction to mark the change of status, from FDL to known dominant lineage, although such threshold is arbitrary and can be changed (see below) to reflect epidemiology and/or public health considerations. Surveillance implementation strategy and FDLs prediction We converted protein sequences into fixed-length numeric features representing small protein subsequences of fixed length k, called k-mers (see Methods). We focused on the S protein as the target of antibody-mediated immunity and the primary antigen in current vaccines. DeepAutoCoV is an AutoEncoder model, consisting of an encoder that compresses the input, a central layer, and a decoder, which reconstructs the input after compression (Figure 1). Anomalies are detected by identifying data points deviating significantly form their learned reconstructions (i.e., over 1.5 standard deviations from the median), based on the mean square error (MSE). DeepAutoCoV training process includes checkpointing and early stopping mechanisms to prevent overfitting and saving the model when it has achieved its best performance (see Methods). Each week, it is evaluated whether new dominant lineages reach 25% of frequency in the dataset of the corresponding nation. If so, we dynamically update the model by retraining it (see Supplementary Methods for details). As a result, the model continuously updates its understanding of the evolving sequence dynamics, enabling it to capture the latest information about the dominant variants, and becoming more robust against false positives. Since our goal was not to identifying all instances of new dominant variants, but a limited number of sequences that have a high probability of being outliers, we decided to optimize the Positive Predictive Value (PPV), calculated as the ratio between true positives and true positives + false positives. In other words, for the reasons explained above, we aim to optimize the number of genomes that we correctly flag as FDL. We compared DeepAutoCoV with another unsupervised approach, a linear distance-base detection strategy, based on the same features we use, as well as a supervised approach via logistic regression (LR), and a dummy method flagging FDLs randomly (See Methods and Supplementary Materials, and Table S7). RESULTS DeepAutoCoV flags FDLs with a median of 31 weeks in advance In the global data set, the model achieves a median prediction of 31 weeks (IQR 10; 42) in advance, i.e., 31 weeks before the 25% threshold is reached. For the global data set, USA, UK, and Denmark, the respective (median) values are 47 (IQR 37; 54) weeks, 47.5 (IQR 8; 63), 9 weeks (IQR 8; 23), and 11 weeks (IQR 7; 15) in advance. France, on the other hand, shows performance, with a median value of 16 weeks in advance (IQR 4; 21). Results from the USA dataset (Figure 2) show high variability, with difference between 1st and 3d quartile being 54 weeks; while the lowest variability is in the Denmark dataset with a difference of 8 weeks. In contrast, other methods show a lower performance, with the best result on the Global data set being 4 weeks (IQR 0; 8) with the linear approach, and 6 weeks (IQR 0; 8) for the dummy method. The supervised approach does not correctly flag any FDLs. DeepAutoCoV flags FDLs during their early emergence, proportional to the data set size (Table 1). The median first detection happens when FDLs have a frequency, among all the lineages sampled in given week, from 0.05% (global dataset) to 3% (France). To better understand best- and worst- case scenarios in our simulation, we focused on the five best and five worst FDL for each data set (Figure S2 and Figure S3), i.e., the FDLs predicted with either the largest (best) or the smallest (worst) advance notice. In the global data set best-case scenario (Figure S2a), the median delay between the very first appearance of an FDL and its discovery is 10 weeks (IQR 10; 15). Conversely, in the worst-case scenarios (Figure S2b), the model predicts FDLs with a median of 26 weeks in advance (IQR 11; 26), and again a median delay of 10 weeks (IQR 8; 19). We also calculated the total sequences correctly predicted as FDLs per dataset in their emergence period, i.e., before they reach the 25% frequency threshold (Table S2–S6). Lineage AY.43 exhibits the highest count of matches in the World dataset (4,262 identified instances). In the country-specific datasets (Figure 3), the top FDL matches are AY.103 for USA (6,351 matches); AY.4.2 for UK (2,940 matches); AY.4 for Denmark (1,395 matches) and AY.4 for France (117 matches). DeepAutoCoV also correctly identifies tens to thousands of FDLs in the global data set. As expected the number of correctly identified FDLs decreases with smaller datasets. Certain FDLs, such as AY.43 and B.1.1.7, become dominant both at the Global level and in country-specific datasets (UK, and Denmark). On the other hand, there are FDLs pertaining only to specific nations that are not listed among the Global dataset FDLs. For instance, XE became an FDL in the UK, but not in the USA, Denmark, or France. The median fraction of correctly identified FDLs, i.e., the PPV in the Global dataset is 0.27 (IQR 0.07; 0.29). In other words, in our genomic surveillance scenario where the top 100 flagged genomes are considered for further analyses, the weekly median of truly identified FDLs is 27 (Figure S4). For the other datasets (Figure S5), the PPV is 0.28 (IQR 0.09; 0.31) in the USA; 0.26 (IQR 0.07; 0.29) in the UK; 0.26 (IQR 0.05; 0.28) in Denmark and 0.38 (IQR 0; 0.40) in France. DeepAutoCoV outperforms all other approaches: in the Global dataset, other models show a PPV of 0.04 (linear-distance model); 0.02 (dummy approach); and 0 (LR; it does not correctly flag any FDL). Detail performance results are reported in Table S7. Other methods to identify FDLs or novel variants being monitored (previously labeled variants of interest or concern) are present in literature, but none of them is based on anomaly detection. In the paper by Begui et al the authors blend Machine Learning with pseudo molecular simulations for novel variant predictions. Unlike our purely data-driven approach, their methodology is based on both injected biological knowledge and transformer-based latent representations (NLP-based classification). We cannot compare their method and ours directly, as there are based on completely different data sets (their dataset is based on a smaller number of sequences and a shorter time (December 2019 - March 2022) and analyzes only global data. we were not able to obtain their dataset). Another relevant approach employs transformers to predict not the emerging lineages, but the mutation probability of each S amino acid. DeepAutoCoV detects vaccine reference strains months in advance DeepAutoCoV can help predicting in advance lineages that will become vaccines reference strains. Our simulation shows that, while FDA approved BA.1 vaccine in August 2022 DeepAutoCoV identifies it, as FDL, 17 months earlier. Similarly, BA.4 and BA.5 vaccines were approved in June 2022, but identified by our model 6 and 7 weeks earlier, respectively. In the case of the XBB.1.5 variant, vaccines were approved in June 2023, while the lineage was identified as an anomaly six months earlier. DeepAutoCoV can identify key FDL mutations DeepAutoCoV is interpretable and offers insights about key mutations in the S protein contributing to future dominance. In other words, we can identify critical positions in the primary amino acid sequence that are associated to the FDLs. The underlying mechanism revolves around the binary input representation, i.e, a sequence of 0s and 1s (absence or presence of specific k-mers). In turn, we can analyze sequences flagged as FDLs and extract k-mers that are wrongly reproduced by the model after compression (Figure 1A). By matching these k-mers with the S sequence, we can pinpoint critical mutations. For example, while flagging the AY.4 lineage (Delta variant) DeepAutoCoV flags the k-mers “NSH”, “SHR”, and “HRR” which occupy the positions 680, 681, and 682 respectively. This is indeed a critical position characterizing the Delta variant mutation P681R. This is a Furin cleavage site adjuvating Delta variant replication and transmission. Another example is the BA.2 (Omicron) lineage, where the model flags a k-mer pinpointing the Y505H mutation. This mutation destroys the hydrophobic interactions and has an important role in the diffusion of virus ","This study proposed DeepAutoCoV, a novel genomic AutoEncoder model to predict SARS-CoV-2 FDLs, defined as novel lineages that will reach a frequency of 25% or more among the sequenced genomes in any given week. We applied DeepAutoCoV to both global and national (USA, UK, Denmark, and France) SARS-CoV-2 S protein data sets and checked its robustness with decreasing number of available samples (from ~4.5 million to ~130,000). The USA Centers for Disease Control and Prevention (CDC) uses a four-label system to classify SARS-CoV-2 lineages: Variant of high consequence, Variant of concern, Variant of interest, and Variants being monitored. Typically, a dangerous lineage is flagged as such only after public health data showed its concerning capabilities, such as a spike in infections or hospitalizations. Our main purpose, on the other hand, was robust prediction of FDLs as early as possible. In our envisioned approach, DeepAutoCoV flags FDLs ahead of time, early enough to allow public health interventions, apt to potentially prevent FDL from becoming such and spreading in the population. FDLs detection can, therefore, be the first step in a coordinated genomic surveillance effort (Figure S1). Next steps after FDL identification would include (a) molecular simulations, i.e., computational techniques modeling the behavior of the S protein to study if the FDL structures could provide critical advantages to virus ability to spread, such as enhanced ability to bind the receptors; and (b) virulence assays or other phenotypic wetlab validation [-450]. As resources are limited, it would not be realistic to envision a global or national pandemic surveillance program providing virulence assays or phenotypic wetlab test to include all the sequenced SARS-CoV-2 genomes. For this reason, we designed DeepAutoCoV as the mean to extract a small, manageable number of sequences to be analyzed. In our experiment, DeepAutoCoV is used to retain the top 100 sequences per week in terms of MSE, i.e., the sequences that are most likely FDLs. As a contextual examples, after the end of our considered period (February 2023), new lineages took over the SARS-CoV-2 genomic landscape, with particular XBB.1.5 being the most transmissible Omicron subvariant identified to date. According to CDC data, XBB and XBB.1.5 accounted for 44.1% of COVID-19 cases in the USA on the week of Dec. 31, 2022, compared with 25.9% during the previous week. DeepAutoCoV identified XBB.1.5 as FDL in the World and USA datasets 8 weeks and 3 weeks before they reached the 25% threshold, respectively. Furthermore, DeepAutoCoV provides a list of specific mutations marking the candidate FDLs, providing a hypothesis generating pathway to gain biological insight on these FDLs: in particular our model identifies Q183E mutation that is an unusual mutation as having a global prevalence below 0.01% [54]. DeepAutoCoV has some limitations and makes two main assumptions: (1) the collection date provided with the GISAID sequences corresponds to the submission date (i.e., the sequences are collected and submitted the same day); and (2) when a FDL reaches a frequency of 25% or more among the collected sequences in a given week, it has emerged, i.e., it is no longer an FDL, and its diffusion has been noticed through public health data. In conclusion, our work marks a transformative step in integrating genomic surveillance with AI for enhanced epidemic surveillance, offering valuable insights for—early—public health responses. Additionally, the DeepAutoCoV framework is suitable not only for SARS-CoV-2, but generalizable to other rapidly evolving viruses.",10.1101/2023.10.24.563721
PMC10187275,37205341,Image-based identification and isolation of micronucleated cells to dissect cellular consequences,"Recent advances in isolating cells based on visual phenotypes have transformed our ability to identify the mechanisms and consequences of complex traits. Micronucleus (MN) formation is a frequent outcome of genome instability, triggers extensive disease-associated changes in genome structure and signaling coincident with MN rupture, and is almost exclusively defined by visual analysis. Automated MN detection in microscopy images has proved extremely challenging, limiting unbiased discovery of the mechanisms and consequences of MN formation and rupture. In this study we describe two new MN segmentation modules: a rapid and precise model for classifying micronucleated cells and their rupture status (VCS MN), and a robust model for accurate MN segmentation (MNFinder) from a broad range of microscopy images. As a proof-of-concept, we define the transcriptome of non-transformed human cells with intact or ruptured MN after inducing chromosome missegregation by combining VCS MN with photoactivation-based cell isolation and RNASeq. Surprisingly, we find that neither MN formation nor rupture triggers a unique transcriptional response. Instead, transcriptional changes are correlated with increased aneuploidy in these cell classes. Our MN segmentation modules overcome a significant challenge to reproducible MN quantification, and, joined with visual cell sorting, enable the application of powerful functional genomics assays, including pooled CRISPR screens and time-resolved analyses of cellular and genetic consequences, to a wide-range of questions in MN biology.","Recent advances in automated image analysis have led to the development of high-throughput platforms to isolate specific cell classes and match visual phenotypes to specific genetic and expression profiles. These platforms bring the power of pooled genetic screening and population-based analyses to a huge range of phenotypes that are defined solely by visual changes in subcellular features. One such feature are micronuclei (MN), nuclear compartments containing a few chromosomes or chromatin fragments that result from mitotic segregation errors and persistent DNA damage. Increased MN frequency is a hallmark of carcinogen exposure, cancer development, and aging, and MN are potent drivers of massive genome structure changes, pro-inflammation and metastasis signaling, and senescence. These processes are linked to MN rupture, which exposes the chromatin to the cytosol for the duration of interphase and may contribute to tumorigenesis, metastasis, aging, and inflammatory disorders. Most studies on the biology and consequences of MN formation and rupture take advantage of the fact that MN can be induced with high frequency in cultured cells, for instance by inhibiting the spindle assembly checkpoint kinase Mps1. However, these interventions cause diverse “off-target” nuclear and cellular changes, including chromatin bridges, aneuploidy, and DNA damage. Several sophisticated techniques have been developed to overcome this challenge that enrich or isolate MN or micronucleated cells, including live-cell imaging of single cell arrays, inducing Y chromosome missegregation by disrupting the centromere, and purifying MN from lysed cells by flow cytometry, which have led to new insights into MN rupture, function, and consequences. However, all have significant limitations for unbiased analysis of the cellular consequences of MN formation and rupture and lack features necessary to perform high-throughput analyses on micronucleated cells. For these studies, what is needed is a way to visually identify micronucleated cells within a larger population rapidly and robustly and target them for downstream analysis. Automated detection of MN in microscopy images using conventional morphological transformations is challenging due to the diversity of MN shapes and sizes, their similarity to nuclear features that co-occur at high rates, including nuclear blebs and chromatin bridges, and their frequent proximity to nuclei. To address this, we developed two image analysis pipelines that combine neural network-based pixel classification with pre- and post-processing steps to rapidly identify micronucleated cells from low resolution images (VCS MN) or segment MN with high recall across multiple cell lines, chromatin labels, and imaging conditions (MNFinder). We demonstrate the utility of this approach by combining VCS MN with a phenotype-based cell isolation method, called Visual Cell Sorting (VCS), to define the transcriptomic profile of hTERT-RPE1 cells with none, intact, or ruptured MN by RNAseq after inducing chromosome missegregation. During VCS, single cells expressing nuclear-localized Dendra2 are photoconverted on-demand based on the results of the MN classifier. Specific cell classes are then isolated by gating on Dendra fluorescence ratios during FACS. We show that we can recapitulate an established aneuploidy signature using this method and find that, surprisingly, neither micronucleation nor rupture is sufficient to induce substantial transcriptional changes in these conditions. We envision that the MN segmentation and cell isolation platforms described here will be widely applied to fundamental questions in cell division and nucleus biology and to cell-based models of human disease to enable new discoveries into the contribution of MN to cellular dysfunction.","Plasmid construction pLVX-EF1a-NLS-3xDendra2-blast was created by PCR of NLS-3xDendra2 from pLenti-CMV-Dendra2×3-P2A-H2B-miRFP using primer sequences 5’-caagtttgtacaaaaaagttggcaccATGG-3’ and 5’-TTAGGAAAAATTCGTTGCGCCGCTCCC-3’, followed by ligation into pLVX-EF1a-blast. pLVX-EF1a-H2B-emiRFP703-neo was created by PCR of H2B-emiRFP703 from pH2B-emiRP703 (a gift from Vladislav Verkhusha, AddGene #136567) with primers 5’-ATGCCAGAGCCAGCGAAG-3’ and 5’-TTAGCTCTCAAGCGCGGTGATC-3’, followed by ligation into pLVX-EF1a-neo. Cell culture and construction of cell lines hTERT RPE-1 cell lines were cultured in DMEM/F12 (Gibco) supplemented with 10% FBS (Sigma), 1% penicillin-streptomycin (Sigma), and 0.01 mg/mL hygromycin B (Sigma) at 5% CO2 and 37 °C. U2OS, hTERT-human fetal fibroblasts (HFF), and HeLa H2B-GFP cells were cultured in DMEM (Gibco) supplemented with 10% FBS and 1% pen/strep at 10% CO2 at 37 °C. For ATF3 validation, cells were incubated in 2 μg/mL doxorubicin hydrochloride (Fisher Sci) for 1 hour prior to fixation. For EGR1 validation, cells were incubated in 5 ng/mL hEGF (Peprotech) for 1 hour prior to fixation. For MN induction, cells were incubated 100 nM BAY1217389 (Msp1i, Fisher Sci) for the indicated times. hTERT RPE-1 NLS-3xDendra2/H2B-emiRFP703 and U2OS NLS-3xDendra2/H2B-emiRFP703 cell lines were produced through serial transduction of lentiviruses. RPE1 and U2OS cells were validated by STR sequencing. Lentivirus was produced in HEK293T cells using standard protocols and filtered medium was added with polybrene (Sigma, #H9268) for transduction. Cells were selected with 10 μg/mL blasticidin (Invivogen) and 500 μg/mL active G418 (Gibco) and FACS sorted on an Aria II sorter (BD Biosciences) for the top 20% brightest double positive cells. hTERT RPE-1 NLS-3xDendra2-P2A-H2B-miRFP703 cells were created through viral transduction and FACs sorting for the brightest double positive population. HeLa-H2B cells were a gift from Dr. Daphne Avgousti (Fred Hutchinson Cancer Center) and were originally acquired from Millipore (SCC117). HFF cells were a gift from Dr. Denise Galloway (Fred Hutchinson Cancer Center). Microscopy VCS experiments were performed on a Leica DMi8 widefield fluorescence microscope with Adaptive Focus outfitted with an i8 incubation chamber (Leica) with temperature (PeCon: TempController 2000–1) and gas control (Oko) and a Mosaic 3 Digital Micromirror (Andor). Images were acquired with a 20× 0.8 NA apochromatic objective (Leica) using an iXon Ultra 888 EMCCD camera and MetaMorph v7.1.0.1.161 (Molecular Devices). Fixed cell training and validation images were acquired with a Leica DMi8 laser scanning confocal microscope using the Leica Application Suite (LAS X) software and a 40x/1.15 NA Oil APO CS objective (Leica) or on a Leica DMi8 microscope outfitted with a Yokogawa CSU spinning disk unit, Andor Borealis illumination, ASI automated Stage with Piezo Z, with an environmental chamber and Automatic Focus using a 40x/1.3 NA Oil PLAN APO objective. Images on the spinning disk microscope were captured using an iXon Ultra 888 EMCCD camera and MetaMorph software (v7.10.4). Micronucleus segmentation and cell classifiers VCS MN: The neural net was created using the FastAI 1.0 library in Python, a UNet with Torchvision’s ResNet18 pre-trained model as its base architecture. Training for MN recognition was performed using ~2,000 images of individual cells as training data, a further 164 for validation, and 177 for testing. Training images were of RPE-1 NLS-3xDendra2-P2A-H2B-miRFP703 cells after incubation in 0.5 μM reversine (an Mps1 inhibitor, EMD Millipore) or DMSO for 24 h and taken with a 20x widefield objective on the VCS microscope. Nuclei were segmented on H2B channel images using the Deep Retina neural net and 48×48 px image crops were generated centered on each nucleus. For training, MN pixels in cropped images were manually annotated. MN associated with chromatin bridges were ignored to ensure that labeled MN were discrete nuclear compartments. The VCS MN classifier takes as input a 2-channel 20x image. It applies the Deep Retina neural net to the H2B channel to segment nuclei, discards any touching the edge of the image, and generates a 48×48 px crops centered on each nucleus. Each crop is processed with Sobel edge detection and linearly enlarged to 96×96 px. To accommodate the ResNet18 3-channel architecture, each crop is expanded to the H2B channel, a duplicate of the H2B channel, and the results of Sobel edge detection. Identified MN are mapped back to the full image and assigned to the closest segmented nucleus. MN more than 40 px away from a nucleus are discarded. Once MN are assigned to cells, the classifier calculates the maximum Dendra2 MN/nucleus intensity ratio for each MN. MN with a ratio below 0.16 are classified as ruptured. This threshold was identified using the JRip classifier in Weka 3.8.6 to define the optimal threshold to separate manually annotated intact and ruptured MN. Nuclear segments are classified as MN+ or MN− cells based on the presence or absence of an associated MN segment. MN+ cells are then further classified into those with only intact MN (rupture-) or those with at least one ruptured MN (rupture+). For analysis of MN recall and PPV, MN were segmented using PixelStudio 4.5 on an iPad (Apple). Recall was calculated as the proportion of all MN that overlapped with a predicted segment. Positive predictive value was calculated as the proportion of all predicted segments that overlapped with a MN. Mean Intersection over Union (mIoU) was calculated per object by quantifying the overlap between groups of true positive pixels and their respective ground truths. For analysis of U2OS cells, the VCS MN segmentation module was retrained on a collection of images of RPE1, U2OS, HFF, and HeLa cells after incubation in 100 nM BAY1217389 or 0.5 μM reversine for 24 h. Live images of RPE1 and U2OS NLS-3xDendra2/H2B-emiRFP703 cells were acquired on the VCS microscope at 20x. Images of fixed cells were taken on either the LSM or spinning disk confocal microscopes at 40x after fixation in 4% paraformaldehyde (Electron Microscopy Sciences, #15710) for 5 min at room temperature. Cells were labeled with DAPI as indicated. ~2,300 crops of U2OS NLS-3xDendra2/H2B-emiRFP703 cells taken on the VCS microscope at 20x were used for training with another 233 held back for validation and 910 for testing. Three images of Hoechst labeled U2OS cells taken at 20x on a widefield microscope at 16 bit depth were downloaded from the Broad Bioimage Benchmark Collection (BBBC039v1, and linearly scaled by 0.5. Crops were generated centered on manually-annotated cell nuclei and fed to VCS MN to determine PPV, recall, and mIoU for this data set. MNFinder: The MNFinder neural nets were created using TensorFlow 2.0 without transfer learning. Training was performed using 128×128 px crops generated from the same training and validation data used for retraining the VCS MN. For nucleus/MN segmentation (semantic segmentation) predictions are taken from two UNet-based neural nets, with MN predictions fed into a third ensembling UNet. All UNets are trained independently but are otherwise identical, save for the incorporation of multiscale downsampling into one of the input UNets. For cell segmentation (instance segmentation) a UNet architecture incorporating 3 decoder pathways is used to predict distance maps, proximity maps, and foreground pixels. The distance and proximity map decoders incorporate features from a UNet3+ design: specifically, additional skip connections from multiple layers of the encoder and decoder pathways and deep supervision during training. Training data were generated from annotated nuclei and MN images by generating a concave hull grouping a nucleus and associated MN using the cdBoundary package in Python. This hull was transformed into a distance map by calculating the Euclidean distance transform (EDT) with each pixel value encoding the shortest distance between that pixel and the background. Proximity maps were generated by setting all pixels as foreground pixels except for those belonging to other hulls and applying an EDT, masked by the cell’s boundaries, and raising this to the 4th power to sharpen edges. Both maps are scaled from 0–1 for each cell. MNFinder input images taken at 20x are cropped using a 128×128 px sliding window, advancing the window by 96 px horizontally and vertically to oversample the image. 40x images are scaled down by a factor of 2 prior to input. Crops are expanded into 2-channel images, with the second channel the result of Sobel edge detection. These images are processed by the neural nets, post-processed as described, and reassembled by linear blending into a complete field. Recall and positive predictive values were calculated using the same as for VCS classifier validation. MNFinder was validated on images of RPE1, U2OS, HFF, and HeLa cells after incubation in 100 nM BAY1217389 or 0.5 μM reversine for 24 h. Live images of RPE1 and U2OS NLS-3xDendra2/H2B-emiRFP703 cells were acquired on the VCS microscope at 20x. Images of fixed cells were taken on either the LSM or spinning disk confocal microscopes at 40x after incubation in 4% paraformaldehyde (Electron Microscopy Sciences, #15710) for 5 min at room temperature. PPV and recall for MN segmentation were calculated for individual input UNets and the ensemble UNet. Outline of VCS MN cell isolation experiments Cells for VCS were plated onto 6-well glass-bottom, black-walled plates at a density of 50,000–225,000 cells per well 1–2 days before activation. An extra unactivated well was plated as a control. One day before imaging, 100 nM Mps1i was added to the medium. One hour prior to imaging, cells were washed 1x in PBS and medium changed to phenol red free (GIBCO) containing 10 μM RO-3306 (Sigma). The plate was transferred to the microscope, the plate center and micromirror device were aligned, and the appropriate journals (see) were initiated for VCS activation. Imaging conditions were optimized for each experiment. Images were acquired using MetaMorph and analyzed on a dedicated linked computer. 1-bit masks of MN+ nuclei and MN− nuclei were transmitted back to Meta-Morph, which directed UV pulses at the segmented nuclei. Activation occurred using either a 200 ms or 800 ms pulse of the 405 nm laser. After imaging, the initial 5 positions and last 5 positions were reimaged for quality control as well as 5 random positions in the unactivated well. Classifier predictions were compared to the first 3 and last 3 images from each VCS experiment, each manually annotated prior to downstream analysis, including RNA extraction. Activated and unactivated cells were trypsinized, suspended in 2% FBS, and sorted using a FACS Aria II (BD Biosciences). Compensation for PE-blue excitation of unconverted Dendra2 was performed on the unactivated cells. Dendra2 activation-based sorting gates were defined on single cells positive for both Dendra2 and emiRFP703 using the PE-Blue-A/FITC-A ratio. Cells were sorted into 2% FBS then pelleted and either flash frozen on dry ice or replated onto poly-L-lysine coated coverslips. CellTrace: Activation and sorting accuracy were analyzed for RPE1 RFP703/Dendra2 cells by incubating cells in CellTrace far red (ThermoFisher) for 10 min at 37 °C, trypsinizing and pelleting cells, mixing 1:1 with unlabeled cells, and plating. A classifier segmented nuclei with the Deep Retina neural net on the GFP channel and measured the mean far-red intensity in the nucleus. Threshold intensity for activation was experimentally determined. Cells were sorted by FACs for Dendra2 ratio and CellTrace intensity using compensation to eliminate emiRFP703 spectral overlap and then reanalyzed on the same machine. Mps1i+/− isolation: Cells incubated in Msp1i or DMSO were imaged and activated using a random classifier. 1-bit masks of nuclei generated using the Deep Retina neural net were randomly assigned to receive 800 ms or 200 ms pulses. At least 13k cells were collected per sorting bin and samples were pelleted, flash frozen, and stored at −80°C. Micronucleus+/− isolation: Two wells were imaged sequentially per experiment with the activation time for MN+ and MN− nuclei reversed between wells. Rupture+/− isolation: Cells were plated 2 days before imaging in medium containing 1 μM Cdk4/6i (PD-0332991, Sigma). Twenty-four hours later, cells were rinsed 3x with PBS and the medium replaced with 100 nM BAY. Only 1 well was imaged per experiment with rupture− cells receiving 800 ms and rupture+ cells receiving 200 ms pulses. RNA isolation and sequencing We extracted RNA from frozen cell pellets using the RNAqueous micro kit (ThermoFisher), according to the manufacturer’s protocol. Residual DNA was removed by DNase I treatment and RNA was further purified by glycogen precipitation (RNA-grade glycogen; ThermoFisher) and resuspension in ultra-pure H2O heated to 65 °C. RNA quality and concentration was checked by the Genomics Core at the Fred Hutchinson Cancer Center with an Agilent 4200 Tapestation HighSense RNA assay and only samples with RIN scores above 8 and 28S/18S values above 2 were further processed. cDNA synthesis and library preparations were performed by the Genomics Core using the SMARTv4 for ultra-low RNA input and Nextera XT kits (Takara). Sequencing was also performed on an Illumina NextSeq 2000 sequencing system with paired-end, 50 bp reads. RNAseq and gene-set enrichment analysis We quantified transcripts with Salmon to map reads against the UCSC hg38 assembly at http://refgenomes.databio.org (digest: 2230c535660fb4774114bfa966a62f823fdb6d21acf138d4), using boot-strapped abundance estimates and corrections for GC bias. For comparisons with data from He, et al. and Santaguida, et al., the original FASTA files deposited at the Sequence Read Archive were downloaded with NCBI’s SRA Toolkit and quantified with Salmon. No GC-bias correction was applied as only single-end reads were available. Transcript abundances were processed to find differentially expressed genes (DEGs) with the R package DESeq2 version 3.16 in R 4.2.1, RStudio 2022.07.2 build 576, and Sublime Text build 4143. Files were imported into DESeq2 with the R package tximeta, estimated transcript counts were summarized to gene-level, and low-abundance genes were filtered by keeping only those genes with estimated counts ≥ 700 in at least 2 samples. DEGs were identified using a likelihood ratio test comparing the full model with one with the condition of interest dropped and an FDR of 0.05. Log-fold changes were corrected using empirical Bayes adaptive shrinkage. Operations were performed before pseudogenes were filtered from dataset. GSEA was performed using the R package fgsea version 1.25.1, comparing log-fold changes of all DEGs against the full Homo sapiens Hallmark Gene Sets version 2022.1, part of the MSigDB resource (UC San Diego, Broad Institute). Live-cell imaging for MN rupture frequency analysis RPE1 NLS-3xDendra2/H2B-emiRFP703 cells were plated 2 days before imaging and treated for 24 hours with either 1 μM Cdk4/6i or DMSO. One day before imaging, cells were rinsed and incubated in 100 nM BAY1217389. Nineteen hours later, the media was exchanged for Cdk1i medium, 5 positions were imaged in each well and rupture− cells were activated. These positions and the surrounding area were imaged every hour for 11 hours and the status of photoconverted cells manually recorded. Immunofluorescence (IF) Cells plated on poly-L-lysine coated coverslips or glass bottomed plates were fixed for IF in 4% paraformaldehyde for 5 min unless otherwise indicated. Cells were permeabilized for 30 min at RT in PBSBT (1xPBS (GIBCO), 3% BSA, 0.4% Triton X-100, 0.02% sodium azide (all Sigma)), followed by incubation in primary antibodies diluted in PBSBT for 30 min, secondary antibodies diluted in same for 30 min, and 5 min in 1 μg/mL DAPI (Invitrogen). Coverslips were mounted in VectaShield (VectorLabs) and sealed with nail polish before imaging. Primary antibodies used were: mouse-agH2AX (1:500; BioLegend, 613401), rabbit-a-ATF3 (1:400; Cell Signaling Technology, 18665), rabbit-a-EGR1 (1:1600; Cell Signaling Technology, 4154) and rabbit α H3K27Ac (2 μg/mL; Abcam, ab4729). Secondary antibodies used were: AF647 goat-a-mouse (1:1000; Life Technologies, A21236) and AF488 goat-a-rabbit (1:2000; Life Technologies, A11034). DNA FISH RPE1 cells plated onto poly-L lysine coverslips were fixed in −20°C 100% methanol for 10 min, rehydrated for 10 min in 1xPBS and processed for IF. Cells were then refixed in 4% PFA for 5 min at RT then incubated in 2xSSC (Sigma) for 2 × 5 min RT. Cells were permeabilized in 0.2 M HCl (Sigma), 0.7% TritonX-100 in H2O for 15 min at RT, washed in 2xSSC, and incubated for 1 h at RT in 50% formamide (Millipore). Cells were rewashed in 2xSSC, inverted onto chr 1, 11, or 18 XCE probes (MetaSystems), and the coverslips sealed with rubber cement. Probes were hybridized at 74°C for 3 min and then incubated for 4 hours (chr 18) or overnight (chrs 1 and 11) at 37 °C. After hybridization, coverslips were washed in 0.4xSSC at 74 °C for 5 min, then 2xSCC 0.1% Tween20 (Fisher) for 2 × 5 min at RT. DNA was labeled by incubation in 1 μg/mL DAPI for 5 min at RT, and coverslips mounted in VectaShield. Images were acquired as 0.45 μm step z-stacks through the cell on the confocal LSM with a 40x objective. Cells that had more or less than two FISH foci were classified as aneuploid for that chromosome. Image analysis Dendra2 ratio stability: Nuclei were segmented on images taken at the start and end of an Mps1i +/− VCS experiment by thresholding on the GFP channel, measuring the mean intensity of GFP and RFP, and calculating the RFP:GFP ratio per nucleus for each image group. MN+/− sorting accuracy: Cells replated and fixed after sorting were imaged on the LSM confocal at 40x with 0.45 μm z-stacks through the cell. Image names were randomized prior to quantification of MN+ cells. ATF3 and EGR1 intensity: Images were acquired as 0.45 μm step z-stacks through the cell on the confocal LSM with a 40x objective. Images were corrected for illumination inhomogeneity by dividing by a dark image and background subtracted using a 60 px radius rolling-ball in FIJI (v2.9.0). Single in focus sections of each nucleus was selected and nuclei masks generated by thresholding on RFP-NLS. Mean intensity of ATF3 or EGR1 were calculated for each nucleus and normalized for each replicate by scaling to the median value for the DMSO control. Statistics were calculated on the raw values. Statistical analyses Shorthand p-values are as follows: ns: p-value >= 0.05 *: p-value < 0.05 **: p-value < 0.01 ***: p-value < 0.001 ****: p-value < 0.0001 Generalized estimating equations (GEE) were used to determine statistical differences for nominal data with multiple variables using binomial distributions and a logit link function. For Fig. 1C–D, data were assessed using the formula: (# recalled, # missed) ~ MN status where MN status is whether ruptured or intact. For Fig. 5E, we also used a binomial distribution and a logit link function. For Fig. 5I–J and S5F–G, we used the formula: (# aneuploid, # normal) ~ Status × Chr where Status is whether the cell was MN+/− (Figs 5I, S5F) or Rupture+/− (Figs 5J, S5G) and Chr is chromosome identity. p-values for each individual property were calculated using the drop1 function in R. In Fig. 6D–E and S6 C–D, we used a gamma distribution and the formula mean intensity ~ Population. Statistical significance for differences between single nominal variables in other figures were by Barnard’s exact test. The predicted change to classifier PPV in Fig. 5E was determined by reducing the true positive rate in the rupture− population by the difference in mean rupture frequencies between the beginning and end of the experiments and increasing the true positive rate in the rupture+ population by the same.","Machine vision identifies micronucleated cells within a mixed population We initially developed an automated pipeline to identify micronucleated cells in a mixed population on hTERT RPE-1 (RPE1) cells. RPE1 cells are a near-diploid, non-transformed human cell line with very low frequencies of spontaneous MN that have been extensively used for studies of chromosome missegregation and micronucleation. RPE1 cells also do not activate the cGAS-STING innate immune pathway upon micronucleation, similar to many cancer cells. We anticipated that this would limit inflammatory signaling and increase the sensitivity of downstream analyses of the consequences of MN formation and rupture. To enable automated image analysis and live-cell marking, we co-expressed a fluorescent chromatin marker (H2B-emiRFP703) to identify nuclei and MN with 3xDendra2-NLS (nuclear localization signal) to identify ruptured MN and photoactivate selected cells (Fig. 1A). We treated these cells, referred to as RFP703/Dendra, with a low dose of an Mps1 inhibitor (Mps1i) to induce MN, which increased MN frequency to 50% of cells with 1 MN per cell on average (Fig. S1A–B). To distinguish MN from morphologically similar nuclear features also induced by Mps1i, including chromatin bridges and nuclear blebs, we trained a neural net classifier on low resolution single channel images of RFP703/Dendra after Mps1i incubation. For this first model, called VCS MN, we opted for low resolution training images, increased speed of analysis, and positive predictive value (PPV) privileged over recall to optimize downstream classifier integration into a visual cell sorting platform. To train the model, H2B channel images were passed to a Deep Retina neural net (Caicedo et al., 2019a) to generate nuclear masks, which were then used to crop the field into single cell images, excluding cells on the image edges. On average, 75% of nuclei per field were correctly segmented and cropped. A U-Net classifier using pyTorch’s ResNet18 pre-trained model as its base architecture was then trained on 2,000 single cell crops combining the H2B image with the results of Sobel edge detection. Classified MN pixels were converted to a mask that was mapped back onto the whole field image (Fig. 1B). MN segments were then assigned to “parent” nuclei by proximity, which correctly associated 97% of MN (Fig. S1C). Nuclei associated with at least one MN were labeled as MN+ cells and those associated with no MN were labeled as MN− cells. We validated the ability of VCS MN to classify cells on 6 whole field images from two experiments and calculated recall values of 86% and 65%, respectively, for MN− and MN+ labeled cells, and PPVs of 73% and 93% (Fig. 1C). Analysis of MN classification on cropped images found a recall value of approximately 70% and a PPV of 89% for MN identification (Fig. 1D), indicating that we successfully limited false positives in our MN+ cell class with the tradeoff of decreased purity of the MN− cell pool. We also observed a small, but statistically significant, reduction in recall for ruptured MN (Fig. 1D), likely driven by their smaller size. To determine whether this pipeline could achieve similar accuracy in another cell line, we retrained the VCS MN classifier on images of multiple cell lines acquired at two magnifications (see Methods) and assessed prediction quality on low-resolution images of U2OS cells induced to form MN. We calculated recall values of 82% and 83% in MN− and MN+ cells, respectively, PPVs of 81% and 83%, and an MN PPV of 88% (Fig. 1E). In summary, VCS MN can automatically identify the majority of micronucleated and non-micronucleated cells in low-resolution images of cells from multiple sources containing a mix of contaminating objects with high precision. RPE1 cells with ruptured MN were further classified from the MN+ population based on MN Dendra2 intensity: NLS-3xDendra2 is present in intact MN and absent in ruptured MN (Fig. 1F). We quantified the maximum Dendra2 intensity in the nucleus and corresponding MN segments and MN with a signal less than 0.16 of the nucleus was classified as “ruptured.” This threshold correctly classified approximately 90% of MN (Fig. S1D). When appended to the VCS MN pipeline, this analysis correctly identified 60% of rupture− cells (cells with only intact MN) and 70% of rupture+ cells (cells with at least 1 ruptured MN) with a PPV near 75% in both cases (Fig. 1G). The difference in recall is likely due to the increased probability of a multi-micronucleated cell being correctly classified as MN+ and having at least 1 ruptured MN (Fig. S1E–F). MNFinder accurately segments MN in images of attached cells Due to the analysis constraints we imposed, MN classified by the VCS MN module were typically undersegmented (Fig. 2A) and performance diminished substantially on images taken using different magnifications or with different chromatin labeling agents. Therefore, we developed a new module, called MNFinder, that privileges accurate MN and nuclear segmentation across cell types, DNA labels, and image resolution over PPV. MNFinder takes a single channel chromatin image as input and integrates the results of two independent nucleus/MN (nuc/MN) and cell segmenter pipelines to generate three object group: 1) MN, 2) cells, which group nuclei with associated MN, and 3) nuclei. Both the nuc/MN and cell pipelines use a UNet classifier for initial segmentation followed by additional image processing steps to refine the results (Fig. 2B). We made several changes to the neural net input and architecture to design MNFinder. To adjust for highly imbalanced data sets, we incorporated attention gates in the up-sampling blocks and employed focal loss during training. We also modified the classifier to segment both nuclei and MN to better discriminate between nucleusassociated and MN-associated pixels, increased the size of input image tiles from 48×48 px to 128×128 px to increase contextual information, and oversampled input images by 25%. Final classification results integrate the predictions from all crops containing a given object. For nucleus/MN segmentation, we tested a variety of UNet-derived architectures (Table 1) and found that a basic UNet with attention gates performed well across multiple cell lines. We also observed incorporating multiscale downsample blocks identified some MN otherwise missed, but produced an overall reduction in performance. Therefore we developed an ensemble classifier (Fig. S2A) that takes predicted MN weights from both UNet types as inputs to generate the final MN predictions. Nucleus predictions are retained from the basic attention gate UNet. This classifier was trained on images of live RPE1 and U2OS cells expressing H2B-emiRFP703 or fixed RPE1 cells, HeLa cells, and hTERT human fetal fibroblasts (HFF) labeled with DAPI using 128×128 random crops with image augmentation. To adjust for misclassification of large MN as nuclei, nuclei with an area below 250 px are automatically reclassified and undersegmentation of MN is further limited by expanding MN object boundaries to their convex hull (Fig. S2B). Cell identification is not possible with standard UNets. To overcome this limitation and improve nucleus segmentation, we developed a UNet and image processing pipeline that outputs “cell” masks, defined as the concave hull of each nucleus and its associated MN, based on predicted distance and proximity maps (Fig. S2C). Map predictions are derived from single channel image crops using a multi-decoder UNet with a UNet3+ architecture in the two main decoders. To improve accuracy and decrease training time, we added a third arm to the UNet that classifies foreground pixels and feeds these predictions into the distance and proximity decoders (Fig. S2D). Given the complexity of this UNet, we used a constant feature depth at every level in the encoder and decoder pathways and replaced most concatenation operations with addition. This UNet was trained on the same images as the nucleus/MN segmenter after annotation with concave hulls automatically generated on annotated MN and nuclei (Fig. S2C). To generate cell masks from the predicted distance and proximity maps, the results are summed and used for watershed segmentation followed by elimination of false boundaries based on the proximity map predictions of true cell boundaries (Fig. S2E). In the last step of the MNFinder module, the Nuc/MN and cell segmentation results are integrated to produce a final set of labels, identifying each unique MN, each cell with its nuclei and MN, and each unique nucleus (Fig 2B). We validated MNFinder on single channel images of RPE1 RFP703/Dendra, U2OS RFP703/Dendra, HeLa H2B-GFP, and HFF cells after incubation in Mps1i for 24 hours. Cells were imaged live and fixed, using a 20x widefield and 40x confocal microscopy, and using H2B and DAPI to visualize DNA. In these images, MN were present at a frequency between ~30–70% of cells due to induction of chromosome missegregation. These levels are elevated compared to some cancer cell lines and tumor samples. Therefore, we also analyzed publicly available images of unperturbed U2OS cells from Broad Bioimage dataset BBBC039v1, which have an MN frequency of 8% (Table 2). MNFinder showed significant improvement in recall over VCS MN with an additional improvement in PPV for some conditions (Fig. 2D). Importantly, recall and PPV were largely insensitive to image resolution, DNA label, and cell type. HeLa H2B-GFP 40x images were an outlier in terms of performance for unclear reasons, potentially due to increased nuclear shape diversity. We also calculated the per object mIoU to determine the quality of the segmentation and found that most MN were accurately segmented with mIoU values between 69–79% (Fig. 2D, Table 1). These metrics indicate that MNFinder provides accurate and robust MN segmentation across multiple cell lines and image acquisition settings. MNFinder identifies MN with similar sensitivity and substantially improved specificity compared to existing MN enumeration programs (Table 3) and is the only one to report a high mIoU, which is necessary for quantifying MN characteristics. This module is available as a Python package, MNFinder, via PyPI and on the Hatch Lab GitHub repository. VCS MN suitability for analysis of micronucleated RPE1 RFP703/Dendra cell transcriptomes To demonstrate the utility of our MN segmentation modules, we asked whether the VCS MN could be used for optical cell isolation to obtain cell populations substantially depleted and enriched for MN. Visual cell sorting (VCS) is a recently developed optical cell isolation pipeline that specifically labels and isolates multiple populations of adherent cells in a single experiment by combining on-demand image analysis with UV (405 nm) pulses of different durations targeted with single cell accuracy. It can generate up to four different proportions of converted Dendra2, which can be quantified and sorted by FACS (Fig. 3A). We first validated the utility of micronucleated RPE1 RFP703/Dendra cells for VCS analysis. We confirmed that we could specifically activate and sort two populations of RFP703/Dendra cells by classifying cells in a mixed pool based on CellTrace far red labeling (Fig. S3A–C). We next confirmed that Dendra2 red:green ratios were stable for the duration of a VCS MN isolation experiment by randomly converting RFP703/Dendra cells using a short, long, or no UV pulse and analyzing nuclear fluorescence intensity 0, 4, and 8 hours after activation. Quantification of nuclear red:green ratios from the same fields over time showed the persistence of three distinct fluorescent populations and a minimal loss of red fluorescence (Fig. 3B), indicating that photoconversion persisted over the time required to activate multiple 6-well cell populations. We also confirmed that VCS MN could accurately activate and isolate MN+ and MN− cells. RFP703/Dendra cells were incubated with Mps1i one day prior to imaging to generate MN and Cdk1i added prior to imaging to prevent mitosis, which dilutes the Dendra2(red) signal and frequently alters MN status. Cells were activated based on VCS MN analysis results and isolated by FACS. Isolated cells were replated in medium containing Cdk1i, fixed, and MN content quantified by manual fluorescent image analysis. Comparison of classifier PPV for MN+ and MN− cells during activation and MN+ and MN− cell frequency after FACs found a strong enrichment for the correct cell type in each group, with the increased purity of MN+ classified cells being retained during sorting (Fig. 3C). To determine how MN frequency affects MN cell isolation, we used the PPV and recall values from the VCS MN analysis of untreated U2OS cells (Fig. 1E: U2OS Broad, MN frequency = 8%) to estimate MN+ and – cell population purity. As expected, the purity of the MN− population increases and the purity of the MN+ population decreases compared to populations with a higher MN frequency (Fig. S3D). However, this represents a nearly 7-fold enrichment of MN+ cells in the isolated population and the MN+ cell purity is comparable to the enrichment of tumor cells in patient biopsies. Thus, VCS can be combined with the VCS MN neural net to generate cell populations enriched for MN− and MN+ cells from a variety of conditions that are highly suitable for discovery, including genetic screening, bulk RNA and proteomic analyses, and single cell sequencing. To validate this pipeline for transcriptome analysis, we used RNASeq to define gene expression changes in RPE1 cells after Mps1i incubation and VCS. Cells were incubated with DMSO or Mps1i and each population was randomly activated with short and long UV pulses (Fig. 4A). Conversion of 1,500–2,000 fields at 20x magnification allowed us to isolate 13k cells after FACS for each photoconverted population in each condition. Isolated populations were processed for RNASeq and principal component analysis (PCA) revealed that, as expected, cells clustered first by treatment group (Fig. 4B). Analysis of the DMSO samples found minimal differences in gene expression associated with UV pulse duration (Fig. S4A–B, Table 4), consistent with previous results. Therefore, data from cells activated at both pulse lengths were pooled in subsequent analyses. MA analysis identified 2,200 differentially expressed genes (DEGs) in Mps1i versus DMSO treated cells, 63 of which had absolute fold-changes > 1.5 (Fig. 4C, Table 5–6). We used GSEA analysis to compare our results to previously identified changes in RPE1 cells after mitotic disruption to induce aneuploidy (Table 7–8) and found substantial overlap between enriched Hallmark pathways in Mpsi1i cells isolated by VCS and previous studies (Table 9–11). These included increased expression of inflammation, EMT, and p53 associated genes (Fig. 4D). Additional changes observed in VCS-processed samples fell into similar function categories and are potentially due to differences in sequencing depth. These data confirm that the VCS MN pipeline can accurately and sensitively identifies biologically relevant transcriptional changes in aneuploid RPE1 cells. MN rupture induces few unique transcriptional changes and does not contribute to the initial aneuploidy response. To determine whether MN formation induces a transcriptional response, we treated RFP703/Dendra cells with Mps1i, activated MN+ and MN− cells based on VCS MN analysis results, and isolated differentially activated populations by FACS in duplicate (Fig. 5A). PCA revealed that results clustered largely by replicate and, consistent with this, only a few DEGs were identified with just two having absolute fold-changes greater than 1.5 (Fig. 5 B–C, Table 12). Both highly altered DEGs were also strongly upregulated by Mps1i treatment (Table 13). Although our analysis did identify some batch effects, our data strongly suggest that micronucleation does not induce a unique transcriptional response. We next compared gene expression between micronucleated cells classified as rupture+ and rupture-. Because the overall MN rupture frequency increases over time, we first synchronized cells in G1 using a Cdk4/6 inhibitor followed by release into Mps1i (Fig. 5D). This results in a more consistent rate of MN rupture (Fig. S5A). We modeled how the 4–5 hours required for analysis and activation of 1 well would alter population purity by manually quantifying the frequency of rupture+ cells in images taken 5 hours apart. Based on the increase in rupture+ cells we observed, we estimated only a small decrease in purity of the rupture− population (Fig. S5B), with a sustained high level of enrichment for both populations. PCA revealed that results clustered first by condition, indicating a transcriptional difference between rupture+ versus rupture− cells (Fig. 5E), and the MA plot identified 106 DEGs, 14 of which had absolute fold changes greater than 1.5 fold (Fig. 5F, Table 14). Of these, 3 were unique to cells with ruptured MN (Table 15). GSEA analysis confirmed that most of the pathways altered in MN+ or rupture+ cells overlapped with those identified in the total aneuploid Mps1i population (Fig. 5G, Table 16–17). We next asked whether micronucleation or MN rupture contributed to the transcriptional response to aneuploidy. We first quantified aneuploidy frequency in MN−, ruptur−, and rupture+ cells to determine whether transcription changes could reflect underlying differences in ploidy. Cells were labeled with probes against chromosomes 1, 11, or 18, all of which frequently missegregate into MN (Fig. S6A), and ruptured MN were identified by loss of H3K27Ac (Fig. 6A). Quantification of chromosome foci number found that aneuploidy frequency varied between chromosomes but was consistently higher for MN+ compared to MN− cells. This trend was also observed in rupture+ versus rupture− micronucleated cells (Fig. 6B). Similar results were obtained when transcription loss due to MN rupture was considered (functional aneuploidy) (Fig. S6B). We next compared the fold change of highly upregulated or downregulated genes in the Mps1i dataset to results from analysis of the subsetted populations of MN+ and rupture+ cells. All replicates were analyzed individually to reduce noise from batch effects in the MN+ results. This analysis identified one gene cluster that increased in expression in the subset of Mps1i cells with ruptured MN and included the genes FILIP1L, CREB5, TNFAIP3, ATF3, and EGR1 (Fig. 6C, Table 18). We attempted to validate increased protein expression of EGR1 and ATF3 in rupture+ cells by immunofluorescence. As a positive control, we quantified an increase EGR1 and ATF3 nuclear mean intensity after addition of hEGF and DNA damage by doxorubicin, respectively (Fig. S6C–D). Both genes were defined as upregulated by Mps1i and showed increased expression in Mps1i treated cells compared to controls by immunofluorescence (Fig. 6D–E). However, analysis of rupture+ versus other classes of Mps1i cells found no increase in EGR1 expression and only a small increase in ATF3 that was less than that observed between DMSO and Mps1i treated cells (Fig. 6D–E). Overall, our results strongly suggest that protein expression changes in MN+ and rupture+ cells are driven mainly by increased aneuploidy rather than cellular sensing of MN formation and rupture.","In this study, we present two machine-learning based modules to identify MN and micronucleated cells based on single channel fluorescence images and combine one with visual cell sorting to profile transcriptional responses to MN formation and rupture. We demonstrate that our MN segmentation pipeline, MNFinder, can robustly classify and segment MN from DNA labeled images across multiple cell types and fluorescent imaging conditions. Further, we demonstrate that a separate MN cell classifier, VCS MN, rapidly and robustly identifies micronucleated cells from low resolution images and can be combined with single-cell photoconversion to accurately isolate live cells with none, intact, or ruptured MN from a mixed population. Using this platform, we find that, unexpectedly, neither micronucleation nor rupture triggers gene expression changes beyond those associated with increased aneuploidy. Overall, our study brings a powerful high-throughput optical isolation strategy to MN biology and we anticipate that it will enable a wide range of new investigations. VCS MN isolation has several advantages over current methods to identify the mechanisms and consequences of MN formation and rupture. First, it can be used on any adherent cell line in the absence of genetic perturbations. This overcomes challenges involved with using lamin B2 overexpression to inhibit MN rupture, which is limited to specific cell lines and MN types and is complicated by additional changes in mitosis and gene expression. In addition, it overcomes cell line and MN content restrictions imposed by systems that induce missegregation of single chromosomes or chromosome arms by enabling analysis of all missegregation events in any genetic background. Unlike live single cell assays, it is highly scalable and eliminates selection pressures and restrictions added by clonal expansion. Importantly, VCS MN isolation captures whole live cells, overcoming limitations associated MN purification and permits time-resolved analyses of cellular changes and MN chromatin by population-level analyses. VCS has several advantages over similar optical isolation or in situ sequencing techniques as it can be adapted to any wide-field microscope by adding a digital micromirror to existing equipment, and can be performed on attached cells, which are critical to achieve the nuclear and cytoplasm spreading required for accurate MN identification. VCS MN isolation does have limitations. Due to Dendra2 signal decay and ongoing MN rupture, only about 200,000 cells can be analyzed and targeted per experiment. For optical pooled screening or analysis of rare cells, this limits the number of genes or depth of analysis that can be achieved. Cell fixation would overcome this issue and efforts to improve sample extraction in these conditions are ongoing. VCS MN isolation also requires introduction of at least one photoconvertible or activatable protein to mark the cells and a second fluorescent protein to discriminate ruptured MN. This limits the channels available for additional phenotype identification. However, recent advances in cell structure prediction may vastly expand the phenotypic information available from limited cell labels. VCS MN segmentation and MNFinder precision vary across cell types, and widely divergent nuclear morphologies from the training set could significantly impair performance. Additional training of the neural net should improve this metric, but different algorithm architectures will likely be required to identify MN in signal-rich environments like organoids or tissue samples. We observed upregulation of several pathways, including inflammation, endothelial-to-mesenchymal transition, and p53, in Mps1i-treated RPE1 cells that were previously identified as enriched in similar studies. These results demonstrate the suitability of our platform for detecting biologically relevant transcript changes in aneuploid cells. However, our analysis of micronucleated cells and cells with ruptured MN found only a handful of genes that were uniquely upregulated by MN rupture and no changes that indicated a contribution of either condition to the aneuploidy response. Thus, in line with previous results, our findings suggest that MN and MN rupture are not sensed by the cell outside of their contribution to aneuploidy through dysregulated transcription and limited replication of the sequestered chromatin. Of significant interest is whether similar results will be obtained in cells with more robust cGAS/STING signaling. There is a discrepancy about whether cGAS binding to ruptured MN is sufficient to initiate signaling, and how MN chromatin content may mediate this, that VCS MN isolation is ideal for resolving. VCS MN isolation is a highly flexible platform that enables powerful new approaches to address fundamental questions in MN biology. VCS MN isolation can be used for optical pooled screening, an unbiased method that would be ideal to identify mechanisms of MN rupture, genetic changes that promote proliferation of micronucleated cells, and, in combination with dCas9-based chromosome labeling, mechanisms that enrich specific chromosomes in MN and could drive cancer-specific aneuploidies. Recovering live cell populations of cells with intact and ruptured MN will also enable precise analysis of post-mitotic genetic and functional changes caused by these conditions. For instance, these cell populations can be analyzed for acquisition of disease-associated behaviors, including proliferation, migration, and used in in vivo tumorigenesis and metastasis assays to directly assess their contribution to cancer development. In summary, automated MN segmentation and VCS MN isolation are poised to provide critical insights into a wide-range of questions about how MN form, rupture, and cause disease pathologies.",10.1101/2023.05.04.539483
PMC10370131,37503080,Contextualizing protein representations using deep learning on protein networks and single-cell data,"Understanding protein function and developing molecular therapies require deciphering the cell types in which proteins act as well as the interactions between proteins. However, modeling protein interactions across diverse biological contexts, such as tissues and cell types, remains a significant challenge for existing algorithms. We introduce Pinnacle, a flexible geometric deep learning approach that is trained on contextualized protein interaction networks to generate context-aware protein representations. Leveraging a human multi-organ single-cell transcriptomic atlas, Pinnacle provides 394,760 protein representations split across 156 cell type contexts from 24 tissues and organs. Pinnacle’s contextualized representations of proteins reflect cellular and tissue organization and Pinnacle’s tissue representations enable zero-shot retrieval of the tissue hierarchy. Pretrained Pinnacle’s protein representations can be adapted for downstream tasks: to enhance 3D structure-based protein representations for important protein interactions in immuno-oncology (PD-1/PD-L1 and B7–1/CTLA-4) and to study the effects of drugs across cell type contexts. Pinnacle outperforms state-of-the-art, yet context-free, models in nominating therapeutic targets for rheumatoid arthritis and inflammatory bowel diseases, and can pinpoint cell type contexts that predict therapeutic targets better than context-free models (29 out of 156 cell types in rheumatoid arthritis; 13 out of 152 cell types in inflammatory bowel diseases). Pinnacle is a graph-based contextual AI model that dynamically adjusts its outputs based on biological contexts in which it operates.","Proteins can have distinct roles in different biological contexts. While nearly every cell contains the same genome, the expression of genes and the function of proteins encoded by these genes depend on cellular and tissue contexts. Gene expression and the function of proteins can also differ significantly between healthy and disease states. Therefore, computational methods that incorporate biological contexts can improve the characterization of proteins. However, existing deep learning methods produce protein representations (or embeddings) that are context-free: each protein has only one representation learned from either a single context or an integrated view across many contexts. These methods generate one representation for each protein, providing an integrated summary of the protein. Context-free protein representations are not tailored to specific biological contexts, such as cell types and disease states. Protein representations that lack context can be limited in their ability to predict protein functions that change across different cell types. This can also hamper the prediction of pleiotropy and protein roles in a cell type specific manner. The recent development of sequencing technologies to measure gene expression with single-cell resolution could pave the way toward addressing this challenge. Single-cell transcriptomic atlases measure activated genes across a large number of contexts at the resolution of individual cells. Through attention-based deep learning, which specify models that can pay attention to large inputs and learn the most important elements to focus on in each context, single-cell atlases can be leveraged to boost the mapping of gene regulatory networks that drive disease progression and reveal treatment targets. However, incorporating the expression of protein-coding genes into protein interaction networks remains a challenge. Existing algorithms, including protein representation learning, are unable to contextualize protein representations. We introduce Pinnacle (Protein Network-based Algorithm for Contextual Learning), a context-aware model for comprehensive protein understanding. Pinnacle is a self-supervised geometric deep learning model adept at generating protein representations through the analysis of protein interactions within various cellular contexts1. Leveraging single-cell transcriptomics combined with networks of protein-protein interactions, cell-type-to-cell-type interactions, and a tissue hierarchy, Pinnacle generates high-resolution protein representations tailored to each cell type. In contrast to existing methods that provide a single representation for each protein, Pinnacle generates a distinct representation for each cell type in which a protein-coding gene is activated. With 394,760 contextualized protein representations produced by Pinnacle, where each protein representation is distinctly imbued with cell type specificity, we demonstrate Pinnacle’s capability to integrate protein interactions with the underlying protein-coding gene transcriptomes of 156 cell type contexts. Pinnacle models support a broad array of tasks; they can enhance 3D structural protein representations, analyze the effects of drugs across cell type contexts, nominate therapeutic targets in a cell type specific manner, retrieve tissue hierarchy in a zero-shot manner, and perform transfer learning across cell types. Pinnacle models dynamically adjust their outputs based on the context in which they operate and can pave the way for the broad use of foundation models tailored to diverse biological contexts.","The Methods describe (1) the curation of datasets, (2) the construction and representation of multi-scale single-cell networks, (3) our multi-scale graph neural network, Pinnacle, (4) the finetuning of Pinnacle for target prioritization, and (5) the metrics and statistical analyses used. Datasets Reference human physical protein interaction network. Our global reference protein-protein interaction (PPI) network is the union of physical multi-validated interactions from BioGRID, the Human Reference Interactome (HuRI), and Menche et al. with 15,461 nodes and 207,641 edges. Different sources of PPI have their own methods of curating and validating physical interactions between proteins. BioGRID, HuRI, and Menche et al. are PPI networks from three well-cited publications and databases regarding human protein interactions. By joining the three networks, we construct a comprehensive global PPI network for our analysis. Multi-organ, single-cell transcriptomic atlas of humans. We leverage Tabula Sapiens data source as our multi-organ, single-cell transcriptomic atlas of humans. The data consists of 15 donors, with 59 specimens total. There are 483,152 cells after quality control, of which 264,824 are immune cells, 104,148 are epithelial cells, 31,691 are endothelial cells, and 82,478 are stromal cells. The cells correspond to 177 unique cell ontology classes. Construction of multi-scale networks Our multi-scale networks comprises protein-protein physical interactions, cell type to cell type communication, cell type to tissue relationships, and tissue-tissue hierarchy. Cell type-specific protein interaction networks. For each cell type, we create a cell type specific network that represents the physical interactions between proteins (or genes) that are likely expressed in the cell type. Intuitively, our approach identifies genes significantly expressed in a given cell type with respect to the rest of the cells in the dataset. Concretely, we use the processed Tabula Sapiens count matrix to calculate the average expression of each gene in a cell type of interest and the average expression of the corresponding gene in all other cells. Then, we use the Wilcoxon rank-sum test on the two sets of average gene expression. From the resulting ranked list of genes based on activation, we filter for the top  most activated genes. We repeat these two steps  times and filter for genes that appear in at least 90% of iterations. Finally, we extract these genes’ corresponding proteins from the global protein interaction network and take only the largest connected component. To ensure high-quality representations of cell types in our networks, we keep networks with at least 1,000 proteins. We do not perform subsampling of cells (i.e., sample the same number of cells per cell type) to minimize information loss for constructing protein interaction networks (Supplementary Figure S2). Limitations are described in the Discussion section. Cell type and tissue relationships in the metagraph. We identify interactions between cell types based on ligand-receptor (LR) expression using the CellPhoneDB tool and database. An edge between a pair of cell types indicates that CellphoneDB predicts at least one significantly expressed LR pair (with a p-value of less than 0.001) between them. As recommended by CellPhoneDB, cells are subsampled prior to running the algorithm, which uses geometric sketching to efficiently sample a small representative subset of cells from massive datasets while preserving biological complexity. We choose to subsample 25% of cells and run CellPhoneDB for 100 iterations.We determine cell type-tissue relationships and extract tissue-tissue relationships using Tabula Sapiens meta-data. For relationships between cell types and tissues, we draw edges between cell types and the tissue that the cells were taken from. For tissue-tissue relationships, we select the nodes corresponding to the tissues where samples were taken from and all parent nodes up to the root of the BRENDA tissue ontology. We perform sensitivity and ablation analyses on different components of the metagraph (Supplementary Table S3–S5). Final dataset. We have 156 cell type specific protein interaction networks, which have, on average, 2, 530±677 proteins per network. The number of unique proteins across all cell type specific protein interaction networks is 13, 643 of the 15, 461 proteins in the global reference protein interaction network. In the metagraph, we have 62 tissues (nodes), and 24 are directly connected to cell types. There are 3,567 cell-cell interactions, 372 cell-tissue edges, and 79 tissue-tissue edges. Multi-scale graph neural network Overview. Pinnacle performs biologically-informed message passing through proteins, cell types, and tissues to learn cell type specific protein representations, cell type representations, and tissue representations in a unified multi-scale embedding space. Specifically, Pinnacle traverses through protein-protein physical interactions in each cell type specific PPI network, cell type-cell type communication, cell type-tissue relationships, and tissue-tissue hierarchy with an attention mechanism over individual nodes and edge types. Its objective function is designed and optimized for learning the topology across biological scales, from proteins to cell types to tissues. The resulting embeddings from Pinnacle can be visualized and manipulated for hypothesis-driven interrogation and finetuned for diverse downstream biomedical prediction tasks. Problem Formulation. Let  be a set of cell type specific protein-protein interaction networks, where  is a set of unique cell types. Each  consists of a set of nodes  and edges  for a given cell type  specific protein-protein interaction network. Their nodes  are proteins, and edges  are physical protein-protein interactions (denoted with PP in the superscript). Cell types and tissues form a network, referred to as a metagraph. The metagraph’s set of nodes comprises cell types  and tissues . The types of edges are cell type-cell type interactions (denoted with CC in the superscript)  between any pair of cell types ; cell type-tissue associations (denoted with CT in the superscript)  between any pair of cell type  and tissue ; and tissue-tissue relationships (denoted with TT in the superscript)  between any pair of tissues . Protein-level attention with cell type specificity For each cell type specific protein-protein interaction network , we leverage protein-level attention to learn cell type specific embeddings of proteins. Intuitively, protein-level attention learns which neighboring nodes are likely most important for characterizing a particular cell type’s protein. As such, each cell type specific protein interaction network has its own cell type specific set of learnable parameters. Concretely, at each layer-wise update of layer , the node-level attention learns the importance  of protein  to its neighboring protein  in a given cell type :  where  is an aggregation function (i.e., concatenation across  attention heads),  is the nonlinear activation function (i.e., ReLU),  is the set of neighbors for  (including itself via self-attention),  is an attention mechanism defined as  between a pair of interacting proteins from a specific cell type,  is a PP-specific transformation matrix to project the features of protein  in its cell type specific protein interaction network, and  is the previous layer’s cell type specific embedding for protein . Practically, we leverage the attention function in graph attention neural networks (i.e., GATv2). Proteins of the same identity are initialized with the same random Gaussian vector to maintain their identity during training. Metagraph-level attention on cellular interactions and tissue hierarchy For the metagraph, we use node-level and edge-level attention to learn which neighboring nodes and edge types are likely most important for characterizing the target node (i.e., the node of interest). Intuitively, to learn an embedding for a specific cell type or tissue, we evaluate the informativeness of each direct cell type or tissue neighbor, as well as the relationship type between the cell type or tissue and their neighbors (e.g., parent-child tissue relationship, tissue from which a cell type is found, cell type with which the cell type of interest communicates with). Concretely, at each layer  of Pinnacle, the embeddings of a cell type  are the result of aggregating (via function AGG) the embeddings ( and ) of its direct cell type neighbor  and tissue neighbor  that are projected via edge-type-specific transformation matrices ( and ) and weighted by learned attention weights ( and  respectively):   The embeddings generated from separately propagating messages through cell type-cell type edges or cell type-tissue edges are combined using learned attention weights  and , respectively:  Similarly, the embeddings of a tissue  are the result of aggregating (via function AGG) the embeddings ( and ) of its direct tissue neighbor  and cell type neighbor  that are projected via edge-type-specific transformation matrices ( and ) and weighted by learned attention weights ( and  respectively):   The embeddings generated from separately propagating messages through tissue-tissue edges or tissue-cell type edges are combined using learned attention weights  and , respectively:  For the node-level attention mechanisms (Equations 2, 3, 5, and 6), AGG is an aggregation function (i.e., concatenation across  attention heads), is the nonlinear activation function (i.e., ReLU),  and  are the sets of neighbors for  and  respectively (includes itself via self-attention), , , , and  are edge-type-specific transformation matrices to project the features of a given target node, , , , and  are the previous layer’s embedding for  given the edge type  given the edge type  given the edge type , and  given the edge type , respectively. Practically, we leverage the attention function in graph attention neural networks (i.e., GATv2). Finally, the node-level attention mechanism for a given source node  and edge type  is . For the attention mechanisms over edge types (Equations 4 and 7),  such that  where  is the set of nodes in the metagraph,  is the attention vector,  is the weight matrix, and  is the bias vector. These parameters are shared for all edge types in the metagraph. Bridge between protein and cell type embeddings Using an up-pooling mechanism, we bridge cell type specific protein embeddings with their corresponding cell type embeddings. We initialize cell type embeddings by taking the average of their proteins’ embeddings:  where  is the embedding of protein node  in the PPI subnetwork for cell type . Similarly, we initialize tissue embeddings by taking the average of their neighbors: , where  and  are the embeddings of tissue node  and cell type node , respectively, in the immediate neighborhood of source tissue node . At each layer , we learn the importance  of node  to cell type  such that  After propagating cell type and tissue information in the metagraph (namely, Equations 2–6), we apply  to the cell type embedding of  such that  Intuitively, we are imposing the structure of the metagraph onto the PPI subnetworks based on a protein’s importance to its corresponding cell type’s identity. Pinnacle: Overall objective function Pinnacle is optimized for three biological scales: protein-, cell type-, and tissue-level. Concretely, the loss function  has three components corresponding to each biological scale:  where , , and  minimize the loss from protein-level predictions, cell type-level predictions, and tissue-level predictions, respectively.  is a tunable parameter with a range of 0 and 1 that scales the contribution of the link prediction loss of the metagraph relative to that of the protein-protein interactions. At the protein level, we consider two aspects: prediction of protein-protein interactions at each cell type specific PPI network  and prediction of cell type identity of each protein . The contribution of the latter is scaled by , which is a tunable parameter with a range of 0 and 1:  Intuitively, we aim to simultaneously learn the topology of each cell type specific PPI network (i.e., ) and the nuanced differences between proteins activated in different cell types. Specifically, we use binary cross-entropy to minimize the error of predicting positive and negative protein-protein interactions in each cell type specific PPI network:  and center loss for discriminating between protein embeddings  from different cell types, represented by embeddings denoted as :  At the cell type level, we use binary cross-entropy to minimize the error of predicting cell type-cell type interactions and cell type-tissue relationships:  such that   Similarly, at the tissue level, we use binary cross-entropy to minimize the error of predicting tissue-tissue and tissue-cell type relationships:  such that   The probability of an edge of type  between nodes  and  is calculated using a bilinear decoder:  where  and  are embeddings of nodes  and , and  is the embedding for edge type . Note that any decoder can be used for link prediction in Pinnacle. Training details for Pinnacle Overview. Pinnacle is trained in a self-supervised manner using cell type identity and graph connectivity (i.e., cell type specific protein interaction networks and metagraph) as a supervised signal to define positive vs. negative links in self-supervised link prediction. Specifically, Pinnacle predicts whether an edge (and its type) exists between a pair of nodes and the cell type(s) that the protein is activated in. For the link prediction tasks, a randomly selected subset of edges is masked from the model. Practically, this means that the graphs being fed as input into Pinnacle during train, validation, or test do not contain the masked edges. Protein-protein edges are randomly split into train, validation, and test sets. The metagraph edges are not split into train, validation, and test sets because there are relatively few of them, and they are critical for injecting cell type and tissue organization to the model. The proteins involved in the train edges are considered in the cell type identification term of the loss function. Hyperparameter tuning. We leverage Weights and Biases to select optimal hyperparameters via a random search over the hyperparameter space. The best-performing hyperparameters for Pinnacle are selected by optimizing the ROC and Calinski-Harabasz score on the validation set. The hyperparameter space on which we perform a random search to choose the optimal set of hyperparameters is: the dimension of the nodes’ feature matrix  [1024, 2048], dimension of the output layer , lambda  [0.1, 0.01, 0.001], learning rate for link prediction task  [0.01, 0.001], learning rate for protein’s cell type classification task  [0.1, 0.01, 0.001], number of attention heads , weight decay rate  [0.0001, 0.00001], dropout rate  [0.3, 0.4, 0.5, 0.6, 0.7], and normalization layer  [layernorm, batchnorm, graphnorm, none]. The best hyperparameters are as follows: the dimension of the nodes’ feature matrix = 1024, dimension of the output layer = 16, lambda = 0.1, learning rate for link prediction task = 0.01, learning rate for protein’s cell type classification task = 0.1, number of attention heads = 8, weight decay rate = 0.00001, dropout rate = 0.6, and normalization layers are layernorm and batchnorm. Further, Pinnacle consists of two custom graph attention neural network layers (as specified in Section 3) per cell type specific PPI network and metagraph, and is trained for 250 epochs. Implementation. We implement Pinnacle using Pytorch (Version 1.12.1) and Pytorch Geometric (Version 2.1.0). We leverage Weights and Biases for hyperparameter tuning and model training visualization, and we create interactive demos of the model using Gradio. Models are trained on a single NVIDIA Tesla V100-SXM2–16GB GPU. Generating contextualized 3D protein representations After pre-training Pinnacle, we can leverage the output protein representations for diverse downstream tasks. Here, we demonstrate Pinnacle’s ability to improve the prediction of protein-protein interactions by injecting context into 3D molecular structures of proteins. Overview. Given a protein of interest, we generate both the context-free structure-based representation via MaSIF and a contextualized PPI network-based representation via Pinnacle. We calculate the binding score of a pair of proteins based on either context-free representations or contextualized representations of the proteins. To quantify the added value, if any, provided by contextualizing protein representations with cell type context, we compare the size of the gap between the average binding scores of binding and non-binding proteins in the two approaches. Dataset. The proteins being compared are PD-1, PD-L1, B7–1, CTLA-4, RalB, RalBP1, EPO, EPOR, C3, and CFH. The pairs of binding proteins are PD-1/PD-L1 (PDB ID: 4ZQK) and B7–1/CTLA-4 (PDB ID: 1I8L). The non-binding proteins are any of the four proteins paired with any of the remaining six proteins (e.g., PD-1/RalB, PD-1/RalBP1, PD-L1/RalBP1). The PDB IDs for the other six proteins are 2KWI for RalB/RalBP1, 1CN4 for EPO/EPOR, and 3OXU for C3/CFH. Structure-based protein representation learning model. We directly apply the pretrained model for MaSIF to generate the 3D structure-based protein representations. We use the model pretrained for MaSIF-site task, named all_feat_3l_seed_benchmark. The output of the pretrained model for a given protein is , where  is the number of patches (precomputed by the authors of MaSIF) and  is the dimension of the pretrained model’s output layer. As proteins vary in size (i.e., the number of patches to cover the surface of the protein), we select a fixed  number of patches that are most likely to be part of the binding site (according to the pretrained MaSIF model). For each protein, we select  patches, which is the average number of patches for PD-1, PD-L1, B7–1, and CTLA-4, resulting in a matrix of size 200 × 4. Finally, we take the element-wise median on the 200 × 4 matrix to transform it into a vector of length 200. This vector becomes the structure-based protein representation for a given protein. Experimental setup. For each cell type context of a given protein, we concatenate the 3D structure-based protein representation (from MaSIF) with the cell type specific protein representation (from Pinnacle) to generate a contextualized structure-based protein representation. To create the context-free protein representation, we concatenate the structure-based protein representation with an element-wise average of Pinnacle’s protein representations. This is to maintain consistent dimensionality and latent space between context-free and contextualized protein representations. Given a pair of proteins, we calculate a score via cosine similarity (a function provided by sklearn) using the context-free or contextualized protein representations. Lastly, we quantify the gap between the scores of binding and non-binding proteins using context-free or contextualized protein representations to evaluate the added value (if any) of contextual AI. Fine-tuning Pinnacle for target prioritization After pre-training Pinnacle, we can fine-tune the output protein representations for diverse biomedical downstream tasks. Here, we demonstrate Pinnacle’s ability to enhance the performance of predicting a protein’s therapeutic potential for a specific therapeutic area. Overview. For each protein of interest, we feed its Pinnacle-generated embedding into a multilayer perceptron (MLP). The model outputs a score between 0 and 1, where 1 indicates strong candidacy to target (i.e., by a compound/drug) for treating the therapeutic area and 0 otherwise. Since a protein has multiple representations corresponding to the cell types it is activated in, the MLP model generates a score for each of the protein’s cell type-specific representations (Figure 4a). For example, Protein 1’s representation from Cell type 1 is scored independently of its representation from Cell type 2. The output scores can be examined to identify the most predictive cell types and the strongest candidates for therapeutic targets in any specific cell type. Therapeutic targets dataset We obtain labels for therapeutic targets from the Open Targets Platform. Therapeutic area selection. To curate target information for a therapeutic area, we examine the drugs indicated for the therapeutic area of interest and its descendants. The two therapeutic areas examined are rheumatoid arthritis (RA) and inflammatory bowel disease. For rheumatoid arthritis, we collected therapeutic data (i.e., targets of drugs indicated for the therapeutic area) from OpenTargets for rheumatoid arthritis (EFO 0000685), ankylosing spondylitis (EFO 0003898), and psoriatic arthritis (EFO 0003778). For inflammatory bowel disease, we collected therapeutic data for ulcerative colitis (EFO 0000729), collagenous colitis (EFO 1001293), colitis (EFO 0003872), proctitis (EFO 0005628), Crohn’s colitis (EFO 0005622), lymphocytic colitis (EFO 1001294), Crohn’s disease (EFO 0000384), microscopic colitis (EFO 1001295), inflammatory bowel disease (EFO 0003767), appendicitis (EFO 0007149), ulcerative proctosigmoiditis (EFO 1001223), and small bowel Crohn’s disease (EFO 0005629). Positive training examples. We define positive examples (i.e., where the label ) as proteins targeted by drugs that have at least completed phase 2 of clinical trials for treating a certain therapeutic area. As such, a protein is a promising candidate if a compound that targets the protein is safe for humans and effective for treating the disease. We retain positive training examples that are activated in at least one cell type specific protein interaction network. The final number of positive training examples for RA and IBD are 152 and 114, respectively. Negative training examples. We define negative examples (i.e., where the label ) as druggable proteins that do not have any known association with the therapeutic area of interest according to OpenTargets. A protein is deemed druggable if it is targeted by at least one existing drug. We extract drugs and their nominal targets from DrugBank. We retain negative training examples that are activated in at least one cell type specific protein interaction network. The final number of negative training examples for RA and IBD are 1,465 and 1,377, respectively. Data processing workflow. For a therapeutic area of interest, we identify its descendants. With the list of disease terms for the therapeutic area, we curate its positive and negative training examples. We split the dataset such that about 60%, 20%, and 20% of the proteins are in the train, validation, and test sets, respectively. We additionally apply two criteria to avoid data leakage and ensure that all cell types are represented during training/inference: Proteins are assigned to train (60%), validation (20%), and test (20%) datasets based on their identity; this is to prevent data leakage where cell type specific representations of a single protein are observed in multiple data splits. We also ensure that there are sufficient numbers of train, validation, and test positive samples per cell type; proteins may be reassigned to a different data split so that each cell type is represented during training, validating, and testing stages. With these criteria, the train, validation, and test dataset splits may not necessarily consist of approximately 60%, 20%, and 20% of the total protein representations (Supplementary Table S6). Finetuning model details Model architecture. Our multi-layer perceptron (MLP) comprises an input feedforward neural network, one hidden feedforward neural network layer, and an output feedforward neural network layer. In between each layer, we have a non-linear activation layer. In addition, we use dropout and normalization layers between the input and hidden layer (see the Implementation section for more information). Our objective function is binary cross-entropy loss. Hyperparameter tuning. We leverage Weights and Biases to select optimal hyperparameters via a random search over the hyperparameter space. The best-performing hyperparameters are selected by optimizing the AUPRC on the validation set. The hyperparameter space on which we perform a random search to choose the optimal set of hyperparameters is the dimension of the first hidden layer , dimension of the second hidden layer , learning rate  [0.01, 0.001, 0.0001], weight decay rate  [0.001, 0.0001, 0.00001, 0.000001], dropout rate  [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], normalization layer  [layernorm,batchnorm,none], and the ordering of dropout and normalization layer (i.e., normalization before dropout, or vice versa). Implementation. We implement the MLP using Pytorch (Version 1.12.1). In addition, we use Weights and Biases for hyperparameter tuning and model training visualization. Models are trained on a single NVIDIA Tesla M40 GPU. Metrics and statistical analyses Here, we describe metrics, visualization methods, and statistical tests used in our analysis. Visualization of embeddings We visualize Pinnacle’s embeddings using a uniform manifold approximation and projection for dimension reduction (UMAP) and seaborn. Using the python package, umap, we transform Pinnacle’s embeddings to two-dimensional vectors via the parameters: n_neighbors = 10, mindist = 0.9, ncomponents = 2, and the euclidean distance metric. The plots are created using the seaborn package’s scatterplot function. Visualization of cell type embedding similarity The pairwise similarity of Pinnacle’s cell type embeddings is calculated using cosine similarity (a function provided by sklearn). Then, these similarity scores are visualized using the seaborn package’s clustermap function. For visualization purposes, similarity scores are mapped to colors after being raised to the 20th power. Spatial enrichment analysis of Pinnacle’s protein embeddings To quantify the spatial enrichment for Pinnacle’s protein embedding regions, we apply a systematic approach, SAFE, that identifies regions that are over-represented for a feature of interest (Supplementary Figures S4–S5). The required input data for SAFE are networks and annotations of each node. We first construct an unweighted similarity network on Pinnacle protein embeddings: (1) calculate pairwise cosine similarity, (2) apply a similarity threshold on the similarity matrix to generate an adjacency matrix, and (3) extract the largest connected component. The protein nodes are labeled as 1 if they belong to a given cell type context and 0 otherwise. We then apply SAFE to each network using the recommended settings: neighborhoods are defined using the shortpath_weighted_layout metric for node distance and neighborhood radius of 0.15, and p-values are computed using the hypergeometric test, adjusted using the Benjamin-Hochberg false discovery rate correction (significance cutoff ). Due to computation and memory constraints, we sample 50 protein embeddings from a cell type context of interest and 10 protein embeddings from each of the other 155 cell type contexts. We use a threshold of 0.3 in our evaluation of Pinnacle’s protein embedding regions (Figure 2; Supplementary Figures S4). We also evaluate the spatial enrichment analysis on networks constructed from different thresholds to ensure that the enrichment is not sensitive to our network construction method: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] (Supplementary Figures S5). We use the Python implementation of SAFE: https://github.com/baryshnikova-lab/safepy. Analysis of representations for proteins with similar function Data. We extract human housekeeping genes from the Housekeeping and Reference Transcript Atlas (https://housekeeping.unicamp.br/) and marker genes from the human gold standard T lymphocyte-specific protein functional networks from HumanBase (https://hb.flatironinstitute.org/) (accessed on November 20th, 2023). Only edges of level C1 (i.e., tissue-specific) are kept. The nodes corresponding to these edges are considered to be marker genes for cell types in the family of T lymphocytes. The lists of marker and housekeeping genes do not overlap, as we remove any overlapping housekeeping genes from the list of marker genes. Analysis. We compare embedding similarities of a marker (orange) or housekeeping (gray) gene’s contextualized protein representation (from Pinnacle) across different cell type contexts. For each marker/housekeeping gene, its cell type-specific protein representations are compared in similar contexts (i.e., between different T lymphocyte cell types; a total of 10 T lymphocyte cell types) or different contexts (i.e., between a T lymphocyte cell type and a non-immune cell type; a total of 115 non-immune cell types). We perform the two-sample Kolmogorov-Smirnov test (via ks_2samp from scipy). Statistical significance of tissue embedding distance Tissue embedding distance between a given pair of tissue nodes is calculated using cosine distance (a function provided by sklearn). Tissue ontology distance between a given pair of tissue nodes is calculated by taking the sum of the nodes’ shortest path lengths to the lowest common ancestor (functions provided by networkx. We use the two-sample Kolmogorov-Smirnov test (a function provided by scipy) to compare Pinnacle embedding distances against randomly generated vectors (via the randn function in numpy to sample an equal number of vectors from a standard normal distribution). We also use the Spearman correlation (a function provided by scipy) to correlate Pinnacle embedding distance to tissue ontology distance. We additionally generate a null distribution of tissue ontology distance by calculating tissue ontology distance on a shuffled tissue hierarchy (repeated 10 times). Concretely, we shuffle the node identities of the Brenda Tissue Ontology and compute the pairwise tissue ontology distances. Statistical significance of binding and non-binding proteins’ score gaps We perform a one-sided non-parametric permutation test. First, we concatenate the scores for the  binding pairs and  non-binding pairs. Next, for 100,000 iterations, we randomly sample  scores as the new set of binding protein scores and  scores as the new set of non-binding protein scores, calculate the mean  of the  binding protein scores and the mean  of the  non-binding protein scores, calculate the score gap by taking the difference of the means as , and keep track of the score gaps that are greater than or equal to the true score gap calculated from the real data. Lastly, we calculate the p-value, defined as the fraction of 100,000 iterations in which the permuted score gap is greater than or equal to the true score gap (i.e., one-sided non-parametric permutation test). Performance metric for therapeutic target prioritization For our downstream therapeutic target prioritization task (Methods 5), we use a metric called (Average Precision and Recall)@K () to evaluate model performance.  leverages a combination of  and Recall@K to measure the ability to rank the most relevant items (in our case, proteins) among the top K predictions. In essence,  calculates Precision@K for each , multiplying each  by whether the kth item is relevant, and divides by the total number of relevant items  at :  where  Given the nature of our target prioritization task, some key advantages of using  include robustness to (1) varied numbers of protein targets activated across cell type-specific protein interaction networks and (2) varied sizes of cell type specific protein interaction networks.","Constructing context-aware protein, cell type, and tissue networks. Generating protein representations embedded with cell type context calls for protein interaction networks that consider the same context. We assembled a dataset of context-sensitive protein interactomes, beginning with a multi-organ single-cell transcriptomic atlas that encompasses 24 tissue and organ samples sourced from 15 human donors (Figure 1a). We compile activated genes for every expert-annotated cell type in this dataset by evaluating the average gene expression in cells from that cell type relative to a designated reference set of cells (Figure 1a; Methods 2). Here, ‘activated genes’ are defined as those demonstrating a higher average expression in cells annotated as a particular type than the remaining cells documented in the dataset. Based on these activated gene lists, we extracted the corresponding proteins from the comprehensive reference protein interaction network and retained the largest connected component (Figure 1a). As a result, we have 156 context-aware protein interaction networks, each with 2, 530 ± 677 proteins, that are maximally similar to the global reference protein interaction network and still highly cell type specific (Supplementary Figures S1–S2). Our context-aware protein interaction networks from 156 cell type contexts span 62 tissues of varying biological scales. Further, we constructed a network of cell types and tissues (metagraph) to model cellular interactions and the tissue hierarchy (Methods 2). Given the cell type annotations designated by the multi-organ transcriptomic atlas, the network consists of 156 cell type nodes. We incorporated edges between pairs of cell types based on the existence of significant ligand-receptor interactions and validated that the proteins correlating to these interactions are enriched in the context-aware protein interaction networks in comparison to a null distribution (Methods 2; Supplementary Figure S3). Leveraging information on tissues in which the cell types were measured, we began with a set of 24 tissue nodes and established edges between cell type nodes and tissue nodes if the cell type was derived from the corresponding tissue. We then identified all ancestor nodes, including the root, of the 24 tissue nodes within the tissue hierarchy to feature 62 tissue nodes interconnected by parent-child relationships. Our dataset thus comprises 156 context-aware protein interaction networks and a metagraph reflecting cell type and tissue organization. Overview of Pinnacle model. Pinnacle is a self-supervised geometric deep learning model capable of generating protein representations predicated on protein interactions within a spectrum of cell type contexts. Trained on an integrated set of context-aware protein interaction networks, complemented by a network capturing cellular interactions and tissue hierarchy (Figure 1b–c), Pinnacle generates contextualized protein representations that are tailored to cell types in which protein-coding genes are activated (Figure 1d). Unlike context-free models, Pinnacle produces multiple representations for every protein, each contingent on its specific cell type context. Additionally, Pinnacle produces representations of the cell type contexts and representations of the tissue hierarchy (Figure 1d–e). This approach ensures a multifaceted understanding of protein interaction networks, taking into account the myriad of contexts in which proteins function. Given multi-scale model inputs, Pinnacle learns the topology of proteins, cell types, and tissues by optimizing a unified latent representation space. Pinnacle integrates different context-specific data into one context-aware model (Figure 1f) and transfers knowledge between protein-, cell type- and tissue-level data to contextualize representations (Figure 1g). To infuse cellular and tissue organization into this embedding space, Pinnacle employs protein-, cell type-, and tissue-level attention along with respective objective functions (Figure 1b–c; Methods 3). Conceptually, pairs of proteins that physically interact (i.e., are connected by edges in input networks) are closely embedded. Similarly, proteins are embedded near their respective cell type contexts while maintaining a substantial distance from unrelated ones. This ensures that interacting proteins within the same cell type context are situated proximally within the embedding space, yet are separated from proteins belonging to other cell type contexts. This approach yields an embedding space that accurately represents the intricacies of relationships between proteins, cell types, and tissues. Pinnacle disseminates graph neural network messages between proteins, cell types, and tissues using a series of attention mechanisms tailored to each specific node and edge type (Methods 3). The protein-level pretraining tasks consider self-supervised link prediction on protein interactions and cell type masking on protein nodes. These tasks enable Pinnacle to sculpt an embedding space that encapsulates both the topology of the context-aware protein interaction networks and the cell type identity of the proteins. Pinnacle’s cell type- and tissue-specific pretraining tasks rely exclusively on self-supervised link prediction, facilitating the learning of cellular and tissue organization. The topology of cell types and tissues is imparted to the protein representations through an attention bridge mechanism, effectively enforcing tissue and cellular organization onto the protein representations. Pinnacle’s contextualized protein representations capture the structure of context-aware protein interaction networks. The regional arrangement of these contextualized protein representations in the latent space reflects the cellular and tissue organization represented by the metagraph. This leads to a comprehensive and context-specific representation of proteins within a unified cell type and tissue-specific framework. Pinnacle’s representations capture cellular and tissue organization. Pinnacle generates protein representations for each of the 156 cell type contexts spanning 62 tissues of varying hierarchical scales. In total, Pinnacle’s unified multi-scale embedding space comprises 394,760 protein representations, 156 cell type representations, and 62 tissue representations (Figure 1a). We show that Pinnacle learns an embedding space where proteins are positioned based on cell type context. We first quantify the spatial enrichment of Pinnacle’s protein embedding regions using a systematic method, SAFE (Methods 6.3). Pinnacle’s contextualized protein representations self-organize in Pinnacle’s embedding space as evidenced by the enrichment of spatial embedding regions for protein representations that originate from the same cell type context (significance cutoff ; Figure 2; Supplementary Figures S4–S5). Next, we evaluate embedding regions to confirm that they are separated by cell type and tissue identity by calculating the similarities between protein representations across cell type contexts. Protein representations from the same cell type are more similar than protein representations from different cell types (Figure 3a). In contrast, a model without cellular or tissue context fails to capture any differences between protein representations across cell type contexts (Figure 3b). Further, we expect the representations of proteins that act on multiple cell types to be highly dissimilar, reflecting specialized cell type-specific protein functions. We calculate the similarities of protein representations (i.e., cosine similarities of a protein’s representations across cell type contexts) based on the number of cell types in which the protein is active (Supplementary Figure S6). Representational similarities of proteins negatively correlate with the number of cell types in which they act (Spearman’s ; p-value < 0.001), and the correlation is weaker in the ablated model with cellular and tissue metagraph turned off (Spearman’s ; p-value < 0.001). Although Pinnacle learns protein representations in a self-supervised manner using context-aware protein, cell type, and tissue networks alone, it can effectively capture shared protein functions. We analyze the embedding similarities of contextualized protein representations for both marker and housekeeping genes, as provided by Pinnacle, across different cell type contexts. For each T lymphocyte marker or housekeeping gene, we compare its cell type-specific protein representations in similar contexts (i.e, between different T lymphocyte cell types) and in different contexts (i.e., between a T lymphocyte cell type and a non-immune cell type) (Methods 6.4). Our findings reveal that housekeeping genes exhibit higher embedding similarity in similar contexts compared to marker genes (Supplementary Figure S7; p-value = 3.2 × 10−14). This result aligns with the expectation that housekeeping genes maintain shared functions across these cell types. Housekeeping genes in different contexts also show higher embedding similarity compared to marker genes (Supplementary Figure S7; p-value = 1.0 × 10−91), a reflection of their consistent functions across non-immune cell types. Conversely, marker genes in similar contexts display higher embedding similarity than those in different contexts (Supplementary Figure S7; p-value = 3.1 × 10−26), consistent with their specificity to T lymphocyte cell types. Their protein representations are more similar within T lymphocyte contexts compared to when these marker genes are in the context of non-immune cell types. These analyses imply that the protein embedding regions in Pinnacle are organized in accordance with cellular contexts, potentially capturing subtle nuances not explicitly delineated in the data or the model itself. This includes the possibility of cell type-dependent roles of proteins, a complexity that adds depth to our understanding of protein functions in varying biological contexts. In addition to the meaningful separation of Pinnacle’s protein embedding regions, we examine whether the regions are organized by the tissue hierarchy. We leverage Pinnacle’s tissue representations to perform zero-shot retrieval of the tissue hierarchy and then compare tissue ontology distance to tissue embedding distance. Tissue ontology distance is defined as the sum of the shortest path lengths from two tissue nodes to the lowest common ancestor node in the tissue hierarchy, and tissue embedding distance is the cosine distance between the corresponding tissue representations. We expect a positive correlation: the farther apart the nodes are according to the tissue hierarchy, the more dissimilar the tissue representations are. As hypothesized, embedding distances in the latent space and the corresponding distances in the tissue ontology of the same tissues are positively correlated (Spearman’s ; p-value = 1.85 × 10−119; Figure 3c), and the distribution of tissue embedding distances cannot be attributed to random effects (Kolmogorov-Smirnov two-sided test = 0.50; p-value < 0.001). When the tissue ontology is randomly shuffled, the correlation with distances in the embedding space diminishes significantly (Spearman’s ; p-value = 0.349; Figure 3c). Since Pinnacle uses the metagraph to systematically integrate tissue organization into both cell type and protein representations, it follows that all of Pinnacle’s representations inherently reflect this tissue organization (Methods 3; Supplementary Figures S8–S9). Pinnacle enhances 3D structural representations of protein interactions. Protein-protein interactions (PPI) depend on both 3D structure conformations of the proteins and cell type contexts within which protein act. Considering alternative conformations of proteins has yielded improvements in binding predictions. However, protein representations produced by existing AI models on the basis of 3D molecular structures lack cell type context information. We hypothesize that incorporating cellular context information can better differentiate binding from non-binding proteins (Figure 3d). As no large-scale dataset with matched structural biology and genomic readouts currently exists to perform systematic analyses, we focus on PD-1/PD-L1 interacting proteins and B7–1/CTLA-4 interacting proteins, which are important interactions involving immune checkpoint proteins that are critical in cancer immunotherapies. We examine these protein interactions to test whether Pinnacle’s contextualized representations can enhance and complement 3D structure-based representations of proteins derived from structural biology. Because 3D structures of molecules, which contain precise atom or residue level contact information, provides complementary knowledge to protein-protein interaction networks, which summarize binary interactions between proteins, we expect that context-aware protein interaction networks can further improve the ability to differentiate between binding and non-binding proteins across different cell types. We compare contextualized and context-free protein representations for binding proteins (i.e., PD-1/PD-L1 and B7–1/CTLA-4) and non-binding proteins (i.e., one of the four binding proteins paired with RalB, RalBP1, EPO, EPOR, C3, or CFH). Cell type context is incorporated into 3D structure-based protein representations by concatenating them with Pinnacle’s protein representation (Figure 3e; Methods 4). Context-free protein representations are generated by concatenating 3D structure-based representations with an average of Pinnacle’s protein representations across all cell type contexts (Methods 4). Contextualized representations, resulting from a combination of protein representations based on 3D structure and context-aware PPI networks, give scores (via cosine similarity) for binding and non-binding proteins of 0.9690±0.0049 and 0.9571±0.0127, respectively. With Pinnacle’s context-aware protein representations, which have no knowledge of 3D structure, binding and non-binding proteins are scored 0.0385 ± 0.1531 and 0.0218 ± 0.1081, respectively. In contrast, using context-free representations, binding and non-binding proteins are scored at 0.9789 ± 0.0004 and 0.9742 ± 0.0078, respectively. Further, comparative analysis of the gap in scores between interacting vs. non-interacting proteins yields gaps of 0.011 (PD-1/PD-L1) and 0.015 (B7–1/CTLA-4) for Pinnacle’s contextualized representations (p-value = 0.0299; Supplementary Figure S10), yet only 0.003 (PD-1/PD-L1) and 0.006 (B7–1/CTLA-4) for context-free representations (Figure 3f and Supplementary Figure S10). Incorporating information about biological contexts can help better distinguish protein interactions (namely, PD-1/PD-L1 and B7–1/CTLA-4) from non-interacting proteins in specific cell types, suggesting that Pinnacle’s contextualized representations can enhance protein representations derived from 3D protein structure modality. Modeling context-dependent interactions involving immune checkpoint proteins has the potential to deepen our understanding of how these proteins are used in cancer immunotherapies. Moreover, we benchmark our contextualized protein representations (structure-free) and contextualized structure-based protein representations against two null distributions and four context-free approaches. We show that randomly sampling pairs of proteins from different cell type contexts, padded (no 3D structure; score gap 0.0431) or concatenated with the structure-based protein representations (score gap 0.0356), cannot produce the score gap observed in the contextualized protein representations (Pinnacle without 3D structure) nor contextualized structure-based protein representations (Pinnacle with 3D structure) (Supplementary Figure S10). Similarly, we find that context-free protein representations are unable to predict intercellular communication (i.e., protein interactions between different cell types). Such is demonstrated using context-free protein representations generated by a graph attention neural network on the global reference protein interaction network (i.e., GAT), padded (no 3D structure; score gap −0.1319) and concatenated with the structure-based protein representations (score gap −0.0486), and context-free protein representations generated by BIONIC, a graph convolutional neural network designed for multi-modal network integration, padded (score gap 0.0046) and concatenated with the structure-based protein representations (score gap 0.0043). Our benchmarking results suggest that incorporating context can improve 3D structure prediction of protein interactions. Model benchmarking shows that Pinnacle outperforms context-free models in predicting therapeutic targets. With the representations from Pinnacle that are infused with cellular and tissue context, we can fine-tune them for downstream tasks (Figure 1f–h). We hypothesize that Pinnacle’s contextualized latent space can better differentiate between therapeutic targets and proteins with no therapeutic potential than a context-free latent space. Here, we focus on modeling the therapeutic potential of proteins across cell types for therapeutic areas with cell type-specific mechanisms of action (Figure 4). Certain cell types are known to play crucial and distinct roles in the disease pathogenesis of rheumatoid arthritis (RA) and inflammatory bowel disease (IBD) therapeutic areas. There is currently no cure for either types of conditions, and the medications prescribed to mitigate the symptoms can lead to undesired side effects. The new generation of therapeutics in development for RA and IBD conditions is designed to target specific cell types so that the drugs maximize efficacy and minimize adverse events (e.g., by directly impacting the affected/responsible cells and avoiding off-target effects on other cells). We adapt Pinnacle models to predict the therapeutic potential of proteins in a cell type specific manner. We independently fine-tune Pinnacle to predict therapeutic targets for RA and IBD diseases. Specifically, we perform binary classification on each contextualized protein representation, where  indicates that the protein is a therapeutic candidate for the given therapeutic area and  otherwise. The ground truth positive examples (where ) are proteins targeted by drugs that have at least completed one clinical trial of phase two or higher for indications under the therapeutic area of interest, indicating that the drugs are safe and potentially efficacious in an initial cohort of humans (Figure 4a–b). The negative examples (where ) are druggable proteins that have not been studied for the therapeutic area (Figure 4b; Methods 5). The binary classification model can be of any architecture; our results for nominating RA and IBD therapeutic targets are generated by independently training a multi-layer perceptron for each therapeutic area (Figure 4c). To evaluate Pinnacle’s contextualized protein representations, we compare Pinnacle’s fine-tuned models against three context-free models. We independently apply a random walk algorithm and a standard graph attention network (GAT) on the context-free reference protein interaction network. The BIONIC model is a graph convolutional neural network designed for (context-free) multi-modal network integration. We find that Pinnacle’s protein representations for all cell type contexts outperform the random walk model for both RA (Figure 4d) and IBD (Figure 4e) diseases. Protein representations from 44.9% (70 out of 156) and 37.5% (57 out of 152) cell types outperform the GAT model for RA (Figure 4d) and IBD (Figure 4e) diseases, respectively. Although both Pinnacle and BIONIC can integrate the 156 cell type-specific protein interaction networks, Pinnacle’s protein representations outperform BIONIC in 18.6% of cell types (29 out of 156) and 8.6% of cell types (13 out of 152) for RA (Figure 4d) and IBD diseases (Figure 4e), respectively, highlighting the utility of contextualizing protein representations. Pinnacle outperforms these three context-free models via other metrics for both RA and IBD therapeutic areas (Supplementary Figure S11). We have confirmed that there is no significant correlation between the node degree of proteins in cell type-specific PPI networks and performance in RA and IBD models (Supplementary Figure S12). Additionally, there is only a moderate correlation between Pinnacle’s performance and the enrichment of positive targets in these cell type-specific PPI networks (Supplementary Figure S13). These findings underscore that Pinnacle’s predictions cannot be solely ascribed to the characteristics of the cell type-specific PPI networks. Benchmarking results indicate that combining global reference networks with advanced deep graph representation learning techniques, such as GAT, can yield better predictors than using network-based random walk methods alone. Integrative approaches, exemplified by methods like BIONIC, enhance performance, a finding consistent with the established benefits of data integration. Contextualized learning approaches, such as Pinnacle–of which there are relatively few–can elevate model performance and make context-specific predictions. Pinnacle can nominate therapeutic targets across cell type contexts. There is existing evidence that treatment effects vary on the basis of the cell type in which therapeutic targets are expressed. For instance, CD19-targeting chimeric antigen receptor T (CAR-T) cell therapy has been highly effective in treating B cell malignancies, yet causes a high incidence of neurotoxicity. A recent study shows that CAR-T cell is inducing off-target effects by targeting the CD19 expressed in brain mural cells, which is likely causing the brain barrier leakiness responsible for neurotoxicity. We hypothesize that the predicted protein druggability varies across cell types, and such variations can provide insights into the cell types’ relevance for a therapeutic area. Among the 156 biological contexts modeled by Pinnacle’s protein representations, we examine the most predictive cell type contexts for nominating therapeutic targets of RA. We find that the most predictive contexts consist of CD4+ helper T cells, CD4+  memory T cells, CD1c+ myeloid dendritic cells, gut endothelial cells, and pancreatic acinar cells (Figure 5a). Immune cells play a significant role in the disease pathogenesis of RA. Since CD4+ helper T cells (Pinnacle-predicted rank = 1), CD4+  memory T cells (Pinnacle-predicted rank = 2), and CD1c+ myeloid dendritic cells (Pinnacle-predicted rank = 3) are immune cells, it is expected that Pinnacle’s protein representations in these contexts achieve high performance in our prediction task. Also, patients with RA often have gastrointestinal (GI) manifestations, whether concomitant GI autoimmune diseases or GI side effects of RA treatment. Pancreatic acinar cells (Pinnacle-predicted rank = 5) can behave like inflammatory cells during acute pancreatitis, one of the concomitant GI manifestations of RA. In addition to GI manifestations, endothelial dysfunction is commonly detected in patients with RA. While rare, rheumatoid vasculitis, which affects endothelial cells and a serious complication of RA, has been found to manifest in the large and small intestines (gut endothelial cell context has Pinnacle-predicted rank = 4), liver, and gallbladder. Further, many of the implicated cell types for RA patients (e.g., T cells, B cells, natural killer cells, monocytes, myeloid cells, and dendritic cells) are highly ranked by Pinnacle (Supplementary Table S1). Our results suggest that injecting cell type context to protein representations can significantly improve performance in nominating therapeutic targets for RA diseases while potentially revealing the cell types underlying disease processes. The most predictive cell type contexts for nominating therapeutic targets of IBD are CD4+  memory T cells, enterocytes of epithelium of large intestine, T follicular helper cells, plasmablasts, and myeloid dendritic cells (Figure 5d). The intestinal barrier is composed of a thick mucus layer with antimicrobial products, a layer of intestinal epithelial cells, and a layer of mesenchymal cells, dendritic cells, lymphocytes, and macrophages. As such, these five cell types expectedly yield high predictive ability. Moreover, many of the implicated cell types for IBD (e.g., T cells, fibroblasts, goblet cells, enterocytes, monocytes, natural killer cells, B cells, and glial cells) are high ranked by Pinnacle (Supplementary Table S2). For example, CD4+ T cells are known to be the main drivers of IBD. They have been found in the peripheral blood and intestinal mucosa of adult and pediatric IBD patients. Patients with IBD tend to develop uncontrolled inflammatory CD4+ T cell responses, resulting in tissue damage and chronic intestinal inflammation. Due to the heterogeneity of CD4+ T cells in patients, treatment efficacy can depend on the patient’s subtype of CD4+ T cells. Thus, the highly predictive cell type contexts according to Pinnacle should be further investigated to design safe and efficacious therapies for RA and IBD diseases. Conversely, we hypothesize that the cell type contexts of protein representations that yield worse performance than the cell type-agnostic protein representations may not have the predictive power (given the current list of targets from drugs that have at least completed phase 2 of clinical trials) for studying the therapeutic effects of candidate targets for RA and IBD therapeutic areas. In the context-aware model trained to nominate therapeutic targets for RA diseases, the protein representations of duodenum glandular cells, endothelial cells of hepatic sinusoid, myometrial cells, and hepatocytes performance worse than the cell type-agnostic protein representations (Figure 5a). The RA therapeutic area is a group of inflammatory diseases in which immune cells attack the synovial lining cells of joints. Since duodenum glandular cells (Pinnacle-predicted rank = 153), endothelial cells of hepatic sinusoid (Pinnacle-predicted rank = 126), myometrial cells (Pinnacle-predicted rank = 119), and hepatocytes (Pinnacle-predicted rank = 116) are neither immune cells nor found in the synovium, these cell type contexts’ protein representations expectedly perform poorly. For IBD diseases, the protein representations of the limbal stem cells, melanocytes, fibroblasts of cardiac tissue, and radial glial cells have worse performance than the cell type-agnostic protein representations (Figure 5d). The IBD therapeutic area is a group of inflammatory diseases in which immune cells attack tissues in the digestive tract. As limbal stem cells (Pinnacle-predicted rank = 152), melanocytes (Pinnacle-predicted rank = 147), fibroblasts of cardiac tissue (Pinnacle-predicted rank = 135), and radial glial cells (Pinnacle-predicted rank = 107) are neither immune cells nor found in the digestive tract, these cell type contexts’ protein representations should also perform worse than context-free representations. The least predictive cellular contexts in Pinnacle’s models for RA and IBD have no known role in disease, indicating that protein representations from these cell type contexts are poor predictors of RA and IBD therapeutic targets. Pinnacle’s overall improved predictive ability compared to context-free models indicates the importance of understanding cell type contexts in which therapeutic targets are expressed and act. Predictive cell type contexts reflect mechanisms of action used in successful RA therapies. Recognizing and leveraging the most predictive cell type context for examining a therapeutic area can be beneficial for predicting candidate therapeutic targets. We find that considering only the most predictive cell type contexts can yield significant performance improvements compared to context-free models (Supplementary Figure S14). To further illustrate the utility of the hypotheses generated by our models, we examine the most predictive cell type contexts of two protein targets for widely used treatments that mitigate the symptoms of RA: JAK3 and IL6R. Disease-modifying anti-rheumatic drugs (DMARDs), such as Janus kinase (JAK) inhibitors (i.e., tofacitinib, upadacitinib, and baricitinib), are commonly prescribed to patients with RA. For JAK3, Pinnacle’s five most predictive cell type contexts are T follicular helper cells, microglial cells, DN3 thymocytes, CD4+  memory T cells, and hematopoietic stem cells (Figure 5b). Since the expression of JAK3 is limited to hematopoietic cells, mutations or deletions in JAK3 tend to cause defects in T cells, B cells, and NK cells. For instance, patients with JAK3 mutations tend to be depleted of T cells, and the abundance of T follicular helper cells is highly correlated with RA severity and progression. JAK3 is also highly expressed in double negative (DN) T cells (early stage of thymocyte differentiation), and the levels of DN T cells are higher in synovial fluid than peripheral blood, suggesting a possible role of DN T cell subsets in RA pathogenesis. Lastly, dysregulation of the JAK/STAT pathway, which JAK3 participates in, has pathological implications for neuroinflammatory diseases, a significant component of disease pathophysiology in RA. Tocilizumab and sarilumab are FDA approved for the treatment of RA and target the interleukin six receptor, IL6R. For IL6R, Pinnacle’s five most predictive cellular contexts are classical monocytes, NAMPT neutrophils, intermediate monocytes, mesenchymal stem cells, and regulatory T cells (Figure 5c). IL6R is predominantly expressed on neutrophils, monocytes, hepatocytes, macrophages, and some lymphocytes. IL6R simulates the movement of T cells and other immune cells to the site of infection or inflammation and affects T cell and B cell differentiation. IL6 acts directly on neutrophils, essential mediators of inflammation and joint destruction in RA, through membrane-bound IL6R. Experiments on fibroblasts isolated from the synovium of RA patients show that anti-IL6 antibodies prevented neutrophil adhesion, indicating a promising therapeutic direction for IL6R on neutrophils. Lastly, mice studies have shown that pre-treatment of mesenchymal stem/stromal cells with soluble IL6R can enhance the therapeutic effects of mesenchymal stem/stromal cells in arthritis inflammation. Pinnacle’s hypotheses to examine JAK3 and IL6R in the highly predictive cell type contexts, according to Pinnacle, to maximize therapeutic efficacy seems to be consistent with their roles in the cell types. It seems that targeting these proteins may directly impact the pathways contributing to the pathophysiology of RA therapeutic areas. Further, our results for IL6R suggest that Pinnacle’s contextualized representations could be leveraged to evaluate potential enhancement in efficacy (e.g., targeting multiple points in a pathway of interest). Predictive power of cell type contexts elucidates the mechanisms of action used in successful IDB therapies. Similar to RA therapeutic area, we must understand the cells in which therapeutic targets are expressed and act to maximize the efficacy of molecular and cellular therapies for IBD therapeutic area. To support our hypothesis, we evaluate Pinnacle’s predictions for two protein targets of commonly prescribed treatments for IBD diseases: ITGA4 and PPARG. Vedolizumab and natalizumab target the integrin subunit alpha 4, ITGA4, to treat the symptoms of IBD therapeutic area. Pinnacle’s five most predictive cell type contexts for ITGA4 are regulatory T cells, dendritic cells, myeloid dendritic cells, granulocytes, and CD8+  cytotoxic T cells (Figure 5e). Integrins mediate the trafficking and retention of immune cells to the gastrointestinal tract; immune activation of integrin genes increases the risk of IBD. For instance, ITGA4 is involved in homing memory and effector T cells to inflamed tissues, including intestinal and non-intestinal tissues, and imbalances in regulatory and effector T cells may lead to inflammation. Circulating dendritic cells express the gut homing marker encoded by ITGA4; the migration of blood dendritic cells to the intestine leads these dendritic cells to become mature, activated, and contribute to gut inflammation and tissue damage, indicating that future studies are warranted to elucidate the functional properties of blood dendritic cells in IBD. Balsalazide and mesalamine are aminosalicylate drugs (DMARDs) commonly used to treat ulcerative colitis by targeting peroxisome proliferator activated receptor gamma (PPARG). Pinnacle’s five most predictive cell types for PPARG are paneth cells of the epithelium of large intestines, endothelial cells of the vascular tree, classic monocytes, goblet cells of small intestines, and serous cell of epithelium of bronchus (Figure 5f). PPARG is highly expressed in the gastrointestinal tract, higher in the large intestine (e.g., colonic epithelial cells) than the small intestine. In patients with ulcerative colitis, PPARG is often substantially downregulated in their colonic epithelial cells. PPARG promotes enterocyte development and intestinal mucus integrity by increasing the abundance of goblet cells. Further, PPARG activation can inhibit endothelial inflammation in vascular endothelial cells, which is significant due to the importance of vascular involvement in IBD. Additionally, PPARG agonists have been shown to act as negative regulators of monocytes and macrophages, which can inhibit the production of proinflammatory cytokines. Intestinal mononuclear phagocytes, such as monocytes, play a major role in maintaining epithelial barrier integrity and fine-tuning mucosal immune system responsiveness. Studies show that newly recruited monocytes in inflamed intestinal mucosa drive the immunopathogenesis of IBD, suggesting that blocking monocyte recruitment to the intestine could be one avenue for therapeutic development. Lastly, PPARG is found to regulate mucin and inflammatory factors in bronchial epithelial cells. Given the pulmonary complications of IBD, PPARG could be a promising target to investigate for treating IBD and pulmonary symptoms. The predictive power of cell type contexts to examine ITGA4 and PPARG, according to Pinnacle, for IBD therapeutic development are thus well-supported.","Pinnacle is a flexible geometric deep learning approach for contextualized prediction in user-defined biological contexts. Integrating single-cell transcriptomic atlases with the protein interactome, cell type interactions, and tissue hierarchy, Pinnacle produces latent protein representations specialized to each biological context. Pinnacle’s protein representations capture cellular and tissue organization spanning 156 cell types and 62 tissues of varying hierarchical scales. In addition to multi-modal data integration, a pretrained Pinnacle model generates protein representations that can be used for downstream prediction for tasks where cell type dependencies and cell type-specific mechanisms are relevant. One limitation of the study is the use of the human protein interactome, which is not measured in a cell type-specific manner. No systematic measurements of protein interactions across cell types exist. So, we create cell type-specific protein interaction networks by overlaying single-cell measurements on the reference protein interaction network, leveraging previously validated techniques for the reconstruction of cell-type-specific interactomes at single-cell resolution and conducting sensitivity network analyses to confirm the validity of the networks used to train Pinnacle models (Supplementary Figures S2–S3). This approach enriches networks for cell type-relevant proteins (Supplementary Figure S2). The resulting networks may contain false-positive protein interactions (e.g., proteins that interact in the reference protein interaction network but do not interact in a specific cell type) and false-negative protein interactions (e.g., proteins that interact only within a particular cell type context that may be unknown, or even undiscovered). Nevertheless, strong performance gains of Pinnacle over context-free models indicate the importance of contextualized prediction and suggest a direction to enhance existing analyses on protein interaction networks. We can leverage and extend Pinnacle in many ways. Pinnacle can accommodate and supplement diverse data modalities. We developed Pinnacle models using Tabula Sapiens, a molecular reference atlas comprising almost 500,000 cells from 24 distinct tissues and organs. However, since the tissues and cell types associated with specific diseases may not be entirely represented in the atlas of healthy human subjects, we anticipate that our predictive power may be limited. Tabula Sapiens does not include synovial tissues associated with RA disease progression, but these can be found in synovial RA atlases and stromal cells obtained from individuals with chronic inflammatory diseases. To enhance the predictive ability of Pinnacle models, they can be trained on disease-specific or perturbation-specific networks. In addition to using physical protein contacts, Pinnacle can also be applied to cell type-specific protein networks created from other modalities, such as cell type-specific gene expression networks. Furthermore, we show that Pinnacle’s representations can supplement protein representations generated from other data modalities, including protein molecular surfaces. While this study is concentrated on protein-coding genes, information on protein isoforms and differential information, such as alternative splicing or allosteric changes, can be used with Pinnacle when such data become available. Also, in this study, Pinnacle representations capture physical interactions between proteins at the cell type level. These representations can be combined with representations produced from protein sequences and structures to generate higher resolution predictions. In addition to prioritizing potential therapeutic protein targets, Pinnacle’s representations can be fine-tuned to identify populations of cells with specific characteristics, such as drug resistance, adverse drug events, or disease progression biomarkers. Lastly, to move towards a “lab-in-the-loop” framework, where computational and experimental scientists can iteratively refine the machine learning model and validate hypotheses via experiments, recent techniques on conformal prediction and evidential layers can be integrated with Pinnacle to quantify uncertainty of model outputs. Existing protein representation learning models are context-free and are limited in directly analyzing protein phenotypes that are resolved by contexts and vary with cell types and tissues. To address this limitation, we introduce Pinnacle that produces specialized protein representations tailored to cell type contexts. We demonstrate that contextual learning can provide a more comprehensive understanding of protein roles across cell type contexts. As experimental technologies advance, it is becoming feasible to generate adaptive protein representations across cell type contexts and leverage contextualized representations to predict cell type specific protein functions and nominate therapeutic candidates at the cell type level. Looking to the future, understanding protein functions and developing molecular therapies will require a comprehensive understanding of the roles that proteins have in different cell types as well as the interactions between proteins across diverse cell type contexts. Approaches, such as Pinnacle, can pave the way to realizing this future potential by generating contextualized protein representations and using them to predict cell type-specific protein functions and nominate therapeutic targets at the cell type level.",10.1101/2023.07.18.549602
PMC10153143,37131837,Integrating Expert Knowledge with Deep Learning Improves QSAR Models for CADD Modeling,"In recent years several applications of graph neural networks (GNNs) to molecular tasks have emerged. Whether GNNs outperform the traditional descriptor-based methods in the quantitative structure activity relationship (QSAR) modeling in early computer-aided drug discovery (CADD) remains an open question. This paper introduces a simple yet effective strategy to boost the predictive power of QSAR deep learning models. The strategy proposes to train GNNs together with traditional descriptors, combining the strengths of both methods. The enhanced model consistently outperforms vanilla descriptors or GNN methods on nine well-curated high throughput screening datasets over diverse therapeutic targets. Graphical Abstract","Quantitative structure activity relationship (QSAR) modeling is one fundamental task in computer-aided drug discovery (CADD). In a machine-learning setting, traditionally expert-crafted descriptors are used to train a classifier to distinguish active and inactive compounds. Numerous studies recently applied graph neural networks (GNNs) to molecule-related tasks, given the intrinsic graph nature of molecules. GNNs can learn molecular representations from molecular input data. Compared with the expert-crafted descriptors, the learned representations are believed to have several advantages over the fixed descriptors, including parsimony, interpretability, and task relevance. Even though previous studies have some promising results in GNN performance in molecular tasks, many of them either do not compare them with traditional approaches (e.g. only compares their method with other GNN methods), or the task is not on QSAR modeling (such as quantum mechanics dataset QM9 used in). Recently, some authors expressed doubts about deep learning performance over traditional methods in molecular tasks. Ultimately, it remains a mystery whether GNNs are consistently better than methods that rely on traditional descriptor in CADD. We hypothesize that these conflicting reports might invite an integrated method that combined GNNs with traditional descriptors to outperform both individual approaches, at least at the moment. Here we report such an integrated strategy. Evaluation on nine well-curated high throughput screening datasets demonstrates the effectiveness of this strategy, even over diverse therapeutic targets.","We train a neural network to predict activity combinig a GNN-derived molecular representation with the traditional expert-crafted descriptors. Specifically, for a representation  from the GNN, we concatenate it with the descriptor .   where  is the input molecular graph and,  is a descriptor.  is a classifier, usually a Multi-Layer-Perceptron (MLP).  is the predicted activity. The model is trained by optimizing the binary cross entropy loss :  where  is the number of samples in a batch, and  is the experimentally determined active/inactive status of the -th molecule. Experiments Datasets We validate the effectiveness of the proposed strategy via nine well-curated high-throughput screening (HTS) datasets. To avoid issues with experimental artifacts and high false positive rates, for the validation of our strategy, we chose datasets carefully curated from high throughput screens in the PubChem database. Only datasets with robust secondary validation of compounds were considered. SMILES from the datasets were converted to SDF files using Open Babel, version 2.4.1. Standardized 3D coordinates are generated using Corina, version 4.3. Molecules are further filtered with atom type validity and duplicates with the BioChemical Library (BCL). Random split is used for the experiments, and each dataset is split into 80% for training and 20% for testing. Because preliminary results and previous literature have shown that dropout can help avoid overfitting and the number of known active compounds is limited, we take the model from the last training epoch instead of the one from early stopping determined by validation performance. Multiple splits are used to prove the robustness of the proposed strategy. Evaluation Metric Logarithmic Receiver-Operating-Characteristic Area Under the Curve with the False Positive Rate in the range [0.001, 0.1] : Ranged logAUC is used because only a small percentage of molecules predicted with high activity can be selected for experimental tests in consideration of cost in a real-world drug discovery campaign. This high decision cutoff corresponds to the left side of the Receiver-Operating-Characteristic (ROC) curve, i.e., those False Positive Rates (FPRs) with small values. Also, because the threshold cannot be predetermined, the area under the curve is used to consolidate all possible thresholds within a certain small FPR range. Finally, the logarithm is used to bias towards smaller FPRs. Following prior work, we choose to use . A perfect classifier achieves a  of 1, while a random classifier reaches a  of around 0.0215, as shown below:  Results We used three GNN models in our experiments: GCN, SchNet, and SphereNet. We used the BioChemical Library to generate traditional QSAR descriptors. Following previous examples, we use the optimal descriptors where 391-element molecular-level features are generated. As can be seen from Table 2, BCL outperforms all three GNNs consistently. However, GNN performance is significantly augmented with the BCL descriptors in all nine datasets across all three GNNs. Moreover, although GCN, SchNet, and SphereNet perform differently, the enhanced models get similar results, demonstrating the robustness of this training strategy. In the following paragraph, we discuss the possible reasons for the effectiveness of this strategy. The rationale for this approach is three-fold. First, data available for training in drug discovery campaigns is usually limited due to the high cost of experimental assays, and hence a way to increase usage efficiency is desired. The expert-crafted descriptors supplement GNNs with prior knowledge, i.e., descriptors that worked well in QSAR modeling in the past, which reduces the need for GNNs to learn that knowledge from a large amount of data. Secondly, GNNs typically have difficulty learning molecular-level features due to their limited receptive field or learning non-additive molecular-level features such as total polar surface area. On the other hand, molecular-level descriptors provide the global features directly. Thirdly, GNN intrinsically suffers from problems such as oversmoothing and oversquashing that introduce information loss in obtaining the global learned embedding from the atomic features. Meanwhile, the descriptors extract the molecular features directly and hence circumvent information loss, complementing GNN learned embeddings. BCL features can be categorized into three types: 1). scalar 2). signed 2D autocorrelation 3). signed 3D autocorrelation. Thus, we did further analysis of each feature type’s impact provided in the supplementary materials. We note that a previous work ChemProp D-MPNN used a similar strategy to boost their designed GNN in practice, further illustrating this method’s practical value.",,,10.1101/2023.04.17.537185
PMC10327002,37425715,SynBot: An open-source image analysis software for automated quantification of synapses,"Summary The formation of precise numbers of neuronal connections, known as synapses, is crucial for brain function. Therefore, synaptogenesis mechanisms have been one of the main focuses of neuroscience. Immunohistochemistry is a common tool for visualizing synapses. Thus, quantifying the numbers of synapses from light microscopy images enables screening the impacts of experimental manipulations on synapse development. Despite its utility, this approach is paired with low throughput analysis methods that are challenging to learn and results are variable between experimenters, especially when analyzing noisy images of brain tissue. We developed an open-source ImageJ-based software, SynBot, to address these technical bottlenecks by automating the analysis. SynBot incorporates the advanced algorithms ilastik and SynQuant for accurate thresholding for synaptic puncta identification, and the code can easily be modified by users. The use of this software will allow for rapid and reproducible screening of synaptic phenotypes in healthy and diseased nervous systems.","Proper brain function relies on the correct wiring of neuronal networks. Asymmetric neuron-neuron connections, known as synapses, are the fundamental cell biological units of neural circuits. Importantly, synapse loss or dysfunction is a hallmark of many neurodevelopmental and neurodegenerative disorders. Therefore, studying synapse connectivity is crucial to understanding how neuronal circuits are established during development, remodeled throughout life, and impacted by diseases. Synapses are composed of two neuronal structures: the presynapse, located in the axon terminal, and the postsynapse, located on the dendrite. The presynapse contains specialized, neurotransmitter-filled, synaptic vesicles. Some of these vesicles are docked at the presynaptic active zone and are fused with the membrane when an action potential reaches the synapse. Neurotransmitters then diffuse into the extracellular space between the pre-and post-synapse, called the synaptic cleft. Neurotransmitters in the synaptic cleft bind post-synaptic neurotransmitter receptors. These receptors are transmembrane proteins that pass ions and/or recruit intracellular signaling partners to transduce a signal into the post-synaptic cell and are anchored at the postsynapse through a scaffold of postsynaptic density proteins. Synapses are dynamic structures that can be strengthened or lost due to changes in the inputs that neurons receive. On the other hand, perturbations in genes that control synaptogenesis would also impact the number and organization of synaptic structures. Non-neuronal cell types, such as astrocytes, microglia, and oligodendrocytes, also serve as critical regulators of synapse formation and elimination. In particular, astrocytes, the major perisynaptic cells in the brain, strongly induce the formation of excitatory and inhibitory synapses through direct contact with neuronal processes or via the secretion of several synaptogenic proteins (reviewed in). The gold standard methods for interrogating the structure and function of synapses are electron microscopy and electrophysiology, respectively. Electron microscopy (EM) allows the experimenter to visualize the synapse with a high enough resolution (~2nm) to resolve the pre-and post-synaptic compartments individually. The characteristics of presynaptic vesicles and postsynaptic densities are used to identify excitatory versus inhibitory synapses. An experimenter can count the number of opposing pre-and post-synaptic sites in electron micrographs of samples from various conditions to determine if these conditions alter the number of synaptic structures. A common method for investigating synaptic function is the whole-cell patch-clamp analysis of miniature postsynaptic currents. The frequency of miniature postsynaptic currents provides information about the number of synapses a cell receives or the probability of release at the presynaptic site. On the other hand, the amplitude of these currents measures postsynaptic strength. Electron microscopy and electrophysiology continue to provide high-resolution structural and functional information about synaptic connectivity, but these techniques have major limitations. First, they require extensive sample preparation and specialized equipment, making them difficult to establish in a laboratory that does not specialize in them. Second, they have very low throughput and sample only a small subset of synapses or neurons, making them unsuited for screening multiple experimental conditions. To address these limitations, higher-throughput methods for assessing synapse numbers using immunostaining have proven useful. These histological methods use antibodies to label pairs of pre-and post-synaptic proteins that are specialized to distinct synaptic subtypes. For example, presynaptic markers such as Vesicular Glutamate transporter 1 (VGlut1), Vesicular Glutamate transporter 2 (VGlut2), or bassoon can be paired with postsynaptic markers postsynaptic density protein 95 (PSD95) or Homer-1 to label excitatory synapses. On the other hand, the Vesicular GABA transporter (VGAT), together with the postsynaptic Gephyrin, mark inhibitory synapses. See Verstraelen et al., 2020 for a detailed comparison of synaptic marker performance. When these markers are imaged using light microscopy, due to the resolution limit (200–300nm), and the short (20–30nm) distance between the pre-and post-synaptic compartments, the color signals appear to overlap at synapses. Therefore, synaptic structures can be quantified by counting the number of colocalized pre-and post-synaptic markers using specialized image analysis software. One of the first programs for performing this analysis is the Puncta Analyzer plugin for ImageJ, which has been widely used in neuroscience and yielded results that were then validated through EM and electrophysiology. A major limitation of Puncta Analyzer is that all analysis steps require user input, which are time consuming and can be highly subjective. Thus, image analysis using Puncta Analyzer is a lengthy process that requires extensive user training. In addition, the source code for Puncta Analyzer is complex and difficult to edit to allow for customization. To circumvent these technical challenges, several alternative analysis pipelines have been developed that automate portions of synapse counting. These include methods to count synapses along a neurite (SynD and SynPAnal), using additional filtering steps to improve synapse detection from manual thresholding (SynapseJ), using automated thresholding algorithms in FIJI (Synapse Counter), or development of statistical thresholding algorithms (SynQuant). While these methods provided important features beyond what is available in Puncta Analyzer, none have been as widely used as Puncta Analyzer, which is well suited to quantify densely packed synapses like those seen in the mouse brain tissues. To circumvent the limitations of Puncta Analyzer, here we developed and tested a new open-source ImageJ-based synapse analysis software we call SynBot, which is optimized not only for in vitro but also for high-noise in vivo images and allows for a wide variety of options to tailor the analysis to the experimenter’s needs. We compared the accuracy and efficiency of SynBot with Puncta Analyzer using simulated and experimental data which were previously validated by EM and electrophysiology.","RESOURCE AVAILABILITY Lead contact Further information and requests for resources and reagents should be directed to and will be fulfilled by the lead contact, Cagla Eroglu (cagla.eroglu@duke.edu). Materials availability This study did not generate new unique reagents. Data and code availability Microscopy data and associated synapse counts reported in this paper are available on Zenodo at the following DOI: 10.5281/zenodo.12191805 All original code has been deposited at our lab Github site (https://github.com/Eroglu-Lab/Syn_Bot) and is publicly available on Zenodo at the following DOI: 10.5281/zenodo.12192447 Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request. EXPERIMENTAL MODEL AND SUBJECT DETAILS Animals Mice and rats were used for experiments as specified in the text and figure legends. All mice and rats were used in accordance with the Institutional Animal Care and Use Committee (IACUC) and the Duke Division of Laboratory Animal Resources (DLAR) oversight (IACUC Protocol Numbers A147-17-06 and A117-20-05). All mice and rats were housed under typical day/night conditions of 12-hour cycles. For all experiments, age, and sex-matched mice were randomly assigned to experimental groups based on genotypes. The primary rat neurons and astrocytes were isolated from wildtype Crl: CD(SD) Sprague-Dawley rats from Charles River Laboratories (RRID: RGD_734476). METHOD DETAILS In vitro Synapse Assay with Astrocyte Conditioned Media Cortical tissue digestion Primary rat astrocytes and neurons were isolated from postnatal day 1 (P1) rat pups. Pups were rapidly decapitated, and brains dissected out and placed into a petri dish of PBS. Brains were then micro-dissected to isolate the cerebral cortex, and the cortices were chopped into pieces approximately 1 cubic millimeter in volume. A transfer pipet was then cut where it began to taper to create a larger opening. This cut pipet was then used to suck up the cortex chunks, and tissue was transferred to a tube of 7.5 units/mL papain digestion solution (Worthington, Cat# LK003178) by gently tapping on the pipette without depressing it. Cortices were then digested for 45 minutes at 33°C with a brief swirling at the 15-and 30-minute points. The digestion solution is then aspirated off and the tissue is resuspended in a 2 mg/ml trypsin inhibitor ovomucoid solution (Lo Ovo) (Worthington, Cat# LS003083). Cells were then spun in a centrifuge at 300g for 11 minutes at room temperature. The supernatant was then removed, and the cells were resuspended in a 4 mg/ml Ovomucoid solution (Hi Ovo). Cells were then spun again at 300g for 11 minutes and the supernatant removed. The resulting cells were then either used for isolating neurons or astrocytes according to the following sections. Neuronal Isolation by Immunopanning The pelleted cells from the cortical tissue digestion were resuspended in Panning Buffer (0.02% BSA with 0.5 ug/ml insulin in DPBS with calcium, magnesium, glucose, and pyruvate (Thermo Fisher, Cat# 14287080)). The cells were then filtered through a 20μm nylon mesh (Elko Filtering, Cat# 03-20/14) to remove clumps. These filtered cells were then added to a petri dish which had been treated with Griffonia Simplicifolia Lectin I (Vector Laboratories, Cat# L-1100) for 1 hour at room temperature prior to adding the cells. The cells were incubated on this lectin plate for 10 minutes at room temperature. The plate was then forcefully shaken to resuspend any loosely adhered cells. The solution in the lectin plate was then transferred to a second lectin plate for a 15-minute incubation at room temperature. The second lectin plate was then forcefully shaken, and the solution transferred to a plate treated with an L1CAM antibody (α-L1) (Developmental Studies Hybridoma Bank, Cat# ASCS4) which binds to neurons. The cells were incubated on the α-L1 plate for 45 minutes at room temperature. The media was then gently removed from the α-L1 plate and the plate was washed 5 times with panning buffer. Cells bound to the α-L1 plate were then resuspended in the panning buffer and collected in a conical tube. The cells were then spun for 11 minutes at 300g. The supernatant was removed, and the cells were resuspended in neuronal growth media (NGM: Neurobasal (Gibco, Cat# 21103049), B27 supplement (Gibco, Cat# 17504044), 2 mM L-Glutamine (Gibco, Cat# 25030-081), 100 U/mL Pen/Strep (Gibco, Cat# 15140), 1 mM sodium pyruvate (Gibco, Cat# 11360-070), 4.2 μg/mL Forskolin (Sigma, Cat# F6886), 50 ng/mL BDNF (PeproTech, Cat# 450-02), and 10 ng/mL CNTF (PeproTech, Cat# 450-13)). The cells were then counted, and plated at a low density of 60,000 cells each onto poly-D-lysine-treated (Sigma, Cat# P6407) and mouse laminin (Cultrex, Cat# 3400-010-01) treated glass coverslips in a 24-well plate with neuronal growth media. Cells were then kept in a tissue culture incubator at 37°C with 10% CO2. Astrocyte Isolation and ACM Production The pelleted cells from the cortical tissue digestion were resuspended in astrocyte growth media (AGM: DMEM (GIBCO, Cat# 11960), 10% FBS (Thermo Fisher, Cat# 10–437-028), 10 mM, hydrocortisone (Sigma, Cat# H-0888), 100 U/mL Pen/Strep, 2 mM L-Glutamine, 5 mg/mL Insulin (Sigma, Cat# 11882), 1 mM Sodium Pyruvate, 5 mg/mL N-Acetyl-L-cysteine (Sigma, Cat# A8199)). The cells were then filtered through a 20μm nylon mesh to remove cell clumps. The filtered cells were then spun for 9 minutes at 300g. The supernatant was then removed, and the cells were resuspended in AGM and counted. 15 million cells were then plated in a T-75 flask with a non-aerating top. Cells were then kept in a tissue culture incubator at 37°C with 10% CO2 with the cap slightly unscrewed to allow airflow. On DIV3, astrocyte flasks were washed 3 times with DPBS without calcium or magnesium (Gibco, Cat# 14190144). After the third wash, 20 mL of DPBS without calcium or magnesium was added and the flask was vigorously shaken to remove loosely adherent cells. This shake-off process significantly enriches for astrocytes. Media was then replaced with AGM and flasks were returned to their incubator. On DIV 5, astrocytes were treated with 2.43 mg/ml Cytosine arabinoside (AraC) (Sigma, Cat# C1768) through a full media change. On DIV 7 astrocytes were passaged using 0.05% Trypsin-EDTA. 3 million cells each were then plated into 10 cm culture dishes. On DIV 9, astrocytes were switched to a minimal media (Neurobasal medium minus phenol red (Gibco, Cat# 12348017), 100 U/mL pen/strep, 2mM L-glutamine, and 1mM sodium pyruvate) for astrocyte conditioning. Astrocytes were then cultured for 5 days without any media change, and on DIV 14, the ACM was collected. The ACM was then centrifuged for 5 minutes at 1,100g to pellet cellular debris. The supernatant was concentrated by centrifugation for 1 hour at 3220 g at 4°C in a 5kDa Cutoff Vivaspin tube (Sartorius, Cat # VS2012). ACM was aliquoted into low protein binding tubes (~100ul per aliquot), flash-frozen in liquid nitrogen, and stored at −80°C until use. An aliquot of each batch of ACM was thawed and used to measure the protein content using the Pierce BCA protein assay kit (Thermo Fisher, Cat# 23225). Neuron Feeding and ACM Treatment On DIV2, neurons were treated with 2.43 mg/ml AraC through a half-media change. A full media change was then performed 24 hours later (DIV3) to remove the AraC. Neurons were then given another half-media change on DIV6. Neurons were then fed with ACM on DIV8 and DIV 11 by adding either 50 mg/ml ACM for excitatory synapses or 100 mg/ml ACM for inhibitory synapses in a half media change. Neuron Staining and Imaging On DIV 13, neurons were fixed with 4% paraformaldehyde (Electron Microscopy Sciences, Cat# 15710) in PBS. Cells were then washed 3 times with PBS and then blocked for 30 minutes at room temperature with antibody blocking buffer (150 mM NaCl, 50 mM Tris-Base, 1% BSA, 100 mM L-lysine 50% normal goat serum (Thermo Fisher, Cat# 01–6201), 0.2% Triton (Roche, Cat# 11332481001). The blocking buffer was then removed, and cells were incubated with primary antibodies against the pre-and post-synaptic markers of interest overnight at 4°C in antibody incubation buffer (150 mM NaCl, 50 mM Tris-Base (VWR, Cat# 101174–856), 1% BSA, 100 mM L-lysine, 10% normal goat serum). The following day cells were washed 3 times with PBS and incubated in fluorescent secondary antibodies in antibody incubation buffer for 2 hours at room temperature protected from light. Cells were then washed with PBS 3 times and incubated with 1:10,000 DAPI (Invitrogen, Cat#D1306) for 5 minutes. Cells were then washed 3 times with PBS and mounted onto coverslips with homemade mounting media (20mM Tris pH 8.0, 90% Glycerol, 0.5% N-propyl gallate) and sealed with nail polish. Single focal plane images were taken using an inverted Olympus FV3000 confocal laser scanning microscope with a 60X oil immersion objective. The images were taken in a 1024X1024 resolution with a 2X digital zoom. For the picture acquisition, the DAPI channel was used to select neuronal cell bodies distant from other neurons to avoid overlap of cell soma and to do a blind selection of the neurons. DAPI positive nuclei with abnormal morphology were excluded for the analysis. 20 cells for each condition were selected for the analysis. Images were also saved as .bmp extension with only Bassoon (green) and Gephyrin (red channel) for the synaptic analysis. Mouse Brain Tissue Synapse Assay Mouse perfusion and cryosectioning P21 α2δ−1 WT and KO mice were euthanized by intraperitoneal injection of 0.6mL of 12.5 mg/mL 2,2,2-tribromoethanol (Avertin) (Sigma, Cat# T48402–25G) followed by exsanguination through transcardially perfusion of tris-buffered saline (TBS: 137mM NaCl, 2.68 mM KCl, 24.8 mM Tris-base (Thermo Fisher, Cat# J75825.A7)). Following TBS perfusion, animals were perfused with warm, 4% paraformaldehyde (PFA) in TBS solution. Brains were then removed and post-fixed in 4% PFA overnight at 4°C. After post-fixation, brains were washed 3 times with TBS and then incubated in 30% sucrose in TBS for 48–72 hours. Brains are fully cryopreserved once they sink to the bottom of their container in the 30% sucrose. Brains were then placed in cryomolds and frozen in 50% Optimal Cutting Temperature Compound (OCT) (Tissue Tek, Cat# 4583) in TBS on dry ice. Brains were then kept at −80°C until sectioning. Brains were cryosectioned to a thickness of 15–40 μm and stored in 50% glycerol (Acros Organics, Cat# 15892–0010) in TBS at −20°C. Tissue Section Synapse Staining and Imaging Three tissue sections per animal were washed 3 times for ten minutes with 0.2% TritonX-100 in TBS (TBST) on a shaker at room temperature. Sections were then blocked with 5% normal goat serum in TBST for 1 hour at room temperature on a shaker. Sections were then stained with primary antibodies for the pre and postsynaptic markers of interest overnight at 4°C on a shaker. Sections were then washed 3 times in TBST for 10 minutes and incubated with fluorescent secondary antibodies for 2 hours at room temperature on a shaker protected from light. After secondary incubation, sections were washed 3 times with TBST for 10 minutes. After washing, sections were mounted onto microscope slides and sealed with nail polish. Stained tissue is ready to image after nail polish is dried and should be imaged within 48 hours of secondary staining for most accurate results. Images were acquired using a Leica SP5 confocal microscope with a 63X oil immersion objective. For 1024 X 1024 images, a 1.64X digital zoom was used to achieve an XY resolution of 0.126 um × 0.126 μm pixel size and a z-stack of 15 optical planes was acquired using a 0.34 μm z-step to image ~ 5 μm of the sample. In general, images were acquired such that confocal settings were optimized on control conditions with the goal of minimizing oversaturated pixels but maintaining real synaptic puncta. Image Analysis Synapse Quantification with Puncta Analyzer In vitro synapse assay images were analyzed using Puncta Analyer by first converting the images to the RGB format and then running the Puncta Analyzer plugin on each image. A circular ROI was applied to each image with a radius of 301 pixels around the cell body of the imaged neuron. A threshold for each image was chosen to minimize the background while preserving the signal. In vivo, synapse assay images were Z-projected such that every 3 optical planes were projected to produce 5 max-projections corresponding to 1 μm, per 15-image stack covering the 5 μm. Images were then analyzed with the Puncta Analyzer plugin using default settings. A threshold for each image was chosen by users to minimize the background while preserving the signal. Synapse Quantification with SynBot Images were analyzed with the SynBot macro using the default settings for the manual thresholding method. A threshold for each image was chosen to minimize the background while preserving the signal. Threshold performance was checked visually using the “_colocs” image SynBot creates that displays which puncta were counted as synapses in each image. For the ilastik thresholding method, images were first preprocessed into individual color channels using the extract_channels macro (available at https://github.com/Eroglu-Lab/Syn_Bot). These single-channel images were then used to train ilastik projects for each image channel. Each ilastik project was trained using the pixel classification mode in ilastik. A random subset of images from the target color channel was chosen for the training set. For both in vitro and in vivo synapse analysis, we used all the image features available. We then annotated approximately 20 synapses as label 1 and marked several regions of the background as label 2 on the first image. We then used the live update feature to view the classifications of each pixel and added additional annotations to the first image and subsequent images as necessary. We then saved the ilastik project files and used these for the SynBot ilastik thresholding method (all ilastik project files used for this paper are available at https://github.com/Eroglu-Lab/Syn_Bot). For the SynQuant thresholding method, images were analyzed using the SynQuant batch thresholding option and SynBot noise reduction. The following SynQuant parameters were used: Simulated data: Z-score threshold = 10, minimum object size = 10, maximum object size = 100, minimum object fill = 0.5, maximum width to height ratio = 4, Z axis multiplier = 1, and estimated noise standard deviation = 12. In vitro data: Z-score threshold = 10, minimum object size = 10, maximum object size = 100, minimum object fill = 0.5, maximum width to height ratio = 4, Z axis multiplier = 1, and estimated noise standard deviation = 20. In vivo data: Z-score threshold = 10, minimum object size = 10, maximum object size = 100, minimum object fill = 0.5, maximum width to height ratio = 4, Z axis multiplier = 1, and estimated noise standard deviation = 12. Creation of Simulated Synapse Images To produce these simulated images, we first used a set of 20 real VGlut1-PSD95 synapse images from Risher et. al., 2019. We split the two image channels and then measured the pixel intensity histogram from these images. We generated a Gaussian noise background for each image by multiplying the mean and standard deviation of the original image’s histogram by a multiplier (0.00, 0.25, 0.50, 0.75, or 1.00). Since the pixel intensity histogram includes the true signal as well as the background, the 1.00 multiplier will produce an image with a higher background than that of the original image. This makes the range of 0 to 1 for our noise multiplier represent images with no background at (0.00), moderate background comparable to experimental data (0.25–0.75), and high background beyond what would be acceptable for analysis (1.00). We next took 10 synaptic puncta from each channel of the original image and pasted these onto the background images we generated. Synaptic puncta were pasted into 1000 possible positions such that 334 positions had a red puncta only, 333 positions had a green puncta only, and 333 positions had a red puncta and a green puncta at the same location. Note that the locations for pasting these synaptic puncta were defined by the top left corner of their bounding box. This resulted in varying levels of overlap similar to real synaptic imaging data and explains the precision and recall of SynBot being below 1.0 since many of these synapses were no longer overlapping after being thresholded. The end product of this code was 100 total images with 20 images having each of the 5 background levels. The ImageJ macro code used for generating these simulated images is available at https://github.com/Eroglu-Lab/Syn_Bot). QUANTIFICATION AND STATISTICAL ANALYSES SynBot was used to quantify synapse numbers in the experiments shown. Subsequent statistical analysis was performed with the R statistical software (R Core Team). Student’s t-tests were used to test for significance between conditions for the in vitro experiments, and linear mixed-effects models were used (R package nlme, R Core Team) to test the significance of in vivo experiments and account for the nested structure of the data (multiple z-stacks per image and multiple images per animal). Statistical details of each experiment can be found in the figure legends. For the cell culture experiments, each n represents a neuron that was imaged. For mouse tissue sections experiments, each n represents the average synapse density of a mouse that was included in the experiment. For each mouse, at least 15 max-projection images corresponding to 3 independent z-stacks were used. The full R code for performing the statistical analyses and producing the plots in this paper is available at https://github.com/Eroglu-Lab/Syn_Bot.","Synapse labeling by immunohistochemistry Synapse quantification relies on the use of immunohistochemistry to fluorescently label the pre-and post-synaptic compartments (Figure 1A). A standard immunostaining workflow is used, including 1) fixation and permeabilization of the sample, 2) application of primary antibodies against at least one pre-synaptic and one post-synaptic marker, 3) application of fluorescent secondary antibodies against the species of the primary antibodies used, and 4) image acquisition by fluorescence microscopy (Figure 1B). This protocol results in images where either excitatory (Figure 1C) or inhibitory (Figure 1D) synapses can be visualized. The details of the methods are explained in the Methods section and at protocols.io (https://www.protocols.io/view/synbot-protocols-3byl4qewjvo5/v2). Developing an automated synapse quantification tool Our first goal in developing SynBot was to enable the efficient quantification of large imaging datasets. SynBot combines many of the image processing and analysis steps that were performed for analysis with Puncta Analyzer into one automated workflow. Toward this aim, we first developed an ImageJ macro to automate the pre-processing steps that convert raw images into numerical representations, which can be used to calculate synaptic colocalizations (Figure 2). First, SynBot checks the image files to determine if they are z-stacks of confocal images or single images (either single optical section from a confocal or epifluorescence image). Then SynBot processes the z-stack files to produce max projections of each 1μm stack. This step is of particular use for in vivo synapse number analyses, which will be discussed in detail in a later section. Second, all images are converted to the RGB format, the color format Image J uses. Third, based on user selection, the FIJI Subtract Background, and Gaussian Blur Filter functions are applied to remove noise from the image. The fourth step is the thresholding of individual synaptic puncta to set a value for the background of the image and exclude the pixels in the image that have an intensity value less than the background. A threshold value can be determined in several different ways. The most common ones are manual thresholding and ilastik-based automated thresholding. The users can also set the threshold to a specified value. Fifth, the user decides whether to quantify synapses within a region of interest or within the whole image. After entering these user inputs, the program runs the FIJI Analyze Particles function to record the location and area of each punctum in the region of interest of the thresholded image. This function records the X and Y coordinates and area of each punctum for each color channel. Each punctum is then compared to the puncta in the other channel of the image by approximating each punctum to a circle or through a pixel-by-pixel approach discussed below. SynBot analysis workflow The files required for running SynBot and the installation instructions can be found at https://github.com/Eroglu-Lab/Syn_Bot. There are also tutorials and instructional videos on protocols.io (https://www.protocols.io/view/synbot-protocols-3byl4qewjvo5/v2). Once the Syn_Bot.ijm macro is started, a selection menu is displayed to allow the users to select their analysis parameters (Figure 2). The selection menu has 6 main sections: 1) Channels, 2) Preprocessing, 3) Thresholding, 4) ROI type, 5) Analysis type, and 6) Experiment Directory. The detailed functions of each option on the SynBot selection menu are defined in Table S1. Below, we summarize the utility of each section. Channels: The fundamental objective of SynBot is to determine the overlap between different color channel objects (i.e., pre-or post-synaptic puncta). SynBot does this by first converting images into the red, green, and blue (RGB) format, which allows for consistent labeling of channels throughout the program. Users can select either the 2-channel option to analyze colocalization between the red and green channels or the 3-channel option to analyze colocalization between the red, green, and blue channels. If your images are in CMYK (cyan, magenta, yellow, and key (black)) format, or you would like to analyze red and blue or green and blue puncta colocalization, then there is also the “Pick channels” option. Z-stack images are max projected to produce individual projections that SynBot analyzes as separate images. For the experiments described below, every 3 Z-stacks were combined to produce a 1 um max projection. The number of stacks to project together can be adjusted by the user or set to 1 to allow individual processing of each optical section. Preprocessing: Images can be preprocessed after RGB conversion to reduce noise or adjust the brightness. Noise reduction is performed by applying FIJI’s subtract background plugin followed by a Gaussian blur filter to aid in object detection. This process removes some of the image background to aid in object detection (see https://imagej.net/plugins/rolling-ball-background-subtraction). Brightness adjustment changes the intensity values of each image such that there is an equal percentage of saturated pixels across images. As these methods modify the image data that SynBot will use for analysis, we advise users to apply them with caution and ensure they are producing accurate results. These modifications should also be consistent across groups of images that will be included in the same experiment. Thresholding: One of the most challenging aspects of image analysis is thresholding, the process of distinguishing the foreground (i.e., synaptic puncta) of an image from the background. To discriminate between true synaptic puncta and the background noise is not trivial due to the small size of synaptic puncta and variability in staining and imaging parameters between users and microscopes. Therefore, here we include a number of possible options that SynBot users can utilize. The most popular of these options is manual thresholding, where the user selects a threshold value for each channel of each image, and SynBot runs the rest of the analysis. In that sense, this option is similar to Puncta Analyzer, the predecessor of SynBot. However, we designed Synbot to have several time-saving user-friendly features. For example, Synbot automatically displays each of the image channels to be thresholded, minimizing the time required for user input. However, the manual thresholding method requires extensive user training to use properly and necessitates a significant amount of hands-on user time. To address the limitations of manual thresholding, we implemented several automated thresholding methods. First, we created a new ilastik-based thresholding method, where a machine learning model is used to threshold each channel of each image. ilastik is an open-source program developed by Anna Kreshuk’s lab at the European Molecular Biology Laboratory. ilastik is trained by the user in a small number of images (typically 3–5) to threshold by extracting a set of user-defined features from the image and then feeding these features into a random forest machine learning model. This model is then applied to the rest of the images in the data set to determine if a given pixel is part of the foreground or background of the image. As in all thresholding methods, ilastik requires careful troubleshooting to ensure accurate results. The training should be done for each experiment independently and should not be used across datasets unless they are collected under identical conditions, such as replicates of the same experiment performed by the same investigator. However, using ilastik has a distinct advantage: it can be applied to a large number of images without the need for further user input and only takes 5–15 minutes to generate the trained model. In addition to ilastik, we also integrated the SynQuant algorithm for thresholding into SynBot. This method was developed by Wang et al. in 2020, to specifically threshold synapses from immunofluorescence images through a statistical probability-based approach. This is explained in detail in Wang et al., 2020 but in short, the method assesses each potential synaptic object by comparing it to its neighboring pixels based on fluorescence intensity, object size, and local contrast. The SynQuant implementation within SynBot allows the user to adjust the following parameters: Z-score threshold (a statistical measurement of how many standard deviations values are from the mean of a population), minimum object size, maximum object size, minimum object fill, maximum width to height ratio, and estimated noise standard deviation. For most datasets, the noise standard deviation and Z-score threshold are the only parameters that require adjustment. The SynQuant batch processing of images through SynBot takes approximately 5–10 minutes for the size of data sets presented here without any need for user input, so trying different parameters on the same images is feasible and recommended to determine the most accurate conditions for analyses. We added another feature to Synbot to aid the reproducibility of results between users. All the thresholding values are saved for each image analyzed. Therefore, an analysis performed with SynBot can be efficiently reproduced later using the “Threshold From File” option. This option asks the user to supply a CSV file containing the threshold values for each channel of each image. This option does not work for the ilastik or SynQuant thresholding options because these algorithms do not use a set thresholding value, but they can already be easily reproduced since they are unsupervised methods. ROI Type: SynBot’s colocalization analysis can be targeted to a region of interest (ROI) within the input image. One common application is the restriction of the analysis to the region around the soma of a cultured neuron, as in Figure 5. Alternatively, users can implement their own complex ROIs by using the FIJI clear functions to remove unwanted regions prior to running SynBot on the entire image. During the selection of the ROI, the user is also asked to input a minimum and a maximum pixel size for each channel puncta. The area of the ROI selected for each image is recorded and included in the summary.csv output file. Analysis Type: Puncta analyzer calculated colocalization based on a circular approximation colocalization approach (Figure 3). Therefore, we included this option also in SynBot. With the circular-approximation analysis mode selected, the coordinates and radius of each punctum for the channels to be analyzed are passed through the Syn_Bot_Helper java plugin (packaged within https://github.com/Eroglu-Lab/Syn_Bot). We used a Java plugin because it can perform the necessary calculations much more efficiently than if they were done with ImageJ macro language. These values for each punctum in the first channel are then compared to each punctum in the second channel to detect colocalizations and calculate their area using the geometry summarized in Figure 3. First, the distance between the centers of the two circles is calculated (Figure 3A). If this distance is less than the sum of the radii of the two circles, then they are counted as a colocalization, and the area of the colocalization is calculated (Figure 3B). The basic idea of the area calculation is to use the two intersection points of the circles along with the center of each circle to define two triangles while also finding the areas of the sector of each circle between the two intersection points. Subtracting the area of each triangle from the area of its surrounding sector gives half of the overlapping area contributed by that punctum. This can be done for both puncta to give the total area of the overlap (co-localization). The coordinates and area of each colocalization are then stored and added to the colocalized puncta count. This method would only work if the synaptic puncta had high circularity, which can be determined using the Measure tool in FIJI and is recorded for individual puncta by SynBot. In our experience, most synaptic markers used in primary culture, or the mouse cortex are approximately circular. A table of experimental circularity values for the antibodies used in this paper is provided in Table S2. We also directly compared this circular approximation to the pixel-by-pixel alternative and found nearly identical results for the types of images shown here (Figure S3). The circular-approximation method is too slow to practically analyze three channel images and fails to correctly calculate colocalizations between non-circular objects. To overcome these limitations, here we developed a pixel-overlap colocalization method within SynBot (Figure 3C). Rather than approximating each punctum to a circle, this analysis mode compares puncta in a pixel-by-pixel fashion using the FIJI Image Calculator plugin’s AND function. The AND function generates a new image that contains pixels that overlap between the red punctum and a green punctum. This new image can then be used to quantify co-localization with a third channel (blue). The coordinate and area information for colocalized puncta is then collected. Thus, this method is more accurate and versatile and runs more quickly than the circular approximation method. Experiment Directory: The final user input field for the SynBot selection menu is to pick a folder that contains experimental images, hereafter called the experimental directory. The directory folder should include at least one subfolder. The subfolder(s) of the experimental directory should contain only the image files to be used by SynBot. We often divide experimental groups between the subfolders of the experimental directory, but this is not required and does not affect the analysis. It is also important that image names contain only one “.” character preceding their file extension. The program uses this “.” to separate the file extension from the image name and will not run properly if there are multiple. Incorrect file naming can be especially problematic with some image type conversion programs that name each image with both the old and new extension (e.g., converting a Leica .lif file to .tif format can result in a name like “image1.lif.tif”). SynBot Outputs: A key feature of SynBot is that the users can easily check if the method is counting the correct objects as colocalized puncta because after each image is analyzed, SynBot creates and saves a feedback image within the Output folder using the name of the image with “_colocs” appended to the end. This “_colocs” image shows the original input image with white circular overlays at the center position of each recorded colocalization. New SynBot users are strongly encouraged to check these images after running the macro and adjust their thresholding parameters or retrain their ilastik model if they see too few or too many calculated colocalizations that do not match what is evident by eye. The thresholded images for each channel are also saved so the users can later validate whether each channel was accurately thresholded. The primary output of SynBot, which contains the numbers of colocalized puncta per image, is the “Summary” csv file. This file contains the most commonly used summary measurements for synapse counting, such as the total numbers of red, green (and blue if triple colocalization is used), and the numbers of colocalized puncta from each image. The file also includes the thresholds and the settings for minimal pixel size. To allow for analysis of the cumulative properties of synaptic puncta, the macro also saves the X and Y coordinates and area of each individual puncta for the red, green, blue, and colocalized puncta into separate CSV files. These results are then ready for statistical analyses by using other software such as R or GraphPad. Quantification of synaptic colocalization by SynBot in simulated images To validate SynBot’s performance and compare the automated thresholding methods that are integrated within SynBot, we tested SynBot on simulated synapse images. We produced 20 simulated images with 667 red puncta, 666 green puncta, and 333 points where these puncta overlap to form a synapse. We then added a Gaussian noise background to each image with a mean and standard deviation proportional to the intensity histogram of the original image multiplied by 0.00, 0.25, 0.50, 0.75, or 1.00 to simulate images with varying noise levels (Figure 4A). This produces a set of images with no background (0.00), a background similar to what would be acquired in an experiment (0.25–0.75), and an excessive level of background (1.00). We then ran SynBot on these images using manual, ilastik, or SynQuant thresholding. Since the position of these simulated synapses was known, we computed the recall (True Positive / (True Positive + False Negative)) (Figure 4B) and precision (True Positive / (True Positive + False Positive)) (Figure 4C) for each method on our simulated images. These metrics are well suited to analyses where true positive, false positive, and false negative counts are obtained but true negative counts are not well defined (every point on the image without a synapse could be considered a true negative). We found that the recall and precision values were similar for each method. However, the methods all had decreased accuracy when the images contained excessive background. We found no significant differences between the recall or precision obtained with these thresholding methods when using a 2-way ANOVA at each noise level. Together these analyses indicate that SynBot accurately counts synapses in realistic images containing low and high background noise. Quantification of astrocyte-induced synaptogenesis by SynBot in vitro. To test SynBot’s utility and accuracy, we next used a glia-free cortical neuronal culture system. These neurons form few synapses when cultured alone; however, astrocytes secrete proteins that strongly promote excitatory and inhibitory synapse formation in these cultures. The synaptogenic effects of astrocyte-conditioned media (ACM) treatment are robust and promote a highly reproducible increase in the colocalization of pre and postsynaptic puncta. This histological effect has been validated by numerous studies using electron microscopy and electrophysiology. Therefore, to test the utility of SynBot and compare it to Puncta Analyzer, here we used ACM treatment of purified cortical neuron cultures to induce synapse formation. For these studies, neurons and astrocytes were individually prepared from postnatal day 1 (P1) Sprague-Dawley rat pups. First, we dissected the cerebral cortex and performed enzymatic digestion with papain and DNAse inhibitor for 45 minutes at 32C. To stop the enzymatic digestion, we treated the tissue with ovomucoid, followed up by mechanical dissociation to obtain a single cell suspension (Figure 5A). This single cell suspension was then used to isolate neurons or astrocytes. As illustrated in Figure 5B, neurons were purified from the single cell suspensions first through a series of negative panning steps to remove unwanted cell types in the following manner: first using Baneiraea Simplicifolia Lectin 1, we removed macrophages, second, using an anti-mouse secondary antibody-coated plate and an anti-rat secondary antibody-coated plate we removed potential non-specific Fc-binding cells such as microglia and cell debris. After the negative selection, we then incubated the cell suspension on a positive panning petri dish coated with anti-L1CAM antibody. Neonatal cortical neurons highly express L1CAM, which is absent from other cortical cell types, such as astrocytes and oligodendrocytes. Both excitatory and inhibitory neurons of the cortex are retained on the plate after incubation, and gentle washes are used to remove unbound cells. Finally, the neurons were collected from the positive-panning plate by gentle pipetting and pelleted for resuspension in neuronal growth media and plated on glass coverslips which are coated with poly-D-lysine (PDL) and laminin at a density of 70 thousand cells per coverslip. To isolate astrocytes, we plated the cortical single-cell suspension onto a PDL-treated tissue culture flask. After 3 days in vitro (DIV3), we purified the astrocytes, which are tightly attached to the flask, by vigorously shaking so less-adherent cells are removed. To collect the ACM, we incubated the astrocytes with minimal media for 4 days to promote protein secretion. We collected the ACM and concentrated it using centrifugal concentrator tubes (Figure 5C). After protein quantification, we applied 50ug/ml or 100ug/ml ACM to the glia-free neuronal cultures at DIV8 and DIV 11 to induce excitatory and inhibitory synapse formation, respectively (Figure 5D). To label excitatory and inhibitory synapses in culture, we fixed the DIV 13 neuronal cultures and immuno-stained the synapses with pre-and postsynaptic markers. To identify excitatory synapses, we used the presynaptic active zone marker Bassoon together with the excitatory postsynaptic marker Homer1 (Figure 5E). To visualize inhibitory synapses, we used Bassoon in combination with the inhibitory postsynapse marker Gephyrin (Figure 5G). For image acquisition, we took single focal plane images using an inverted Olympus FV3000 confocal laser scanning microscope with a 60X oil immersion objective. The images were taken in a 1024X1024 resolution with a 2X digital zoom. To identify individual neurons, we used the DAPI channel, and we imaged only the neuronal cell bodies at least two cell diameters distant from other neurons. We used this strategy to avoid overlapping cells and to use an unbiased method to select the neurons to image. We imaged at least 20 cells for each condition, and the laser power for each channel was adjusted such that the signal for each channel was bright without many saturated pixels. To determine the efficiency of SynBot to detect pre-and postsynaptic colocalization, we analyzed images from cortical neurons treated with ACM or cultured in growth media only (named here as control) using manual, ilastik, and SynQuant thresholding methods of SynBot. We compared these results to analyses done by its predecessor Puncta Analyzer (Figure 5E–H and S2). To aid these comparisons, we normalized the number of excitatory synapses between the ACM-treated neurons to the control values. As expected, ACM induced a significant increase in synapse numbers (~1.5-fold) with each of the analysis methods used (student’s t test p = 0.005, p = 0.004, p = 0.010, and p = 0.0150 for Puncta Analyzer, SynBot Manual, SynBot ilastik, and SynBot SynQuant respectively) (Figure 5F). Similarly, we found a 2-fold increase in the number of inhibitory synapses when neurons were treated with ACM compared to the control (student’s t-test p < 0.001 for each analysis method) (Figure 5H). These results indicate that SynBot is equally efficient in detecting both types of synaptic contacts as the Puncta Analyzer. Moreover, these results show that the ilastik and SynQuant automated thresholding methods can be used to determine changes in synapse numbers. To determine whether manual or automated thresholding affected the numbers of puncta identified, we also compared the raw numbers of green (presynaptic marker), red (postsynaptic marker), and colocalizations identified between the Puncta Analyzer, SynBot Manual, SynBot ilastik, and SynBot SynQuant. When compared to the other methods, SynBot ilastik consistently identified more green, red, and colocalized puncta from the images, regardless of the antibodies used or the treatment conditions (Figure S1). This result is consistent with ilastik’s tendency to detect more individual objects (here puncta) based on 37 different image features. In contrast, manual thresholding only uses intensity cutoffs and minimal puncta size as features for detection. Two-way ANOVA tests found significant differences in the raw number of colocalized synapses detected by each of the analysis methods for the in vitro excitatory and inhibitory datasets shown in Figure 5. When the number of colocalized puncta detected was normalized to the appropriate control conditions; however, there were no significant differences between the 4 thresholding methods. Together, these results show that SynBot, with its user-friendly and timesaving features, efficiently detects excitatory and inhibitory synaptic contacts in vitro. Quantification of in vivo excitatory synapse numbers using SynBot After validating the use of SynBot in vitro, we next tested its utility in quantifying synapse numbers from mouse brain tissue sections. Immunohistochemistry of brain tissue sections is a common method for synapse number quantification. However, these analyses have the considerable added complexity of a 3-dimensional tissue section that should be imaged. Moreover, thick tissue sections from the brain inherently have more background signals, which can impair the accuracy of the analyses. Finally, antibody penetration into the tissue sections is an important consideration while optimizing synapse staining and imaging procedures. Before using SynBot or any other method, the experimenters should optimize their staining and imaging procedures. The optimized synapse staining and imaging procedure we have used can be found in the methods and protocols.io (https://www.protocols.io/view/synbot-protocols-3byl4qewjvo5/v2). To test SynBot’s efficacy and accuracy in quantifying synapse numbers in vivo, we next analyzed images that were previously acquired and reported in a study by Risher and colleagues in 2018. This paper showed that the neuronal Thrombospondin/Gabapentin receptor α2δ−1 is crucial for proper excitatory synapse formation in the developing mouse cortex. Risher et al., 2018 showed that in α2δ−1 KO animals, there is a significant decrease in VGluT1/PSD95 synapse numbers compared to WT controls. Risher et al. validated these histological findings of impaired synaptogenesis using electrophysiology and electron microscopy that also showed a strong decrease in excitatory synapse function and number, respectively. To perform these experiments, mice were transcardially perfused with 4% paraformaldehyde, and brains were collected, frozen, and cryosectioned (Figure 6A). Intracortical excitatory synapses were marked with the pre-and post-synaptic markers specific for these connections, namely vesicular glutamate transporter 1 (VGluT1) and postsynaptic density 95 (PSD95) (Figure 6B). Images were acquired using a Leica SP5 confocal microscope. A z-stack of 15 optical planes was acquired using a 0.34 um z-step to image ~ 5 μm of the sample. Images were acquired such that confocal settings were optimized on control conditions to minimize oversaturated pixels. For detailed procedures, see the methods section and protocols.io (https://www.protocols.io/view/synbot-protocols-3byl4qewjvo5/v2). We used this well-validated data set to test SynBot’s efficacy for quantifying synapse numbers in mouse brain tissue images. We reanalyzed the P21 α2δ−1 WT and KO images previously published in Risher et al., 2018 using the SynBot manual, ilastik, and SynQuant thresholding methods and compared these to the results Risher et al. obtained using Puncta Analyzer (Figure 6C). All 4 methods found a significant ~50% decrease in the number of synapses in α2δ−1 KO animals when compared to WT littermates (linear mixed effects model, p < 0.001 for each method). In our hands, a pre-trained ilastik model implemented in SynBot or SynQuant automated thresholding was able to perform as well as a highly trained manual user (Figures 6C and S1). As we saw with the in vitro datasets, there were significant differences between the raw numbers of colocalized puncta detected by the 4 thresholding methods but not after normalization to the WT control data (two-way ANOVA with p-value cutoff of 0.05). Quantification using SynBot manual, ilastik, or SynQuant thresholding recapitulated the previously validated finding that α2δ−1 KO mice have fewer intracortical synapses than WT littermates, which was also previously confirmed by EM and electrophysiology. This demonstrates that SynBot can be successfully applied to brain tissue images and that the thresholding methods packaged in SynBot can be used to compare different strategies quickly. Altogether SynBot is a fast, reliable, user-friendly, and versatile software to study synapse density in vitro and in vivo, overcoming previous constraints of Puncta Analyzer.","Quantification of synapse numbers using immunohistochemistry is a favored technique for its ability to label synapses quickly and easily. However, this rapid experimental protocol is often paired with slow analysis methods that are difficult to learn and result in variability between users. SynBot overcomes many of the shortcomings of its predecessor by being easy to learn with built-in and online instructions and easy to use with a user interface that simplifies and streamlines user input. SynBot also allows unsupervised analyses through its automated methods. Importantly, SynBot collects all the data extracted from synaptic images by recording the position and area of each individual punctum from each image channel. These data can be used for other forms of analyses, such as the count and sizes of individual synaptic puncta or the spatial relationship in their distribution across the tissue imaged. One of the most challenging aspects of analyzing these data is the subjective nature of the manual thresholding used in the Puncta Analyzer. The 9 thresholding modes available in SynBot address this by providing the user with simple approaches like a fixed threshold value, streamlined manual thresholding as well as the complex machine learning-based algorithm of ilastik and probability-based algorithm of SynQuant. We have found ilastik to be particularly suited to this workflow as a short training session in ilastik is sufficient to analyze images accurately and reproducibly without the need for further user input. The details about how to train ilastik for synaptic puncta detection can be found in the methods section and protocols.io. When used in the manual mode, SynBot also automatically saves the threshold values applied to each image with the output data, allowing for rapid replication of the analysis by other users. The advantage of implementing SynBot in the ImageJ macro language is the ease with which it can be changed by users to apply to a wider variety of experimental questions. All the source code for the macro is available on our lab GitHub site (https://github.com/Eroglu-Lab/Syn_Bot) and can be edited within FIJI itself without the need for software development programs or in-depth computer science experience. There were only a few parts of SynBot that could not be implemented in ImageJ macro language (circular-approximation colocalization, some dialog menus, ilastik and SynQuant integration) and required Java programming. These sections are stored in the separate GitHub repositories ilastik4ij_Syn_Bot (https://github.com/Eroglu-Lab/ilastik4ij_Syn_Bot) and (https://github.com/freemanwyz/SynQuantSimple), which are freely available for download and can be edited by users with experience in Java programming. Another benefit of SynBot over previous analysis programs is its speed. When analyzing the 100 simulated images shown in Figure 4, for example, SynBot analysis took 24 minutes for manual thresholding, 10 minutes for training and 50 minutes for ilastik thresholding, and 4 minutes with SynQuant thresholding. Unlike manual thresholding, the time spent while ilastik or SynQuant run requires no user input. The speed of SynQuant makes it well-suited for running multiple times using different parameters, making it easy to troubleshoot and optimize accuracy for a given data set. With all of these features, researchers will be able to rapidly screen experimental conditions that alter synapse numbers and can move on to in-depth structural and mechanistic investigations with other methods such as electrophysiology, super-resolution or electron microscopy. Therefore, SynBot will aid in answering many outstanding questions related to synapse development and maintenance. Limitations of the Study There are a few key conditions necessary for SynBot to be successfully applied. Firstly, the signals being analyzed need to be punctate rather than diffuse since SynBot performs object-based colocalizations and must be able to identify discrete objects. Similarly, objects within the images must be non-overlapping to be independently counted. This object-based colocalization is most appropriate when the position of synaptic compartments is changing to alter the number of structural synapses (seen as colocalizations). If the abundance of a synaptic marker or its association with synapses is changing, intensity measurements or pixel-wise colocalization on unthresholded raw images may be more appropriate (see https://imagej.net/imaging/colocalization-analysis for further discussion of different colocalization methods). SynBot uses synaptic puncta position over raw intensity. Thus, we can use thresholding methods such as ilastik and SynQuant that apply different values to each image to separate the foreground from the background. However, if needed SynBot can also apply the “Fixed Value” thresholding, which allows users to apply the same exact threshold value to all images. An important limitation of this fixed thresholding method is that it would only be useful for datasets with images containing minimal background noise, such as in vitro synapse staining. While theoretically possible, in practice staining intensities and background may vary even within a single experiment enough to impair the use of fixed values. We strongly recommend using methods like ilastik or SynQuant instead because they have the flexibility to adjust to the varying image brightness. In particular SynQuant thresholds images using a statistical method that provides consistency between different images. Thus, these automated methods make these analyses much more accessible for untrained new users than manual thresholding. SynBot relies on RGB image conversion to keep track of the different image color channels. This limits SynBot analysis to a maximum of 3 colors and converts each image channel to an 8-bit format. This reduction in bit-depth from the 16-bit images acquired by many modern microscopes reduces the possible thresholding values from 65,536 in a 16-bit image to 256 in an 8-bit image. For the synapse analysis experiments shown here, 256 possible values is adequate for thresholding, but certain analyses may benefit from dividing the image intensity range into a greater number of bins. It should be noted that SynBot has no limitations for image resolution (the number of pixels representing a given area of space) and that this bit-depth conversion has no impact on resolution. SynBot only works on single optical sections or Z-stack projections and is unable to fully incorporate 3-dimensional synapse structures, as is done in other software such as Imaris. Despite these caveats, SynBot is widely applicable to any object-based colocalization analysis for synapses or any other set of imaging markers. SynBot can tailor its analysis to many experimental questions. However, for neurite tracing synapse analysis or 3D constructions alternative methods should be used. Similarly, if the user needs to analyze multiple regions from an image, they should perform repetitive analysis to follow that experimental design. It is also important to note that the accuracy of synapse number analyses is heavily dependent on the quality of staining and imaging. Moreover, proper markers should be used to label synapses. Some common mistakes are to use an axonal or dendritic marker, such as GAD65 or GAD67 (for inhibitory axons) or MAP2 (for neuronal dendrites), as one of the synaptic compartments. These kinds of markers are often extrasynaptically localized and widely distributed, thus yield co-localization with other markers even if they were not at an actual synapse. Similarly, SynBot requires careful troubleshooting, even when using fully-automated thresholding methods. Users should always visually inspect the counted synapses using the “colocs” output images and verify the thresholding performance by viewing the “red_thresholded” and “green_thresholded” output images. It is also advisable to include control conditions whenever possible, such as perturbations known to change synapse numbers. As with any image analysis method, improper use can lead to misleading results.",10.1101/2023.06.26.546578
PMC10312815,37396602,The Dynamic Sensorium competition for predicting large-scale mouse visual cortex activity from videos,"Understanding how biological visual systems process information is challenging due to the complex nonlinear relationship between neuronal responses and high-dimensional visual input. Artificial neural networks have already improved our understanding of this system by allowing computational neuroscientists to create predictive models and bridge biological and machine vision. During the Sensorium 2022 competition, we introduced benchmarks for vision models with static input (i.e. images). However, animals operate and excel in dynamic environments, making it crucial to study and understand how the brain functions under these conditions. Moreover, many biological theories, such as predictive coding, suggest that previous input is crucial for current input processing. Currently, there is no standardized benchmark to identify state-of-the-art dynamic models of the mouse visual system. To address this gap, we propose the Sensorium 2023 Benchmark Competition with dynamic input (https://www.sensorium-competition.net/). This competition includes the collection of a new large-scale dataset from the primary visual cortex of five mice, containing responses from over 38,000 neurons to over 2 hours of dynamic stimuli per neuron. Participants in the main benchmark track will compete to identify the best predictive models of neuronal responses for dynamic input (i.e. video). We will also host a bonus track in which submission performance will be evaluated on out-of-domain input, using withheld neuronal responses to dynamic input stimuli whose statistics differ from the training set. Both tracks will offer behavioral data along with video stimuli. As before, we will provide code, tutorials, and strong pre-trained baseline models to encourage participation. We hope this competition will continue to strengthen the accompanying Sensorium benchmarks collection as a standard tool to measure progress in large-scale neural system identification models of the entire mouse visual hierarchy and beyond.","Introduction Understanding how the visual system processes visual information has been a longstanding goal of neuroscience. Neural system identification, the development of accurate predictive models of neural population activity in response to arbitrary input, is a powerful approach to develop our understanding on a quantitative, testable, and reproducible basis. Systems neuroscience has used a variety of modeling approaches to study the visual cortex in the past, including linear-nonlinear (LN) models, energy models, subunit models, Bayesian models, redundancy reduction models, and predictive coding models. Deep learning has significantly advanced the performance of predictive models, particularly with the introduction of convolutional neural networks (CNNs) trained on image recognition tasks or trained end-to-end on predicting neural responses. More recently, transformer-based architectures have also shown strong performance in predicting neural responses. In some cases, predictive models may be engineered with specific constraints in order to draw insight from interpretable internal parameters. On the other hand, even ""black-box"" models can still provide important scientific utility. For example, high-performing, data-driven models allow unbiased exploration of large stimulus spaces in silico that would otherwise be prohibitively costly with biological experiments, yielding novel insights about the visual system that are evaluated by selective verification by systems neuroscientists in vivo. Additionally, another research focus could be to develop models that generalize well from the training domain (e.g. natural movies) to novel out-of-domain stimuli. Such models can also dramatically extend the variety of questions that can be asked of the same dataset by characterizing classical vision tuning properties (e.g. orientation tuning and receptive field location) or novel hypothesis-driven tuning that may be costly or impossible to characterize in vivo. Thus, improving predictive performance of these models opens up new avenues for important neuroscientific inquiry. Standardized large-scale benchmarks are one important approach to steadily accumulate improvements in predictive models, through constructive competition between models compared on equal ground. Several neuroscience benchmarks already exist, including Brain-Score, Neural Latents '21, Algonauts and Sensorium 2022. There are also several recent large datasets that have been released as high-throughput recording methodologies become more available, including the MICrONS calcium imaging dataset (MICrONS) and calcium imaging and Neuropixel datasets from the Allen Brain Observatory However, these large public datasets typically lack the private test set and benchmark infrastructure for third party evaluation of performance metrics on withheld test data. Importantly, the majority of the above models, competitions, and datasets focus on predicting responses to static stimuli, typically with relatively long presentation times (i.e. hundreds of milliseconds). While this approach has yielded important insights into the spatial preferences of neural populations, understanding how visual neurons process spatiotemporal information is crucial, because real-life visual stimuli are dynamic. Animals need to be able to accurately and quickly detect and respond to external elements in their environment (e.g. when tracking prey or avoiding a predator), as well as correctly estimate their own motion. Thus, further developing and assessing the performance of models designed for neural predictions over time is important. However, the field currently lacks a large-scale benchmark for models predicting single-cell responses to dynamic (movie) stimuli. To address this gap, we propose the sensorium 2023 competition, aimed at fostering the development of more accurate predictive dynamic models of the mouse visual cortex. These predictive dynamic models take as input video stimuli and/or behavioral variables, and as output predict video-rate responses of single neurons (Fig. 1). We designed and collected a large-scale dataset for this competition, including five scans from the primary visual cortex of five mice. In total, the dataset contains responses from 38,819 neurons to a diverse set of videos from various domains, along with behavioral measurements (Fig. 2). The main track will focus on predicting neuronal activity in response to natural videos, with participants encouraged to use behavioral data to enhance their predictions. To test how well the models generalize, a bonus track will evaluate model performance on five out-of-domain stimuli not included in the training set, including parametric stimuli that have been used to characterize classical visual tuning properties. We also provide a starting kit to lower the barrier for entry, with tutorials, code for training baseline models, and APIs for data loading and submission. This competition is part of an ongoing series of sensorium competitions for benchmarking predictive models of neuronal responses with the hope that it facilitates our understanding of the computations carried out by visual sensory neurons. sensorium competition overview The goal of the sensorium 2023 competition is to identify accurate predictive dynamic models of mouse visual cortex. Participants are provided with training data in the form of videos that were shown to the mouse, and the resulting recorded neuronal responses and behavioral variables, all of which were recorded for this purpose and will be made public for the first time at the start of the competition. Participants are then tasked with creating models that predict a test set of withheld neuronal responses from the corresponding video stimuli and behavioral variables. Submissions to the main track are evaluated on a test set of natural video stimuli of the same type present in the training set (i.e. in-domain performance). Submissions to the bonus track are evaluated on a test set of stimulus types not present in the training set (i.e., out-of-domain performance), including static natural images, random dot kinematograms, drifting gabors, gaussian dots, and directional pink noise, as in. The test set trials are divided into two exclusive groups: live and final test. Performance metrics computed on the live test trials will be used to maintain a public leaderboard throughout the submission period, while the performance metrics on the final test trials will be used to identify the winning entries, and will only be revealed after the submission period has ended (Fig. 2d). By separating the live test and final test set performance metrics, we are able to provide feedback from the live test set to participants wishing to submit updated predictions over the course of the competition (up to one submission per day), while avoiding overfitting for the final test set over multiple submissions. In both cases, the withheld competition test set responses will not be (and have never been) publicly released. To make the competition accessible for both computational neuroscientists and machine learning practitioners, we will release a starting kit that contains the complete code to fit our baseline models as well as explore the full dataset.2 Data We recorded data with the goal of comparing models that predict neuronal activity in response to dynamic movies. We also include behavioral variables in our dataset as a common proxy of modulatory effects of neuronal responses. Thus, in generic terms, neural predictive models capture neural responses  of  neurons for  timepoints as a function  of both natural movie stimuli , where  and  are video width and height, and behavioral variables , where  is the types of behavior (, see below). In the following paragraphs, we provide a short description of each one of these quantities. Movie stimuli. We sampled natural dynamic stimuli from cinematic movies and the Sports-1M dataset, as described in (MICrONS). Five additional out of domain (OOD) stimulus types, including natural images from ImageNet, flashing Gaussian dots, random dot kinematograms, directional pink noise (MICrONS), and drifting Gabors were also included in the stimulus, in line with earlier work. Stimuli were converted to grayscale and presented to mice in ~ 8–11 second clips at 30 Hz (Fig. 2b). Neuronal responses. Using a wide-field two-photon microscope, we recorded the responses of excitatory neurons at 8 Hz in layers 2–5 of the right primary visual cortex in awake, head-fixed, behaving mice using calcium imaging. Neuronal activity was extracted as described previously and resampled at 30 Hz to be at the same frame rate as the visual stimuli (Fig. 2a). We will also release the anatomical coordinates of the recorded neurons. Behavioral variables. We provide measurements of four behavioral variables: locomotion speed, which is recorded from a cylindrical treadmill at 100 Hz and resampled to 30 Hz, and pupil size, horizontal and vertical pupil center position, which are extracted from tracked eye camera video at 20 Hz and resampled to 30 Hz. Dataset. Our complete corpus of data comprises five recordings in five animals, which in total contain the neuronal activity of 38,819 neurons to a total of ~ 600 minutes of dynamic stimuli over the dataset, with ~ 120 minutes per recording (Fig. 2c). None of the five recordings have been published before, and are released on the first day of the competition explicitly for this purpose. Each recording has 4 components (Fig. 2c). Training set: 60 minutes of natural movies, one repeat each (60 minutes total). Validation set: 1 minute of natural movies, ten repeats each (10 minutes total). Live test set: 1 minute of natural movies and 1 minute of OOD stimuli, ten repeats each (20 minutes total). Each OOD stimulus type is represented once in the live test set across the five recordings. Final test set: 1 minute of natural movies and 2 minutes of OOD stimuli, ten repeats each (30 minutes total). Each OOD stimulus type is represented twice in the final test set across the five recordings. For the training set and validation set, the stimulus frames, neuronal responses, and behavioral variables are released for model training and evaluation by the participants, and are not included in the competition performance metrics. Please note that train and validation sets only contain natural movies and not the OOD stimuli.","Neurophysiological experiments. All procedures were approved by the Institutional Animal Care and Use Committee of Baylor College of Medicine. Five mice (Mus musculus, 2 females, 3 males, P78–131 on day of first scan) expressing GCaMP6s in excitatory neurons via Slc17a7-Cre and Ai162 transgenic lines (recommended and generously shared by Hongkui Zeng at Allen Institute for Brain Science; JAX stock 023527 and 031562, respectively) were anesthetized and a 4 mm craniotomy was made over the visual cortex of the right hemisphere as described previously. Mice were head-mounted above a cylindrical treadmill and calcium imaging was performed using Chameleon Ti-Sapphire laser (Coherent) tuned to 920 nm and a large field of view mesoscope equipped with a custom objective (excitation NA 0.6, collection NA 1.0, 21 mm focal length). Laser power after the objective was increased exponentially as a function of depth from the surface according to:  Here P is the laser power used at target depth z, P0 is the power used at the surface (not exceeding 21 mW), and  is the depth constant (220 μm). The greatest laser output of 90.86 mW was used at approximately  from the surface, with most scans not requiring more than 70 mW at similar depths. The craniotomy window was leveled with regards to the objective with six degrees of freedom. Pixel-wise responses from an ROI spanning the cortical window (3600 × 4000 μm, 0.2px/μm, approx. 200 μm from surface, 2.47 Hz) to drifting bar stimuli were used to generate a sign map for delineating visual areas. Area boundaries on the sign map were manually annotated. Our target imaging site was a 630 × 630 μm ROI within the boundaries of primary visual cortex (VISp, Supp. Fig. 1). The released scans contained 10 planes, with 25 μm interplane distance in depth, and were collected at 7.98 Hz. Each plane is 630 × 630 μm (252 × 252 pixels, 0.4px/μm). The most superficial plane in each volume was approximately 200 μm from the surface. This 25 μm sampling in z was designed to reduce the number of redundant masks arising from multiple adjacent planes intersecting with the footprint of a single neuron. Movie of the animal's eye and face was captured throughout the experiment. A hot mirror (Thorlabs FM02) positioned between the animal's left eye and the stimulus monitor was used to reflect an IR image onto a camera (Genie Nano C1920M, Teledyne Dalsa) without obscuring the visual stimulus. The position of the mirror and camera were manually calibrated per session and focused on the pupil. Field of view was manually cropped for each session to contain the left eye in its entirety, ranging from 214–284 pixels height × 250–331 pixels width at ca. 20 Hz. Frame times were time stamped in the behavioral clock for alignment to the stimulus and scan frame times. Video was compressed using Labview's MJPEG codec with quality constant of 600 and stored in an AVI file. Light diffusing from the laser during scanning through the pupil was used to capture pupil diameter and eye movements. A DeepLabCut model was trained as previously described on 17 manually labeled samples from 11 animals to label each frame of the compressed eye video (intraframe only H.264 compression, CRF:17) with 8 eyelid points and 8 pupil points at cardinal and intercardinal positions. Pupil points with likelihood >0.9 (all 8 in 87–97% of frames) were fit with the smallest enclosing circle, and the radius and center of this circle was extracted. Frames with < 3 pupil points with likelihood>0.9 (<0.1% frames per scan), or producing a circle fit with outlier > 5.5 standard deviations from the mean in any of the three parameters (center x, center y, radius, <0.3% frames per scan) were discarded (total <0.3% frames per scan). Gaps in behavior were replaced by linear interpolations over the whole session, if there were more than 2 frames with gaps, then the video is removed. (We removed less then 2% of the videos, 70 out of 3640, where one video was rejected due to signal synchronization issues during resampling). The mouse was head-restrained during imaging but could walk on a treadmill. Rostro-caudal treadmill movement was measured using a rotary optical encoder (Accu-Coder 15T-01SF-2000NV1ROC-F03-S1) with a resolution of 8000 pulses per revolution, and was recorded at approx. 100.2, Hz in order to extract locomotion velocity. Visual stimulation. Visual stimuli were presented with Psychtoolbox 3 in MATLAB to the left eye with a 31.8 × 56.5 cm (height × width) monitor (ASUS PB258Q) with a resolution of 1080×1920 pixels positioned 15 cm away from the eye. When the monitor is centered on and perpendicular to the surface of the eye at the closest point, this corresponds to a visual angle of 3.8 °/cm at the nearest point and 0.7 °/cm at the most remote corner of the monitor. As the craniotomy coverslip placement during surgery and the resulting mouse positioning relative to the objective is optimized for imaging quality and stability, uncontrolled variance in animal skull position relative to the washer used for head-mounting was compensated with tailored monitor positioning on a six dimensional monitor arm. The pitch of the monitor was kept in the vertical position for all animals, while the roll was visually matched to the roll of the animal's head beneath the headbar by the experimenter. In order to optimize the translational monitor position for centered visual cortex stimulation with respect to the imaging field of view, we used a dot stimulus with a bright background (maximum pixel intensity) and a single dark square dot (minimum pixel intensity). Dot locations were randomly ordered from a 10 × 10 grid tiling a central square (approx. 90° width and height) with 10 repetitions of 200 ms presentation at each location. The final monitor position for each animal was chosen in order to center the population receptive field of the scan field ROI on the monitor, with the yaw of the monitor visually matched to be perpendicular to and 15 cm from the nearest surface of the eye at that position. Natural Movies: Natural movies from the ""cinematic"" and ""Sports-1M” classes were drawn from the library described in (MICrONS). Each scan contained 360 movies shown one time and 18 movies shown ten times, in both cases drawn equally from the cinematic and Sports-1M classes. Each movie was unique to its respective scan. Spatiotemporal Gabors: Spatiotemporal gabor movies were presented as described in, but with different parameters as described below. For three scans containing spatiotemporal gabors, 72 movies (8 directions × 3 spatial frequencies × 3 temporal frequencies) were shown ten times per scan. Gabor spatial frequencies corresponded to wavelengths of 0.05, 0.1, and 0.2 (fraction of monitor width). Gabor temporal frequencies corresponded to gabor velocities of 0.1, 0.2, and 0.3 (fraction of monitor width per second), in the direction perpendicular to the gabor orientation. Gabor spatial envelope was located in the center of the monitor, with a standard deviation of 0.08 (fraction monitor width, approx. 17 degrees). Each gabor movie was 833 ms in duration, and movies were randomly assorted into 6 sequences of 12 conditions each, for a total of 10 seconds per sequence. Because the stimulus was parametrically constructed, the same movies are shown in each of the three scans, but differ in sequence membership and order. Directional Pink Noise: Directional pink noise was generated as described in (MICrONS). For three scans with directional pink noise stimuli, six movie sequences were shown time times per scan. Each movie sequence was generated from a unique random seed, which determined the underlying pink noise pattern and also the order of 12 equally spaced directional subtrials, with a spatial orientation bias perpedicular to the direction of motion. Each directional subtrial lasted 900 ms, for a total of 10.8 seconds per sequence. Each directional pink noise movie sequence was unique to its respective scan. Random Dot Kinematogram: Random dot kinematograms (RDK) movies were presented as described in, but with different parameters as described below. For three scans containing RDK movies, 32 movies (8 flow trajectories × 2 velocities × 2 coherencies) were shown ten times per scan. RDK movie optical flow corresponded to a translational (up/down/left/right), radial (inward / outward w/r/t monitor center), or rotational (clockwise / anticlockwise w/r/t monitor center) trajectory. RDK movie dots had a velocity of either 0.3 or 0.5 (fraction monitor width / second), and coherency of either 50% or 100% with respect to the global optical flow trajectory. Each dot had a diameter of 1/32 (fraction monitor width, approx. 6.7 degrees at the nearest point) and a lifetime of 1 second. Each RDK movie was 2 seconds in duration, and movies were randomly assorted into 8 sequences of 4 movies each, for a total of 8 seconds per sequence. Each RDK movie was unique to its respective scan. Natural Images: Natural image from ImageNet were presented as in. For three scans containing natural images, 60 images were shown ten times per scan. Randomly selected images were center-cropped to 9:16 aspect ratio and converted to gray scale. Images were presented for 500 ms, preceded by a 400–600 ms blank gray screen (pixel value 127/255). Images were randomly assorted into 6 sequences of 10 images each, for approx. 10 seconds per sequence. Gaussian Dots: Gaussian dots were presented as in, but with different parameters as detailed below. For three scans containing gaussian dots, 210 dot presentations (105 positions × 2 dot intensities) were shown ten times per scan. Dot positions were drawn from a grid of 15 horizontal (−0.35 to 0.35) by 7 vertical (−0.267 to 0.267) positions, where all positions are reported as fraction of monitor width and 0 is the center of the monitor. Dots were presented as either white (pixel value 255 out of 255) or black (pixel value 0) on a gray background (pixel value 127). Dot standard deviation was 0.07 (fraction monitor width, ≈ 15° at the closest point). Dot presentations were 300 ms in duration, and were randomly assorted into 6 sequences of 35 dots each, for a total of 10.5 seconds per sequence. Because the stimulus was parametrically constructed, the same dots are shown in each of the three scans, but differ in sequence membership and order. A photodiode (TAOS TSL253) was sealed to the top left corner of the monitor, and the voltage was recorded at 10 kHz and timestamped on the behavior clock (MasterClock PCle-OSC-HSO-2 card). Simultaneous measurement with a luminance meter (LS-100 Konica Minolta) perpendicular to and targeting the center of the monitor was used to generate a lookup table for linear interpolation between photodiode voltage and monitor luminance in cd/m2 for 16 equidistant values from 0–255, and one baseline value with the monitor unpowered. At the beginning of each experimental session, we collected photodiode voltage for 52 full-screen pixel values from 0 to 255 for one second trials. The mean photodiode voltage for each trial  was fit as a function of the pixel intensity :  in order to estimate the  value of the monitor (≈ 1.60–1.74). All stimuli were shown with no  correction. During the stimulus presentation, sequence information was encoded in a 3 level signal according to the binary encoding of the flip number assigned in-order. This signal underwent a sine convolution, allowing for local peak detection to recover the binary signal. The encoded binary signal was reconstructed for >99% of the flips. A linear fit was applied to the trial timestamps in the behavioral and stimulus clocks, and the offset of that fit was applied to the data to align the two clocks, allowing linear interpolation between them. The mean photodiode voltage of the sequence encoding signal at pixel values 0 and 255 was used to estimate the luminance range of the monitor during the stimulus, with minimum values between 0.001 and 0.65 cd/m2 and maximum values between 8.9 and 11.3 cd/m2 in the released scans. Preprocessing of neural responses and behavioral data. The full two photon imaging processing pipeline is available at (https://github.com/cajal/pipeline). Raster correction for bidirectional scanning phase row misalignment was performed by iterative greedy search at increasing resolution for the raster phase resulting in the maximum cross-correlation between odd and even rows. Motion correction for global tissue movement was performed by shifting each frame in X and Y to maximize the correlation between the cross-power spectra of a single scan frame and a template image, generated from the Gaussian-smoothed average of the Anscombe transform from the middle 2000 frames of the scan. Neurons were automatically segmented using constrained non-negative matrix factorization, then detrended and deconvolved to extract estimates of spiking activity, within the CAIMAN pipeline. Cells were further selected by a classifier trained to separate somata versus artifacts based on segmented cell masks, resulting in exclusion of 9.1% of masks. Functional and behavioral signals were resampled to 30 Hz by linear spline interpolation. The mirror motor coordinates of the centroid of each mask was used to assign anatomical coordinates relative to each other and the experimenter's estimate of the pial surface. Notably, centroid positional coordinates do not carry information about position relative to the area boundaries, or relative to neurons in other scans. Representation/Core We based our work on the models of, which are able to predict the responses of a large population of mouse V1 neurons with high accuracy. For the GRU baseline, we used rotation-equivariant core from with 8 rotations, 8 channels, and 4 layers. The spatial kernels were 9 × 9, followed by 7 × 7. The GRU module, inspired by, was after the core. It had 64 channels (8 channels × 8 rotations = 64), and both input and recurrent kernels were 9 × 9. For the 3D Factorized baseline, we used the core inspired by with 4 layers (16, 32, 64, and 128 channels per layer, resp.). The spatial kernels were 11 × 11 in the 1 st layer and 5 × 5 in all of the subsequent layers. Similarly, the temporal kernels were 11 × 1 in the 1 st layer and 5 × 1 afterwards. The Ensembled baseline cores were same as for the 3D Factorized baseline. Readout To get the scalar neuronal firing rate for each neuron, we computed a linear regression between the core output tensor of dimensions  (width, height, channels) and the linear weight tensor , followed by an ELU offset by one (ELU+1), to keep the response positive. We made use of the recently proposed Gaussian readout, which simplifies the regression problem considerably. The Gaussian readout learns the parameters of a 2D Gaussian distribution . The mean  in the readout feature space thus represents the center of a neuron's receptive field in image space, whereas  refers to the uncertainty of the receptive field position. During training, a location of height and width in the core output tensor in each training step is sampled, for every image and neuron. Given a large enough initial  to ensure gradient flow, the uncertainty about the readout location  is decreasing during training, showing that the estimates of the mean location  becomes more and more reliable. At inference time (i.e. when evaluating our model), we set the readout to be deterministic and to use the fixed position . In parallel to learning the position, we learned the weights of the weight tensor of the linear regression of size  per neuron. To learn the positions , we made use of the retinotopic organization of V1 by coupling the recorded cortical 2d-coordinates  of each neuron with the estimation of the receptive field position  of the readout. We achieved this by learning the common function , a randomly initialized linear fully connected MLP of size 2–30-2, shared by all neurons. Shifter network We employed a free viewing paradigm when presenting the visual stimuli to the head-fixed mice. Thus, the RF positions of the neurons with respect to the presented images had considerable trial-to-trial variability following any eye movements. We informed our model of the trial dependent shift of neuronal receptive fields due to eye movement by shifting , the model neuron's receptive field center, using the estimated eye position (see section Neurophysiological experiments above for details of estimating the pupil center). We passed the estimated pupil center through an MLP (the shifter network), a three layer fully connected network with  hidden features, followed by a tanh nonlinearity, that calculates the shift in Δx and Δy of the neurons receptive field in each trial. We then added this shift to the  of each neuron. Input of behavioral parameters During each presentation of a video, the pupil size and the running speed of the mouse was recorded. We do not have instantaneous pupil dilation change as the target (video) frequency rate is more then the pupil camera sampling frequency. We have used these behavioral parameters to improve the model's predictivity. Because these behavioral parameters have nonlinear modulatory effects, we decided to append them as separate frames to the input images as new channels, such that each new channel simply consisted of the scalar for the respective behavioral parameter recorded in a particular trial, transformed into stimulus dimension. This enabled the model to predict neural responses as a function of both visual input and behavior. Model training. Both train and validation sets contain only unique videos. We isotropically downsampled all videos to a resolution of 36 × 64px  per frame. Furthermore, we normalized input videos as well as standardized behavioral traces and the target neuronal activities, using the statistics of the training trials of each recording. After this we subsampled 150 subsequent frames randomly from each video and trained our network using the batch size = 8. A gradient update was performed after 5 batches, 1 per mouse. Then, we trained our networks with the training set by minimizing the Poisson loss , where  denotes the number of neurons,  the predicted neuronal response and  the observed response. For Poisson loss each frame was treated independently, and no time component was included. After each epoch, i.e. full pass through the training set, we calculated the correlation between predicted and measured neuronal responses on the validation set and averaged it across all neurons. If the correlation failed to increase for five consecutive epochs, we stopped the training and restored the model to its state after the best performing epoch. Then, we either decreased the learning rate by a factor of 0.3 or stopped training altogether, if the number of learning-rate decay steps was reached (n=4 decay steps). We optimized the network's parameters using the Adam optimizer. All parameters and hyper-parameters regarding model architecture and training procedure can be found in our sensorium repository (see Code Availability). Metrics. We chose correlation to evaluate the models performance. Since correlation is invariant to shift and scale of the predictions, it does not reward a correct prediction of the absolute value of the neural response but rather the neuron's relative response changes. It is bound to [−1, 1] and thus easily interpretable. However, without accounting for the unexplainable noise in neural responses, the upper bound of 1 cannot be reached, which can be misleading. Single Trial Correlation To evaluate model performance on variation between individual trials, we will compute correlation  between predicted single-trial activity  and single-trial neuronal responses , as  where  is the -th frame of -th video repeat,  is the corresponding prediction,  is the average response to all the videos in the test subset across all repeats, and  is the average prediction for all the videos in the test subset across all repeats.  is computed independently per neuron and then averaged across all neurons to produce the final metric. Correlation to Average We calculate the correlation to average  in a similar way to the single-trial correlation, but we first average the responses and predictions per frame across all video repeats before computing.  where  is a response averaged over stimulus repeats for a fixed neuron.  http://sensorium-competition.net/   https://github.com/ecker-lab/sensorium_2023/ ",,"Here, we introduced the sensorium 2023 competition for finding the best predictive model for neuronal responses in mouse primary visual cortex to dynamic stimuli. This competition is the second in a series, and shares much of its structure with preceding year's competition, sensorium 2022. Similar to last year, we have included a starting kit with baseline model tutorials, in order to continue supporting accessibility for both neuroscientists and machine learning experts interested in participating. We also once again collected a dedicated large-scale dataset, including an estimated 70% increase in unique neuronhours above the preceding year. Importantly, we made several major changes in this iteration, including moving from static to dynamic stimuli, adding out-of-domain performance in the bonus track, and including behavior in both tracks. These changes pose new technical challenges and broaden the variety of scientific questions to work on. The sensorium 2023 challenge differs from existing benchmarks in that it is the only benchmark for predicting single-cell responses to dynamic natural movie stimuli. The Brain-Score benchmark recently added a dynamic component, asking models to predict the temporal evolution of neural activity, but it still focuses on static images as stimuli. In addition, its scientific goal is not to benchmark predictive models, but to evaluate how well task-pretrained computer vision models match the neural representations along the primate ventral stream. The Neural Latents Benchmark '21 tests models of neural population activity, but focuses on dimensionality reduction and extracting a small set of latent variables from high-dimensional neural population activity, not necessarily in response to visual stimuli. The Algonauts challenge is similar in spirit to last year's sensorium 2022 and focuses on predictive models in response to natural images or natural video, but tests models of functional magnetic resonance imaging (fMRI) in human visual cortex, as opposed to single-cell responses in mouse as in sensorium. This competition also departs from sensorium 2022 in that both tracks now include behavioral measurements as model inputs. One key issue in assessing model performance is the fact that neural responses are noisy – repeated presentation of the same stimulus does not produce identical responses. This question has been addressed by numerous authors, but no clear consensus has emerged. The usual solution is to attempt to estimate the trial-to-trial variability through the use of repeated stimulus presentations, and then estimate a noise-corrected version of the explained variance (See, for an in-depth discussion and evaluation of existing metrics as well as a proposal of an asymptoticaly unbiased estimator). However, not everything determining neural responses is under experimental control. For example, the freely varying behavioral state of the animal modulates neuronal responses, and by including behavioral variables as predictors we can increase the model predictive performance. Yet in consequence, we lose the ability to estimate the ""noise"" level, because every trial is now a unique combination of behavior and stimulus. As a result, there is no way to determine the maximum achievable performance of a model without additional assumptions, and thus existing approaches for addressing unexplainable trial-to-trial fluctuations are not applicable. For this reason we opted to use the simplest possible measure of performance: the correlation coefficient between model prediction and observed response on a single-trial basis. While this metric serves our primary purpose of comparing models, it lacks the desirable property of assigning a perfect model a correlation of 1. Whether and how it is possible to obtain performance estimates with non-vacuous upper bounds once behavioral variables are included as model predictors is an open research question for future work. We plan to continue running the family of sensorium competitions with regular dataset releases and challenges, which will persist as benchmarks once the competition has ended. Our hope is these competitions and datasets are not only a technical resource, but also a basis for community formation around developing and testing models. We expect that encouraging discussion around predictive modeling between machine learning practitioners and computational neuroscientists will create opportunities to exchange ideas and benefit from each other's expertise.",
PMC10635153,37961488,lociPARSE: a locality-aware invariant point attention model for scoring RNA 3D structures,"A scoring function that can reliably assess the accuracy of a 3D RNA structural model in the absence of experimental structure is not only important for model evaluation and selection but also useful for scoring-guided conformational sampling. However, high-fidelity RNA scoring has proven to be difficult using conventional knowledge-based statistical potentials and currently-available machine learning-based approaches. Here we present lociPARSE, a locality-aware invariant point attention architecture for scoring RNA 3D structures. Unlike existing machine learning methods that estimate superposition-based root mean square deviation (RMSD), lociPARSE estimates Local Distance Difference Test (lDDT) scores capturing the accuracy of each nucleotide and its surrounding local atomic environment in a superposition-free manner, before aggregating information to predict global structural accuracy. Tested on multiple datasets including CASP15, lociPARSE significantly outperforms existing statistical potentials (rsRNASP, cgRNASP, DFIRE-RNA, and RASP) and machine learning methods (ARES and RNA3DCNN) across complementary assessment metrics. lociPARSE is freely available at https://github.com/Bhattacharya-Lab/lociPARSE.","Computational prediction of RNA 3-dimensional structures from nucleotide sequence has garnered considerable research effort over the past decade and deep learning-enabled RNA 3D modeling has gained significant attention in the recent past. To facilitate practical applicability of predicted 3D models, it is critical to have a scoring function that can reliably assess their global topology and local quality in the absence of experimental structures. Moreover, the ability of a scoring function to distinguish accurate 3D models of previously unseen RNAs from misfolded alternatives plays an important role in guiding conformation sampling towards the native state. Existing methods for scoring RNA structures roughly belong to two categories: knowledge-based statistical potentials and supervised machine learning. Various knowledge-based statistical potentials have been developed, both at all-atom and coarse-grained levels, using different simulated reference states. However, reliably distinguishing accurate structural models of RNA from less accurate ones has proven to be difficult, because the characteristics of energetically favorable RNA structures are not sufficiently well understood and thus the reference states may deviate largely from the ideal one. Machine learning-based methods aim to overcome such limitation by learning to predict the accuracy of an RNA structural model through supervised learning. Indeed, machine learning-based RNA scoring functions, trained to estimate the unfitness score either at the nucleotide level or at the structural level by learning to predict the root mean square deviation (RMSD) from the unknown true structure, have been shown to be effective in RNA-Puzzles blind structure prediction challenges. Despite the effectiveness, the existing machine learning methods do not consider some key factors that can significantly improve the sensitivity of RNA scoring functions. First, global superposition-dependent RMSD metric is not length normalized, affected by superposition, dominated by outliers in poorly modeled structural regions, and does not take into account the accuracy of local atomic environment. RNA is a flexible molecule in which irregular loops may affect RMSD measures and global superposition may not be optimal, leading to scoring anomalies. Yet, virtually all existing machine learning-based RNA scoring functions use RMSD as the ground truth during supervised training. Second, similar to other macromolecules, RNA structures have no natural canonical orientation. As such, machine learning methods that are not invariant to global Euclidean transformations such as rotation must account for this aspect of variation by tweaking model architecture and/or parameters, which may affect their expressiveness and generalizability. Third, in consideration of RNA as a flexible molecule in which interplay between various local structural motifs define the global topology, an effective scoring function should not be strongly influenced by the relative motions between the tertiary motifs. That is, the effects of relative movement between the motifs should not lead to artificially unfavorable scores. Using the Local Distance Difference Test (lDDT) as the ground truth during supervised training is an attractive alternative to the popular RMSD metric. lDDT compares distances between atoms that are nearby (within 15 Å) in the experimental structure to the distances between those atoms in the predicted structure and offers several advantages over RMSD. First, being superposition-free and based on rotation-invariant properties of a structure, lDDT naturally preserves invariance with respect to the global Euclidean transformations of the input RNA structure such as global rotations and translations. Second, lDDT measures the accuracy of local environment of the model in atomic detail, without being affected by superposition or dominated by outliers in poorly modeled structural regions. Third, lDDT exhibits robustness to movements between tertiary structural units such as domains in proteins that can generalize to RNA tertiary motifs, provided a way can be found that ensures rigid motion between a set of local structural units is invariant under global Euclidean transformations on the said units. A solution to this problem comes from Invariant Point Attention (IPA) proposed in AlphaFold2 as part of the structural module. IPA is a form of attention that acts on a set of 3D point clouds and is invariant under global Euclidean transformations on said points, where 3D point clouds are represented using local frames. How can we capture the aforementioned benefits of lDDT in a neural network architecture for RNA scoring, while maintaining invariance under global Euclidean transformations? Here, we provide such a solution by developing a new attention-based architecture, called lociPARSE (locality-aware invariant Point Attention-based RNA ScorEr), for scoring RNA 3D structures. Different from previous supervised learning approaches that estimate the RMSD metric, our method estimates local nucleotide-wise lDDT scores that are then aggregated over all nucleotides to predict global structural accuracy. Inspired by AlphaFold2, we define nucleotide-wise frames parameterized by rotation matrices and translation vectors operating on predefined RNA conformation at the local level. To model the local atomic environment of each nucleotide as captured by lDDT, the IPA implementation used in the original AlphaFold2 has been modified to incorporate locality information derived from the RNA atomic coordinates. By so doing, we are able to effectively capture the accuracy of each nucleotide while considering the effect of its local atomic environment. Our method significantly outperforms traditional knowledge-based statistical potentials as well as stateof-the-art machine learning-based RNA scoring functions such as ARES on multiple independent test datasets including CASP15 blind test targets across a wide-range of performance measures. In particular, lociPARSE exhibits superior ability to reproduce the ground truth lDDT scores both at the global and local levels, rank predictions for a given target with high fidelity, recognize the best predictions consistently, and better discriminate between ‘good’ and ‘bad’ predictions. An open-source software implementation of lociPARSE, licensed under the GNU General Public License v3, is freely available at https://github.com/Bhattacharya-Lab/lociPARSE.","Model input Our model uses only input features derived directly from nucleotide sequence and RNA 3D structural coordinates. We use just the basic nucleotide-level encodings for our input. These include one-hot encoding of the nucleotide (i.e., a binary vector of 5 entries indicating each of the 4 nucleotide types and one for non standard nucleotide) and the relative position of the nucleotide in its sequence calculated as  (where  is the nucleotide index and  is the sequence length). For our pair features, we use sequential separation of a nucleotide pair and their spatial proximity information. The sequence separation i.e., the absolute difference between the two nucleotide indices, is discretized into 5 bins and represented by one-hot encoding where the first two bins correspond to self-loops and adjacent bonds respectively. Rest of the three bins are defined based on three types of interactions depending on the sequence separation: short-range (2–5), medium-range (6–24) and long-range (>24), similar to. The other component of our pair features includes nucleotide-nucleotide atomic distances between all pair of P, C4′ and glycosidic N atoms, encoded with Gaussian radial basis functions. It is important to note that all of our nucleotide and pair features are invariant in nature, consistent with the invariant layers of the IPA module. Network architecture Construction of local nucleotide frames To perform invariant point attention on a set of 3D points, we represent each nucleotide in a geometric abstraction using the concept of frames. Each nucleotide frame in the form of a tuple is defined as an Euclidean transform , where  is a rotation matrix and  is the translation vector that can be applied to transform a position in local coordinates  to a position in global coordinates  as:  In our setting, we define local nucleotide frames from the Cartesian coordinates of P, C4′, and glycosidic  atoms of the input RNA 3D structure and construct 3-bead coordinate frame using a Gram–Schmidt process specified in Alphafold2 (Algorithm 21) that takes the input coordinates (scaled by 0.1) of  as  C4′ as , and  as . Note that the translation vector  is assigned to the centre atom . Locality-aware invariant point attention The formulation of locality-aware IPA used in our work combines sequence representation, , from each nucleotide  of the input RNA, pair representation  of nucleotide  with other nucleotides  based on nucleotide pair adjacencies capturing the local atomic environment  of nucleotide , where  is the locality information derived from the RNA atomic coordinates. Consequently, the update function of the IPA layer is as follows:  To perform attention on 3D point clouds, IPA derives query , key  and value  embeddings from a linear projection of  to a latent representation of dimension  for each nucleotide , where  and  which represents number of attention heads in the IPA module. 3D query, key and value points are also generated considering the local frame  of each nucleotide , where  and . The IPA module acts on a set of frames (parameterized as Euclidean transforms of the local frame ) and is invariant under global Euclidean transformations  on said frames. By performing locality-aware geometry and edge-biased attention, the IPA module transforms the 3D points from the target nucleotide’s local frame into a global reference frame for computing the attention weights as follows:  where,  is the attention bias derived from the linear projection of  to hidden dimension , weighting factors  and  are taken from the IPA formulation specified in AlphaFold2 and  is a learned scalar value. The attention mechanism acting on a set of local frames ensures invariance under global Euclidean transformations such as global rotations and translations of the input RNA due to the invariant nature of -norm of a vector under such rigid transformations. The attention weights are used to compute the outputs of the attention mechanism, while mapping them back to the local frame and preserving invariance, as follows:    The outputs of the attention mechanism are then concatenated and passed through a linear layer to compute the updated sequence representations  of each nucleotide as follows:  The updated sequence embeddings  for each nucleotide  are subsequently stacked together to obtain the embedding  for all nucleotides in the RNA. Finally, a linear layer followed by a 2 layer fully-connected network implemented as a multilayer perceptron (MLP) are used to obtain the final representation  before estimating nucleotide-wise IDDT scores as follows:  Training and validation datasets To curate our training datatset, we first obtained the training dataset used in trRosettaRNA containing 3,632 RNA targets. We then filtered this set by removing duplicate chains and discontinuous structures, separating monomers from complexes, splitting multiple chains into single chains, and correcting formatting issues in the coordinates files. We removed sequences with length > 200 nucleotides and ensured that our training and test sets are non-redundant by running CD-HIT-est with default parameter settings, which reduced the training set to 1,399 RNA targets. We generated a total of about 52,000 structural models for the 1,399 targets using a combination of different RNA 3D structure prediction tools including recent deep learning-enabled RNA structure prediction methods, physics-based RNA folding, and experimental structure perturbation using PyRosetta. In addition, we separately curated a validation set for ablation study and hyperparameter selection from the Protein Data Back (PDB) with experimental structures released between January 1, 2022 and July 6, 2023. Such a date range was chosen to avoid any overlap with our training dataset collected from trRosettaRNA which used structures released before January 1, 2022. Once again, we used CD-HIT-est with default parameter settings to ensure non-redundancy of the validation dataset, resulting in 60 RNA targets. We generated 3D structural models for each of these 60 RNAs using the recent deep learning-based RNA structure prediction methods. We created a reduced training subset consisting of 6,872 structural models for 1,399 RNA targets through clustering for ablation study and hyperparameter selection. Training details To train our model, lociPARSE, we obtained nuculeotide-wise ground truth lDDT scores by comparing the predicted structural models in our training dataset against the corresponding experimental structures using the docker version of OpenStructure available at https://git.scicore.unibas.ch/schwede/openstructure/-/tree/master/docker. During the ground truth lDDT computation, we enabled the option ‘–lddt-no-stereochecks’ to skip stereochemical quality checks in its calculation following the recent CASP assessment in. lociPARSE was implemented in PyTorch with  loss function to learn the mean absolute error between ground truth lDDT and predictions on nucleotide level, thereby formulating the local nuculeotide-wise quality estimation as a regression task. We trained our model using the Adam optimizer having parameters  and  with a learning rate of 0.001 and dropout rate of 0.1. The training process consists of 50 epochs on an 80-GB NVIDIA A100 GPU. Competing methods and evaluation metrics lociPARSE is compared against both traditional knowledge-based statistical potentials (rsRNASP, RASP, DFIRE-RNA, and cgRNASP) and recent machine learning-based RNA scoring functions (RNA3DCNN and ARES). rsRNASP is an all-atom distance-dependent potential considering short and long-ranged interactions present in RNA based on sequence separation aiming to capture the hierarchical nature of RNA folding. Ribonucleic Acids Statistical Potential (RASP) is another all-atom statistical potential based on the averaging reference state. Similar to rsRNASP, RASP also separates interaction pairs into local and non-local categories and takes into account the base stacking and base pairing interactions present in RNA. DFIRE-RNA is yet another distance-scaled statistical potential designed using finite-ideal-gas reference state. Finally cgRNASP, a coarse-grained counterpart of rsRNASP potential introduces three different variants of coarse-grained potentials for RNA scoring. We have used the 3-bead representation of cgRNASP in this work which takes into account P, C4′ and N atoms. The Atomic Rotationally Equivariant Scorer (ARES) is a equivariant graph neural network which scores RNA structures by identifying complex structural motifs through equivariant convolutions. ARES employs E3NN to predict the global RMSD of the structure. Finally, RNA3DCNN uses 3D convolutional neural network to predict the RMSD-like unfitness score of a nucleotide to its surroundings by considering RNA 3D structure as a 3D image and representing each nucleotide as an array of voxels. For prediction, we used the model that was trained on samples generated from both molecular dynamics (MD) and Monte Carlo (MC) simulations. It is worth noting that except lociPARSE, RNA3DCNN is the only other method that estimates both local and global quality. Our assessment metrics include global and average per-target Pearson , Spearman rank  and Kendall’s Tau rank  correlation coefficients between the estimated score and the ground truth lDDT, computed both between the predicted molecular-level lDDT (pMoL) and ground truth lDDT for the overall structure as well as between the predicted nucleotide-wise lDDT scores (pNuL) and ground truth lDDT for individual nucleotides. Meanwhile, a higher correlation indicates better performance. Diff, another assessment metric, is calculated at the global level as the mean absolute difference between pMoL and ground truth lDDT. Loss or top-1 lDDT loss is calculated as the absolute difference between the ground truth lDDT of the structural model ranked at the top by pMoL and the ground truth lDDT of the most accurate structural model for each target averaged over all targets. Lower values of diff and loss, therefore, indicate better performance. We additionally perform receiver operating characteristics (ROC) analysis using a lDDT threshold of 0.75 to separate ‘good’ and ‘bad’ structural models, following. Consequently, the area under the ROC curve (AUC) quantifies the ability of a scoring function to distinguish good and bad models. Finally, an average of all assessment metrics is taken to combine the results of all the different metrics into a single composite quality score, called , defined as:  where,  global Pearson’s ,  global diff,  per-target average Pearson’s ,  average loss, and  area under the ROC curve. We use the composite quality score for ablation study and hyperparameter selection, where higher values of  indicate better performance.","lociPARSE: locality-aware invariant point attention for RNA scoring An overview of our method, lociPARSE, is illustrated in Figure 1. The core component of our architecture, outlined in Figure 1b, is an invariant point attention (IPA) module which utilizes the geometry of the input RNA 3D structure to revise the nucleotide and pair features. This component is similar to the AlphaFold2’s IPA formulation used in the structure module, but modified herein to incorporate locality information derived from the RNA atomic coordinates. To do this, we introduce locality-aware geometry and edge-biased attention (see Section 4.2.2) based on nucleotide pair adjacencies to capture the local atomic environment of each nucleotide considering the Euclidean distances of the C4′ - C4′ atoms between nucleotide pairs. In our setting, we define local nucleotide frames (see Section 4.2.1) from the Cartesian coordinates of C4′, P, and glycosidic N atoms. The IPA partitions the nucleotide query and value features into 3D vectors and transforms them from the target nucleotide’s local frame into a global reference frame before computing both attention weights and the output of the attention mechanism. Further, we augment nucleotide-nucleotide atomic distances between all pair of 3 atoms P, C4′ and N, encoded with Gaussian radial basis functions as pair features (Section 4.1), and make further use of the pair features to bias attention weights and update scalar features. Our network architecture consists of 4 IPA layers, with the IPA hyperparameters  set to (4, 128, 8, 4) and we use 20 nearest neighbors for the locality computation, determined through ablation experiments using an independent validation set (Section 2.6). The output of the attention layer is invariant to the global Euclidean transformations such as global rotations and translations of the input RNA. Finally, a linear layer followed by a 2 layer fully-connected network are used to estimate the predicted nucleotide-wise lDDT scores (pNuL) before making a prediction at the level of the entire molecule by aggregating nucleotide-level information, leading to predicted molecular-level lDDT (pMoL), thus enabling our method to estimate both local and global quality of the input RNA 3D structure. Experimental setup For training and performance evaluation, we use existing and publicly available benchmark datasets. Our training dataset contains 1,399 RNA targets collected from the training set used in the recent RNA 3D structure prediction method trRosettaRNA. We extracted the sequences from the 1,399 experimental structures and generated a total of about 52,000 structural models using a combination of different RNA 3D structure prediction tools including recent deep learning-enabled RNA structure prediction methods, physics-based RNA folding, and experimental structure perturbation using PyRosetta (see Section 4.3). Details of our training procedure are provided in Section 4.4. Our test data includes 30 independent RNAs, also collected from trRosettaRNA following the train and test splits of the original work. We generated 3D structural models for each of these 30 RNAs using the deep learning-enabled RNA structure prediction methods. We also use 12 RNA targets from CASP15 as an additional independent test set containing targets cleared for public access as of December 20, 2022, where the corresponding 3D structural models are collected directly from the CASP15 website https://predictioncenter.org/casp15/ based on the blind predictions submitted by various participating groups in CASP15 RNA 3D structure prediction challenge. We compare our method lociPARSE with traditional knowledge-based statistical potentials including rsRNASP, cgRNASP, RASP and DFIRE-RNA as well as state-of-the-art machine learning-based RNA scoring functions RNA3DCNN and ARES. To assess the accuracy of different aspects of quality estimation, we use a wide-range of performance measures including the ability to reproduce the ground truth lDDT scores both at the global and local levels, rank predictions for a given target, recognize the best predictions, and discriminate between ‘good’ and ‘bad’ predictions. Details of competing methods and evaluation metrics can be found in Section 4.5. Performance on 30 independent RNA targets Table 1 reports the performance of our new method lociPARSE and the other competing methods on 30 independent RNA targets. lociPARSE consistently outperforms all other tested methods across almost all performance criteria. For instance, lociPARSE attains the highest global Pearson’s  of 0.67 which is much better than the second-best ARES (0.54). The same trend continues for global Spearman’s  (lociPARSE: 0.72 vs. the second-best ARES: 0.63) and global Kendall’s  (lociPARSE: 0.54 vs. the second-best ARES: 0.46). Additionally, lociPARSE attains the lowest diff of 0.09, which is lower than the second-best rsRNASP (0.11). Furthermore, lociPARSE always delivers the highest per-target average correlations. In terms of average lDDT loss, however, DFIRE-RNA attains the lowest average loss (0.05). Meanwhile, lociPARSE, ARES, and rsRNASP are tied at the second spot with a comparably low loss of 0.06. Of note, ARES, the second-best performing method after lociPARSE in terms of global correlations, exhibits poor diff. DFIRE-RNA, the method attaining the lowest average lDDT loss, does not deliver top performance in terms of global correlations. That is, there are complementary aspects of scoring and model quality estimation that can lead to performance trade-offs. Our new method lociPARSE strikes an ideal balance to deliver top-notch RNA scoring performance across a wide range of assessment metrics simultaneously. It is interesting to note that among the other tested methods, the two machine learning-based scoring function ARES and RNA3DCNN show dramatically different performance. While ARES is consistently better than the traditional knowledge-based statistical potentials in terms of global correlations and comparable in terms of per-target average correlations, RNA3DCNN exhibits poor global and per-target average correlations, which are much lower than most knowledge-based statistical potentials. A similar trend can be observed between rsRNASP and its coarse-grained counterpart cgRNASP, where rsRNASP consistently attains good global and per-target average correlations but cgRNASP falls short. That is, subtle methodological differences such as the granularity of RNA conformational space representation or the choice of the neural network architecture can lead to dramatic difference in performance. Meanwhile, the novel use of locality-aware invariant point attention in lociPARSE substantially improves RNA scoring performance, surpassing both machine learning-based scoring functions and knowledge-based statistical potentials. Performance on CASP15 RNA targets To investigate the ability of lociPARSE to distinguish ‘good’ and ‘bad’ models in comparison with the other tested methods, where a threshold of lDDT > 0.75 is used to differentiate ‘good’ and ‘bad’ models following CASP15 official assessment, we performed ROC analysis using all structural models for all targets in CASP15. Figure 2a shows the ROC curves with AUC values. Once again, lociPARSE achieves the highest AUC value of 0.96, which is much higher than the second-best method rsRNASP (0.83), demonstrating its better performance in separating good and bad models compared to the others. Furthermore, as shown in Figure 2b, lociPARSE attains the lowest average lDDT loss of 0.07, which is noticeably lower than the second-best rsRNASP and cgRNASP (0.11). It is interesting to note that DFIRE-RNA, the method attaining the lowest loss in 30 independent RNA targets, yields a poor loss (0.16) in CASP15. By contrast, lociPARSE consistently attains low loss in both test sets, indicating its ability to select the best model that generalizes across different datasets. Supplementary table S1 reporting the full set of assessment metrics for lociPARSE against all other tested methods in the CASP15 set further demonstrates the performance generalizability of lociPARSE, which consistently outperforms all other tested methods across almost all metrics. When local nucleotide-wise quality is evaluated, lociPARSE is orders of magnitude better than RNA3DCNN, the only other method except lociPARSE that can estimate per-nucleotide score for local quality assessment. For example, lociPARSE attains more than four times higher global Pearson’s , Spearman’s , and Kendall’s  than RNA3DCNN, and achieves less than one-third of the diff attained by RNA3DCNN (Table 2). In summary, lociPARSE represents a leap forward in terms of local nucleotide-wise scoring performance. Case study Figure 3 shows a representative example of local nucleotide-wise quality estimation using lociPARSE for a top-ranked structural model submitted by the winning group AIchemy RNA2 (group 232) for the CASP15 target R1108 having length of 69. The predicted nucleotide-wise lDDT (pNuL) scores are in close agreement with the ground truth lDDT with a high Pearson’s  of 0.89 (Figure 3a). Two local problematic regions are estimated by lociPARSE in nucleotide positions (19–27) and (59–63). These two local problematic regions are visually noticeable when the predicted structural model is aligned to the experimental structure. The poorly modeled structural regions around the hairpin loop in nucleotide positions (19 – 27) and part of the helix strand in positions (59 – 63) are obvious even with simple visual inspection (Figure 3b). By contrast, virtually all nucleotides with high pNuL values are structurally well modeled. Ablation study and hyperparameter selection To examine the relative importance of the features and architectural hyperparameters adopted in lociPARSE, we conduct ablation experiments by systematically varying individual parameter during model training using the reduced training set and evaluating the accuracy on the independent validation set (see Section 4.3). Table 3 reports the composite quality score  defined in section 4.5 of the full-fledged version of lociPARSE serving as a baseline and its ablated variants. The results demonstrate that all the parameters adopted in the full-fledged version of lociPARSE positively contribute to the overall accuracy achieved by lociPARSE. For example, we notice performance decline when we vary the value of  used in the nearest neighbors for the locality computation from  used in the baseline to . Furthermore, to bias the attention weights as well as to update scalar features, we make use of the pair features in the form of nucleotide-nucleotide atomic distances between all pair of 3 atoms P, C4′ and , encoded with Gaussian radial basis functions (hereafter called ). We notice a significant performance drop when  features are isolated. Similarly, we notice consistent performance decline from the baseline configuration whenever we vary the network architecture such as the number of IPA layers  or various IPA hyperparameters , justifying our choice of the parameters adopted in the full-fledged version of lociPARSE.","In this work, we developed lociPARSE, a locality-aware invariant point attention model for scoring RNA 3D structures. lociPARSE uses locality information derived from the RNA atomic coordinates to define nucleotide-wise frames together with its local atomic environment. This, coupled with the invariant point attention architecture, allows for the simultaneous estimation of local quality in the form of predicted nucleotide-wise lDDT (pNuL) scores which are then aggregated over all nucleotides to estimate global structural correctness in the form of predicted molecular-level lDDT (pMoL). Our empirical results demonstrate the superiority of our method in scoring RNA 3D structures compared to existing approaches. Our locality-aware attention-based architecture can be extended in several ways, including estimating other local quality measures such as the Interaction Network Fidelity (INF) score, which is a local interaction metric that captures various types of base–base interactions in RNA. In fact, INF and lDDT have been shown to correlate well in a near-linear and size-independent relationship, suggesting that lDDT may capture the subset of interactions measured in INF whereas INF focuses on a selection of RNA-specific interactions. A model with a very similar architecture as lociPARSE would make an excellent candidate for jointly estimating INF and lDDT, thereby capturing complementary aspects of local quality. Further, a promising direction for future work is to investigate the potential benefits of capturing multi-state conformational landscape of RNA, since many RNA targets exhibit conformational flexibility. The lDDT score can be computed simultaneously against multiple reference structures of the same RNA at the same time, without arbitrarily selecting one reference structure for the target or removing parts that show variability. Training our model using multi-reference lDDT to capture different classes of conformations will allow our scoring function to account for conformational flexibility and pave the way to evaluate predictions of conformational ensembles instead of just a single structure. One limitation of our method is that it does not account for stereochemical quality and physical plausibility of the model being evaluated. This is because unlike for proteins, the currently available implementation of lDDT for RNA do not penalize for stereochemical violations. Using a customized version of lDDT that incorporates stereochemical quality checks in its calculation can address such limitation, and this aspect remains an important future direction.",10.1101/2023.11.04.565599
PMC10634702,37961325,Post-transcriptional mechanisms modulate the consequences of adaptive copy number variation,"Copy-number variants (CNVs) are large-scale amplifications or deletions of DNA that can drive rapid adaptive evolution and result in large-scale changes in gene expression. Whereas alterations in the copy number of one or more genes within a CNV can confer a selective advantage, other genes within a CNV can decrease fitness when their dosage is changed. Dosage compensation - in which the gene expression output from multiple gene copies is less than expected - is one means by which an organism can mitigate the fitness costs of deleterious gene amplification. Previous research has shown evidence for dosage compensation at both the transcriptional level and at the level of protein expression; however, the extent of compensation differs substantially between genes, strains, and studies. Here, we investigated sources of dosage compensation at multiple levels of gene expression regulation by defining the transcriptome, translatome and proteome of experimentally evolved yeast (Saccharomyces cerevisiae) strains containing adaptive CNVs. We quantified the gene expression output at each step and found evidence of widespread dosage compensation at the protein abundance (~47%) level. By contrast we find only limited evidence for dosage compensation at the transcriptional (~8%) and translational (~3%) level. We also find substantial divergence in the expression of unamplified genes in evolved strains that could be due to either the presence of a CNV or adaptation to the environment. Detailed analysis of 82 amplified and 411 unamplified genes with significantly discrepant relationships between RNA and protein abundances identified enrichment for upstream open reading frames (uORFs). These uORFs are enriched for binding site motifs for SSD1, an RNA binding protein that has previously been associated with tolerance of aneuploidy. Our findings suggest that, in the presence of CNVs, SSD1 may act to alter the expression of specific genes by potentiating uORF mediated translational regulation.","Copy-number variants (CNVs) are amplifications or deletions of DNA that can span dozens of nucleotides to whole chromosomes. CNVs are frequently observed over both short and long evolutionary time spans, although their selective advantage may be different between the two. In the short term CNVs can result in large changes in gene expression and protein abundance, which provides a selective advantage further driving rapid adaptive evolution. Gene amplification has been shown to mediate rapid adaptation to a variety of selective pressures from nutrient limitation to antibiotics in both natural and experimental populations of microbes. CNVs are also common in cancers, where they can promote tumorigenesis by oncogene amplification and drive genome wide changes in gene expression. A simple model of the short-term fitness effects of CNVs is a variant of the “driver-hitchhiker” model wherein the fitness benefit of a CNV is derived from the amplification of a single gene or “driver”, while fitness costs arise from the amplification of all “hitchhikers”. Since CNVs include up to hundreds of genes the potential fitness costs of hitchhikers can be high. These fitness costs can be categorized as “dosage burden” wherein the fitness cost arises from the burden of the additional DNA replication and gene expression. Conversely, fitness costs may arise from the stoichiometric imbalance of specific “dosage sensitive” genes. The archetypal example of dosage sensitivity is the imbalance of proteins involved in a heteromeric protein complex, leading to the complex having altered function. Importantly, the fitness effect of any CNV is highly dependent on the genetic background and environmental context. Dosage compensation (DC) is one mechanism by which an organism could mitigate the fitness costs of hitchhiker gene amplification. While classically associated with sex chromosome inactivation through chromatin silencing, DC can also be gene specific. Furthermore, while DC is often conceptualized as ‘complete’, such that additional gene copies result in no additional expression, DC can also result in range of increased gene expression levels that are significantly less than what would be expected based on the copy-number. This ‘incomplete’ dosage compensation is sometimes referred to as attenuation. Here, we use the term dosage compensation (DC) to refer to both ‘complete’ (e.g., the gene expression with two copies of the allele is identical to the gene expression from one copy) and ‘incomplete’ (e.g., the gene expression with two copies of the allele is greater than the gene expression from one copy, but less than two times the expression of one copy). Recent work on the transcriptional regulation of CNV amplified genes has found that, depending on genetic background and environment 10–60% of amplified genes feature transcriptional dosage compensation. The mechanism underlying this DC is unclear but changes in transcription factor abundances and their concentrations is one possible mechanism. Translational DC has recently become a focus of interest with the advent of ribosome profiling but has been studied using other methods. At the translational level some global mechanisms have been proposed including spliceosomal or ribosomal components acting to limit the global expression of the organism, and co-translational mechanisms, such as the unfolded protein response, although the universality of these mechanisms remains unresolved. Gene specific mechanisms have also been proposed and identified, such as RNA-binding proteins (RBPs) enabling the identification of specific genes and altering their post-transcriptional and translational regulation. The archetypal case of which is the sex-chromosome dosage compensation in Drosophila, enacted by the RNA-binding protein SXL altering the efficacy of alternative translation initiation at an upstream open reading frame (uORF). Upstream open reading frames (uORFs) are relatively short (6–300 nucleotide long) ORFs that are positioned upstream, in the transcript leader (TL, or 5’ UTR) of the main ORF (mORF) of a gene. uORFs have either canonical start codons (e.g. AUG) or near-cognate codon (NCC,) translation initiation sites (TIS) that allow for alternative translation initiation separate from the mORF, and as such can operate as translational regulatory elements, either preventing or enabling the translation of the downstream mORF. While uORFs have been suggested to be a broad class of stress responsive regulatory elements their role in the CNVs and aneuploids has not previously been evaluated. Similarly, start codons that are upstream, in-frame, and lack a stop codon separating them from the mORF are capable of adding additional peptides to downstream protein. These N-terminal extensions (NTEs) have been shown to alter the function of the protein product, with one recent study identifying an enrichment of mitochondrial targeting signals encoded by NTEs in yeast. One interesting candidate for gene specific translational regulation in yeast is SSD1, which encodes an RBP found to preferential bind to cis-elements in TLs and has been shown to alter translation. SSD1 is believed to play an important role in the response of yeast to aneuploid stress. While initial research suggested that yeast exhibited a particular sensitivity to aneuploidy, subsequent research has shown that this sensitivity is strain dependent, and due to the loss of SSD1 in the lab strain W303. Whether SSD1 has a role in accommodating CNVs in the genome is unknown. Quantitative mass spectrometry has enabled the identification of DC at the level of protein abundance. At the protein level, work in yeast has suggested 10–20% of gene amplifications have significant DC, although genetic differences are a known source of variance. One proposed mechanism for DC at the protein level is that excess subunits of proteins associated with heteromeric complexes are selectively targeted for protein aggregation and degradation by ubiquitination. A recent study using hundreds of natural isolates reports that nearly all aneuploid yeast strains observed had consistent protein level DC with an average decrease of 25% from the expected abundance. Here, we undertook an analysis of multi-level gene expression regulation in long-term experimentally evolved strains of Saccharomyces cerevisiae that acquired adaptive CNVs and their ancestor. These strains evolved over the course of hundreds of generations to glutamine-limited growth media and contain distinct adaptive CNVs containing the same driver gene GAP1 but distinct sets of additional hitchhiker genes within the CNV. We quantified expression at the transcriptional, translational, and protein levels in the glutamine-limited selective conditions in which the CNVs were selected. We find evidence of significant dosage compensation at the transcriptional (~8%), translational (~3%), and protein abundance (~47%) levels. We find that many genes with significant differences in expression are enriched in potential RBP binding sites for SSD1 as well as potential uORFs. Our results suggest that SSD1 and uORFs may act synergistically to limit the protein production of specific genes thereby modulating the impact of CNVs on gene expression.","Strains and growth media Briefly, the strains evaluated in this paper were originally published in, they include both long-term experimentally evolved strains as well as their ancestor. The naming convention in this paper is as follows: Ancestor (DGY1657); Trans (DGY1726); ODIRA_A (DGY1735); ComQuad (DGY1741); and ODIRA_B (DGY1743). Trans and ODIRA_A are from population samples after ~150 generations, ComQuad and ODIRA_B are from populations samples after ~250 generations. All strains are derived from FY4 (BY1747) and modified by the inclusion of a GFP and KanMX reporter cassette. All evolved strains were evolved in miniature chemostats under minimal glutamine growth media for either 150 or 250 generations, as described previously. For this study, strains were struck out on YEPD plates from −80°C freezer stocks and grown for 2 days at 30°C. Single colonies were selected based on the presence of fluorescence produced from the previously described mCitrine reporter. Illumina genomic DNA sequencing For short-read Illumina sequencing of strains we relied on previously generated data (, NCBI SRA accession SRP142330). Briefly, we performed genomic DNA extraction, followed by quantification using SYBR Green I and standardized to 2.5 ng/μL. Libraries were constructed using Illumina Nextera tagmentation. Final concentrations were measured using SYBR Green I, fragment sizes were measured using Agilent TapeStation 2200 before being balanced and pooled. Illumina DNA libraries were sequenced on an Illumina NextSeq 500 using 2×75 paired-end protocol. Illumina genomic DNA alignment and genotyping All sequences were aligned against the Ensembl_R64.1.50 reference genome. We aligned reads using bwa mem (v0.7.17,) and generated BAM files using samtools (v1.14,). FASTQ files for all sequencing are available from the SRA (accession SRP142330). Potential SNV and indel identification of each strain was performed using GATK’s HaplotypeCaller (ver 4.1.9.0,) in single-sample mode and annotated using Ensembl VEP (release 107,). Potential structural and copy-number variants using Illumina paired end data were identified using our custom analysis tool CVish (ver 1.x,). Nanopore genomic DNA sequencing, alignment, and genotyping All yeast strains were grown to greater than 1 × 10^7 cells/mL in 300 mL Glutamine minimal media. Genomic DNA from each strain was extracted using Qiagen 20/G Genomic tips from ~1.5 × 10^9 cells using the manufacturer’s protocol. All genomic DNA was barcoded using Oxford-Nanopore’s native barcoding genomic DNA kit (EXP-NBD104), adapters were added using the ligation sequencing kit (SQK-LSK109). The manufacturer’s protocol (versions NBE_9065_v109_revB_23May2018 and NBE_9065_V109_revP_14Aug2019) was followed with the following exceptions: incubation times for enzymatic repair step were increased to 15 min. All Agencourt AMPure XP beads were incubated for 30 min at 37°C before elution. Adapter ligation time was increased to 10 min. Multiplexed libraries were loaded on MinION flowcells (FLO-MIN106D R9) and run on a MinION sequencer (MIN-101B). All sequences were aligned against the Ensembl_R64.1.50 reference genome. ONT long-read sequences were aligned using minimap2 (2.22,), with variant detection using sniffles2 (2.0.6,), potential ODIRA CNV breakpoints were evaluated with mugio (v1.7,). DNA depth across genomes was calculated per nucleotide using bedtools ‘genomecov’ (ver 2.29.2,). DNA reads and depth estimation: We identified ~15.4% of genes in the ancestral strain had long-read DNA read-depth not near one copy. Some of these are known deletions (MATALPHA) or are regions with known properties of known copy number expansion (CUP1 region, ENA region, and ASP region) or transposons. (Table S12) To prevent the over or underestimation of expected expression in evolved strains, we manually analyzed the locus and flanking regions for signs of CNV breakpoints using both short and long-reads. If no evidence was found for a breakpoint the locus was assigned a copy-number of 1 (Table S13). (Supplement Materials “Gene copy number estimation using long and short reads”) Growth conditions and cell harvesting: Colonies were grown overnight on YEPD then used to inoculate 500 mL of minimal glutamine growth media in chemostats. These were then grown to saturation (approximately 48 hours ~1e7 cells / mL, measured using Coulter Counter) at 30°C in aerobic conditions. After saturation was achieved chemostats were switched to continuous mode with an inflow rate of fresh minimal glutamine growth media at a rate of 0.12 hour / L (corresponding to a population doubling time of ~5.8 hours). This was maintained for 24 hours before the cells were harvested. This was performed in duplicate for each strain for the simultaneous extraction of RNA and RPFs. This was performed for 5 replicates for the generation of material for mass spectrometry. Cells were harvested following previously described methods. Briefly, cycloheximide was added to the media to a final concentration of 100μg/L and incubated for 2 min, before being separated from the media using rapid vacuum filtration using Millipore 0.22μm filters. Cells were resuspended from filters using polysome lysis buffer with cycloheximide and then flash frozen immediately in liquid nitrogen before being transferred and stored at −80°C. RNAseq and ribosome profiling was performed by TB-SEQ, Inc. (Palo Alto, CA). TMT-labeling and LC-MS was performed by Proteomics Laboratory, NYU Langone Medical Center (New York, NY). Mass spectrometry and analysis We used 16Plex TMT labeling to enable pooling of peptides from 16 samples (Ch1–16) for offline fractionation and LC-MS/MS detection and quantification. Five replicates were generated for each strain and the total run was performed in two batches will be 2 batches (“A”,“B”). Ch1 was used as the common reference standard, an equimolar mix of all the samples, labeled with TMT Ch1. All samples were lysed using a TFA approach. Both batches were fractionated by offline HPLC chromatography using reverse phase C18 stationary phase at high pH. Peptides from collected fractions were separated by online HPLC on C18 at low pH coupled to the MS instrument. Spectra were analyzed using MaxQuant (v2.1.0.0,). Settings and parameters are described in the supplemental materials and the mqpar.xml file is provided (File S1). Differential abundance analysis is performed using Perseus (v1.6.15.0,). Data preprocessing and normalization procedures were performed as described are detailed in the supplemental materials. The statistical test for significance in differential abundance is Welch’s t-test of the difference between the normalized protein group abundances in each strain relative to the ancestor. Significance here is defined as q-value < 0.05 using BH multiple hypothesis test correction (ie. BH adj.pval < 0.05), (Table S3). Volcano plots indicating the protein abundances of CNV in each strain are in the Supplemental section. Expression profiling and analysis: RNA and RPFs were aligned using STAR (ver 2.7.6a,) filtering for known non-polyadenylated ncRNA (snRNA, rRNA, and tRNA) using Ensembl_R64.1.1 ncRNA fasta (File S2). Counts per gene were calculated using bedtools (v2.29.2,) coverage -counts -s -b against Saccharomyces_cerevisiae.R64-1-1.50.gff3. Raw read count tables are available for both RNA (Table S14) and RPF (Table S15). For quality control purposes we set an arbitrary expression threshold. Genes were required to not be transposon element associated genes, have at least 100 aligned RNA reads, 10 RPFs, and be detected by MS. This reduced our set to 4236 genes in total. Expected RNA abundances were calculated using a custom script that multiplies the observed abundance in ancestor by the copy-number of that gene in the evolved strain, these are available as supplemental files (Table S16) Differential abundance analysis was performed on both unnormalized observed and unnormalized expected RNAseq reads using DESeq2 (v1.6.3,). Results of DESeq2 are available as supplemental files (File S3). Significance here is defined as BH adj.pval < 0.05. Gene ontology term (GO term) enrichment was calculated using Yeastmine with significance cut off at BH adj-pvalue < 0.05. Redundant terms were then reduced using REVIGO using the Tiny option, UNIPROT Saccharomyces cerevisiae S288c database, and SimRel semantic similarity measure. All Venn diagrams were originally visualized using Venny (v2.1.0). All expression track data was visualized using IGV browser (v2.9.4,). Robust Scaling For the purposes of visualization, sequencing depth differences for CDS aligned reads from RNAseq and ribosome profiling were normalized using TPM. Log2 transformed TPM ratios were calculated for each strain relative to the ancestor. To compare between RNA, RPF, and the run-relative normalized intensities of detected peptides we scaled the data using RobustScaler (SciKit Learn, ver1.2.2). Differential expression ratio analysis Conceptually, an increase in ribosome protected fragment (RPF) abundances over a region could be caused by an increase in the amount of ribosomes per transcript or by an increase in the transcript abundance itself. Translation efficiency (RPF to RNA ratios) attempts to deconvolute these options by normalizing the RPF abundances by the RNA abundances. It is important to note that increased translation efficiency does not equate to increased protein abundance. While both the relative proportion of RNA (median rho = 0.29) and RPF (median rho = 0.33) in the evolved strain relative to the ancestor correlate with the relative difference in protein abundance, the same is not true for the translational efficiency (median rho = 0.03) which exhibits a large degree of difference from the final protein abundance (Figure S8). Protein to RPF ratios are an extension of this methodology that normalizes the protein intensity by the RPF abundance. Similarly, the ratio of protein abundance to RNA abundance, effectively an empirically derived inversion of the RNA-to-Protein coefficient. For the purposes of comparing between replicates and strains, in all cases, ratios are calculated using depth normalized values, either TPM (RNA, RPF) or the MaxQuant reporter intensity corrected value. To determine if the ratio of abundances of any given gene was different between strains, we calculated the ratios in each strain as a proportional ratio of medians. For example, (RPF-evolved / RNA-evolved) / (RPF-ancestor / RNA-ancestor), these were then tested using Fischer Exact Test (FET), Benjamani-Hochberg (BH) was used for multiple hypothesis correction of all p-values. Calculation of Dosage Compensation When allowing for ‘incomplete’ DC or dosage attenuation it is important to consider that gene expression can change regardless of the copy number of the gene. This is especially critical in organisms that have had time to adapt to multiple selective pressures, one of which may be the CNV. As such, it behooves us to consider dosage compensation to be a recurrent, but identifiable trend. That is, that the expression of an amplified gene shows a recurring, measurable, difference when compared to the expression of an unamplified paralog. To calculate this, we first binned each gene based on whether it was a CNV amplified or not. We then generated a proportional ratio of medians for the two bins: eg. (RPF-CNV / RNA-CNV) / (RPF-no_CNV / RNA-no_CNV), these were then tested using Fischer Exact Test (FET), Benjamani-Hochberg (BH) was used for multiple hypothesis correction of all p-values. For determination of transcriptional DC, we performed the same analysis using the observed RNA abundance compared to the expected RNA abundance: eg. (RNA-CNV_observed / RNA-CNV_expected) / (RPF-no_CNV_observed / RNA-no_CNV_expected). Selection of uORFs from previous studies Because uORFs have been known to play a role in translational regulation we sought to determine if previously identified uORF may be present in our data. We calculated the mean TL length of genes containing either ‘predicted’ uORFs from or ‘validated’ uORFs from. Because of the validation methodology used in, uORFs greater than 180bp distance from the main ORF TIS were not tested. We calculated the mean length of SGD annotated TLs for each set (Table S17). We found the predicted uORFs were significantly longer than the background TL length (MWU, 1.16-fold larger, pval < 1e-10) while the validated uORFs were shorter, (MWU, 1.35-fold shorter, pval = 0.71). Comparing genes with significantly different RPF to RNA ratios to these two sets of uORFs we found that the genes were significantly enriched in ‘predicted uORFs’ in the two sets produced different results; being significantly enriched (HGM, 1.3-fold higher, pval = 0.0) in the predicted uORFs. Conversely, they were significantly de-enriched (HGM, 1.4 lower pval = 0.0) in the validated uORFs. This is in agreement with the observation that longer TLs can contain a greater number of regulatory elements, potentially necessary for fine-grained regulation. For the purposes of this study we restrict ourselves to the ‘predicted’ uORFs. Unlike our manual analysis using this uORF set we determine uORF regulation genome wide. Differential positional analysis for the identification of uORFs and NTEs Because translation of uORFs is known to be condition sensitive ( ) and no previous study has been conducted using glutamine-limited growth media or CNVs, we sought to identify them using our ribosome profiling data. For each gene in each strain that exhibited a significant difference in Protein to RNA ratio relative to the ancestor we looked for the presence of ribosome loading with the transcript leader (TL or 5’ UTR). Candidate uORFs were required to have ribosomes inframe to potential alternative translation initiation sites (aTISs) be they canonical AUG start codons or NCC start codons. We used p-site fractionation to determine ribosomal reading frame, followed by start codon identification. As some genes (for example, UTH1, SSD1, SSA4, etc.) had potential uORFs in complex, overlapping, or nested configurations we summed across the TL from the most distal TIS to one codon upstream of the CDS. We evaluated all CNV amplified genes (e.g., ChrXI right arm) and all 411 genes with significantly different Protein to RNA ratios. As a background control we also evaluated all genes located on ChrXI left arm. We then sought to compare our results to the previously reported (e. “predicted”) uORFs. As both the CNV genes and the 411 genes were selected for evaluation they were withheld from the comparison to prevent bias. Comparing the identification on ChrXI left arm we found a 34% agreement (30 out of 88) with 3 uORFs (3%) being only previously identified and 55 (63%) being unique to this study. SSD1 motif analysis To identify genes enriched in SSD1 motifs in their TLs we first generated a list of TLs for all genes using the Yeastmine annotated ‘Five Prime UTR’ or from, taking the longest of either recorded instance. Combined these encompassed 6119 genes, 6080 of which exceeded the length of the motif. We converted these to fasta using bedtools ‘getfasta’ (ver 2.29.2,). These were then scanned for perfect matches to the CNYUCNYU motif. The background rate genome wide was calculated per nucleotide as 1.19e-3, the background rate using only TLs was 1.48e-3 per nucleotide. Significance was calculated using FET, using the proportional ratio of (observed hits of TL / length of TL) versus (Total hits / Total length). We found that of the 186 genes that were significantly enriched in SSD1 motifs only 130 were present in our data set at or above our QC threshold (1e2). Works cited.","Topological structure of copy number variant alleles We isolated four lineages that contained CNVs at the GAP1 locus following long term selection in glutamine-limited chemostats. CNV-containing lineages were identified through the use of a CNV reporter system and isolated using FACS from heterogeneous populations following more than 150 generations of selection. Defining the gene expression consequences of altered copy number requires precise definition of the copy number of each gene. Therefore, we performed hybrid de novo genome assembly, using a combination of long-read (ONT) and short-read (Illumina) sequencing to determine the topological structures for the CNVs in each strain (Figure 1B). On the basis of the CNV structures we also inferred the likely mechanisms of formation, which include transposon mediated tandem amplification and origin of replication dependent amplification (ODIRA, Brewer 2015) and named the CNV alleles to reflect these mechanisms. Because of the difficulty in ONT sequencing of inverted repeat sequences all ODIRA events were fully resolved using custom CNV identification (methods). CNV structures include “ODIRA_A”, an ODIRA event triplication spanning 77 genes (ALY1-SIR1) and “ODIRA_B”, an ODIRA event triplication spanning 91 genes (VPS1-ESL2). “Trans”, a transposon mediated tandem duplication (TOF2-GAP1) nested within a translocation and telomere replacement on ChrVIII. Finally, a complex quadruplex or “ComQuad” resulting from a ODIRA driven triplication followed by a singular amplification of GAP1 by a transposon event that spans the common core 17 genes from (SET3-TRK2). A complete list of genes and their copy number is available (Table S1). Collectively, the four CNV strains include between 17 – 91 genes present at 2, 3, or 4 additional copies. A set of 17 amplified genes centered on the GAP1 gene is common to all strains. Increased levels of gene expression are associated with CNV amplified genes To assess the gene expression consequences of CNVs we analyzed the four evolved CNV strains and the ancestral wildtype in glutamine-limited chemostats. Strains were grown until reaching a steady-state condition in which culture density remained constant (~10–12 generations). Cycloheximide (100μg/L) was added to cells for 2 minutes before harvesting by vacuum filtration. All RNA-seq and ribosome profiling were performed in tandem using two biological replicates for each strain. Mass spectrometry was conducted using 5 biological replicates. To define a dataset for quantitative analysis of gene expression we set a threshold for each gene of at least 100 RNAseq reads in at least one sample, at least 10 RPF reads in at least one sample, and have at least one unique peptide detected by mass spectrometry in at least one sample. Following filtering, 4236 genes remained in our dataset, of which 91 were amplified within a CNV in at least one evolved strain. These 91 amplified genes include the common core of 17 genes which are amplified in every evolved strain. We first sought to compare gene expression differences for CNV amplified genes between each evolved strain and the ancestor (Figure 1B, pink). We define gene expression differences between an evolved CNV strain and the ancestor as the gene expression divergence, which we determine for all genes regardless of copy number. To visualize divergence in gene expression within the CNV amplified region we generated heatmaps of robust scaled expression data (methods). In all four CNV strains we find increased levels of gene expression associated with CNV amplified genes. A median of 96% of genes within a CNV have significantly higher mRNA expression than the ancestor (DESeq2, adj.pvalue < 0.05), which is substantially higher than the median proportion of genes (9%) that are increased in expression for unamplified genes in the same strains (Figure 2). For RPF abundances we find that 94% of genes show significant increases for CNV amplified genes compared with a median of 8% for unamplified genes in the same strains. By contrast, the proportion of amplified genes with higher protein abundance is only 77% (Welch’s t-test, FDR adj.pvalue < 0.05), compared with a median of 10% for unamplified genes. This discrepancy between an increase in transcript and RPF abundance for virtually all CNV genes whereas almost one-quarter of the corresponding proteins are not increased in abundance suggests that regulatory mechanisms act on successive levels of expression to modulate protein expression of CNV amplified genes. Global patterns of divergence in gene expression In addition to adaptive CNVs causing changes in gene expression for genes that are located within the amplified region, genes in the rest of the genome that are unaltered in copy number may differ in expression as result of the CNV or other adaptive variation. Therefore, we quantified the global frequency of divergence in gene expression for each evolved strain at each level of gene expression. Divergence in RNA abundance In total, 85 of the 91 CNV amplified genes were significantly diverged in RNA abundance (BH adj.p-value < 0.05). All genes that were significantly diverged had increased RNA abundance compared with the ancestor. A small number of genes in the common CNV core including, DAL80, UTH1, PLN1, TRK2, and an additional gene not in the common core, ECM4, exhibited increases that were not statistically significant in at least one CNV strain despite being amplified. We also find substantial numbers of unamplified genes throughout the rest of the genome with significant divergence in mRNA abundance relative to the ancestor (median per strain of 1020 genes, (Table S2, Figure S1). Of these genes, only 206 were significantly diverged in every background, with 40 being consistently increased and 156 being consistently decreased in abundance. These 206 genes are significantly enriched in gene ontologies associated with metabolism, stress, transport, and ER / protein-folding (Figure S1). Divergence in Ribosome (RPF) abundance Of the 91 CNV amplified genes, 77 were significantly diverged in RPF abundance (BH adj.p-value < 0.05) and exhibited increased RPF abundance compared with the ancestor. A total of 13 genes were not significantly diverged in RPF abundance in at least one strain despite being amplified including TIF1, RPS21A, and genes within the common core including DAL80, UTH1, and GAP1. Genome-wide, we find a substantial number of unamplified genes are diverged in RPF abundance (median per strain of 724 genes, (Table S2, Figure S2). Of these genes, only 95 are significantly divergent in every background, with 22 being consistently increased and 69 being consistently decreased in RPF abundance. These 95 genes are significantly enriched in gene ontologies associated with metabolism (Figure S2). Divergence in Protein abundance We found that 44 of 91 CNV amplified genes were significantly diverged in protein abundance (Welch’s t-test, BH adj.pval < 0.05) and had higher protein abundance than the ancestor (Table S3). However, 27 genes exhibit less than significant divergence in protein abundance in at least one strain they are amplified in, including TOF2, LAS1, ALY1, and members of the common core including DAL80, UTH1, and GAP1. We evaluated changes in protein abundance of unamplified genes. We find the median number of genes in each strain with significantly divergent protein abundances (Welch’s t-test, BH adj.pval < 0.05) from the ancestor is ~573, with 85 genes being significantly divergent in every strain, of which 43 are consistently higher and 41 are consistently lower. These 85 genes are enriched with gene ontologies associated with metabolism as well as mitochondria and ATP synthesis (Figure S3). The discrepancy between the patterns of divergence at the RNA, RPF, and protein level are noteworthy as our experimental design is likely to result in increased statistical power for mass spectrometry data, which were analyzed in five biological replicates compared with RNAseq and RPF analysis. Despite this, we identified a smaller proportion of CNV amplified genes that result in increased protein expression than the proportion that result in increased RNA and RPF abundance suggesting that DC mechanisms act primarily at the level of protein abundance. Gene expression ratios as measures of gene regulation Observed RNA to Expected RNA ratio Typically, in differential expression analysis the copy-number of the underlying genes are assumed to be uniform throughout the genome and between genotypes. As that is not the case in our study, we first sought to correct for changes in copy-number (Figure 1B, blue). Here, we define a null model in which the expected RNA abundance for genes within a CNV is determined by the abundance in the ancestor multiplied by the copy-number in the evolved strain. For example, a twofold increase in gene copy-number is expected to result in a two-fold increase in mRNA. We determined the copy number corrected gene expression for all genes and then compared the observed RNA abundance to this expected value (methods, Table S4), (Figure 3A). In total, more than half (54 of 91) of the CNV amplified genes were found to have significantly different Observed to Expected RNA ratios. Of these 54 CNV amplified genes 27 had significantly lower ratios in at least one strain, whereas 28 had significantly higher, in at least one strain. One gene, PTR2, exhibited both increased and decreased ratios in different strains. For the 17 common core genes we find 2 genes (KAE1, SHB17) have significantly higher ratios in more than one strain whereas 5 genes have significantly lower ratios in more than one (DAL80, UTH1, PLN1, TRK2, and, notably, GAP1) (Table S2). Interestingly, we find GAP1, the hypothesized driver of CNV formation, to have lower ratios in all evolved strains, although this reduction is only significant in two of the CNV strains (Trans and ODIRA_A). DAL80, a transcription factor that is the negative regulator of GAP1, has significantly lower ratios in all evolved strains. We found that 2,934 unamplified genes had significantly different Observed to Expected RNA ratios (Figure 3A, BH adj.p-value < 0.05) in at least one evolved strain (Figure S5). 1450 of these were significantly higher in at least one strain and 1635 were lower. Of these 366, (12.4%) were significantly different in all evolved strains and are enriched for gene functions associated with metabolism, stress response, chaperone cofactor-dependent protein refolding, and ER organization. RPF to RNA ratios To investigate the consequences of CNVs at the level of translation regulation we normalized the ribosome abundance, as measured by RPFs aligned to mORFs only, by transcript abundance. This translation efficiency ratio is an estimate of the amount of translating ribosomes per transcript. Changes in translation efficiency are associated with altered translation initiation rates at the start codon as well as differences in elongation rates throughout the CDS. In addition, differential usage of uORFs can result in changes in ribosome association with transcripts. In total, 16 CNV amplified genes were found to have significantly different RPF to RNA ratios. Notably, only one of these, PAM17, had increased RPF to RNA ratios whereas the other 15 CNV amplified genes exhibited decreased ratios (Table S5). Of these, GAP1 is the only one of the 18 genes in the common core of CNV amplified genes with lower ratios in every evolved strain (median ratio of 0.82 or 1.22-fold less than the ancestor). For the unamplified genes, we found that 802 genes had significantly different RPF to RNA ratios (Figure 3B, FET, p-value < 0.05) relative to the ancestor, 288 (36%) of these had significantly lower ratios than the ancestor, while the majority (549, 68%) had significantly higher ratios than the ancestor (Table S5). Of the 802 genes that had significantly different ratios, only 65 were significantly different from the ancestor in all evolved strains. These genes were enriched in functions associated with ribosome assembly and translational fidelity in addition to metabolism (Figure S6). Protein to RPF ratios Differences in the ratio of protein to RPF abundance within the mORF can be evidence of post-translational regulation, likely the result of changes in protein stability. We find that most common CNV amplified genes (16 of 17) had significantly lower Protein to RPF ratios in at least one strain (Table S6). This includes GAP1, which had significantly lower ratios in ODIRA_B and ComQuad (median 2.6-fold less than the ancestor) but not Trans and ODIRA_A (median 1.4-fold less). Similar to RPF to RNA ratios, we find a significantly lower Protein to RPF ratios in 19 of 91 CNV amplified genes, compared to 98 of 4209 unamplified genes (FET, 11-fold higher, pval < 1e-10). We find 275 unamplified genes (Figure 3C>) have significantly different Protein to RPF ratios between the evolved and the ancestor strains (FET, pval <0.05, Table S6). Of the 275 genes with significantly different ratios only 21 were significantly different from the ancestor in all evolved strains. These genes were enriched in ontologies associated with metabolism and catabolism (Figure S7). Dosage Compensation We sought to identify evidence of dosage compensation of amplified genes. Because all evolved strains have adapted to both glutamine-limited growth media as well as the CNVs themselves, a given gene may have deviations in gene expression regardless of the copy-number. One example of this is RSC4 (Figure 3E) which is only amplified in ODIRA_B but has lower Observed to Expected RNA ratios in every evolved strain and significantly so in both ODIRA_A and ComQuad (DESeq2, adj.pval <= 0.05). Conceptually, dosage compensation should require that the expression efficiency of a gene, when amplified, is significantly lower than when unamplified. An example of this is GLC7 (Figure 3E), which is only amplified in ODIRA_B but has significantly lower Observed to Expected RNA ratios when amplified compared with the unamplified copy in the other evolved strains. To identify genes with potential transcription dosage compensation we calculated the median Observed to Expected RNA ratios of all genes when amplified versus when unamplified in evolved strains. Seven genes out of 91 (~7.7%), DAL80, UTH1, TIF1, GLC7, PLN1, RPL40B and GAP1 had significantly lower ratios (FET, BH adj.pval<0.05) when amplified, consistent with dosage compensation at the transcriptional level. Only one gene, PCK1, had significantly higher ratios when amplified (Table S7). To identify genes with potential translational dosage compensation we calculated the median RPF to RNA ratios of each gene when amplified and when unamplified in evolved strains. Three genes out of 91 (~3.3%), RPS26B, RPS21A, and GAP1 had significantly lower ratios (FET, BH adj.pval<0.05) when amplified, consistent with dosage compensation at the translational level. Notably, two genes, PLN1 and RPL40B had significantly higher ratios when amplified, potentially counteracting the decreased transcriptional efficiency (Table S8). To identify genes with potential post-translational dosage compensation we calculated the median Protein to RPF ratio of each gene when amplified and when unamplified (FET, BH adj-pval < 0.05). We found 43 genes out of 91 (~47.3%) had significantly lower ratios when amplified. Notably, GAP1, the putative target of selection, does have lower ratios, although it is not significant. We did not find any amplified genes with significantly higher ratios (Table S9). Differential positional analysis of ribosome footprint profiles identifies altered translational landscape In addition to quantifying ribosome abundances, ribosome profiling also captures ribosome positions. Identifying alterations in ribosome positions can be used to identify translational regulatory features, such as the presence of upstream open reading frames (uORFs), N-terminal extensions (NTEs) to existing ORFs, and potential de novo genes in the form of novel ORFs (Figure 3). uORFs Previously published research has identified potential uORFs upstream of several genes that are amplified by CNVs in one or more strains, including GAP1, DAL80, and UTH1. Using ribosome profiling data from each strain we manually identified potential uORFs in 49 of the 109 CNV amplified genes. This frequency is not significantly different from the observed background rate of 84 of 212 genes on the unamplified arm of ChrXI (FET, 1.1-fold higher, p-value = 0.63) (Table S10), (FIGURE 4A). This suggests that genes within CNVs are not significantly enriched for uORFs and their presence is independent of the CNV itself. NTEs Using manual analysis of ribosome profiling data we identified only one N-terminal extension (NTE) associated with a CNV amplified gene (YKR045C), and three NTEs for genes in the unamplified arm of ChrXI (APE1, MRPL31, and ZRT3). Although our mass spectrometry library was not designed to be enriched for NTEs we did identify 4 genes (PRC1, YDJ1, ADE12, NOB1) with peptides that uniquely map to non-AUG NTEs (see example FIGURE 4B). We also identified an NTE in YBP2, although this is most likely a mis-annotated AUG start codon. Of these eight genes, only YDJ1 has been described previously as having an NTE albeit under different growth conditions. YDJ1 is a HSP40 co-chaperone that enables unfolded protein binding and is involved in the high-molecular-weight complex (HMC) with the stress associated SSA1 and SSA2 proteins. YDJ1, SSA1, and SSA2 all exhibit significantly higher expression efficiencies. Differential expression efficiency One caveat with the use of ribosome abundance metrics is that not all ribosomes are actively translating or translating at the same speed. Indeed, in addition to ribosome queuing, ribosome collisions, and translation pause sites, elongation rates are part of the regulatory dynamic that responds to cellular and environmental signals. To best encapsulate these changes, we calculated the ratio of protein to RNA for each gene with the goal of inferring regulatory modulation at both the translational and post-translational levels. We find that 84 of 91 amplified genes have significantly different Protein to RNA ratios compared to the ancestor and the vast majority (82 or 90%) of these have lower ratios than the ancestor (Figure 4C). For the unamplified genes we found 411 genes with significantly different ratios (Figure 3D, p-value < 0.05) relative to the ancestor (Figure 4D). Of these, 178 (44%) of these had significantly lower ratios than the ancestor, whereas 228 (56%) had significantly higher ratios than the ancestor (Table S11). Enrichment of uORFs in genes with significantly different expression efficiency Translation efficiency can be altered by uORF mediated translational regulation. We found that genes with significant RPF to RNA ratios were also significantly enriched in uORFs (Hypergeometric test (HGM), 1.3-fold enriched, pval = 1e-4), suggesting that changes in RPF to RNA ratios could be a product of uORF mediated translational regulation. This motivated us to test if there was similar enrichment of uORFs in genes with altered overall expression of Protein to RNA. Indeed, we found similar enrichment of predicted uORFs in genes with significantly different Protein to RNA ratios (HGM, 1.3-fold enriched, pval = 1e-4). While verifying that the predicted uORFs were occupied by ribosomes, we observed many previously unreported uORFs. To better understand the effect uORFs had on expression we manually evaluated all 91 CNV amplified genes (Figure 4C) and all 411 genes with significantly different Protein to RNA ratios (Figure 4D) for the presence of uORFs. We found that CNV amplified genes with significantly different ratios are enriched for uORFs (FET, 1.9-fold, pval=9.7e-3). Similarly, we found enrichment of uORFs in the 411 genes with significantly different ratios (FET, 2.0-fold, pval=1.2e-4). This is consistent with uORF mediated regulation acting to alter the protein output for a given transcript level. Notably, CNV amplified genes with uORFs are not significantly enriched in any gene ontology terms, but non-CNV amplified genes with significantly different expression efficiencies and uORFs are significantly enriched in genes associated with metabolism and nitrogen metabolism but also stress response pathways and, intriguingly, protein folding. The stress response and protein folding pathways are particularly interesting as they include HSP26, HSP31, HSP32, HSP42, HSP78, HSP82, and HSP104 as well as SSA1, SSA2, SSA4, SSC1, SSE1, SSE2 and YDJ1. This suggests that uORFs mediated translational regulation may be an important factor adaptation to stress, including both nutrient limitation stress and gene amplification stress. RNA binding protein SSD1 potentially regulates translation by associating with uORFs Recently, a conserved SSD1p RNA binding motif (CNYUCNYU) was identified using an analysis of high confidence binding sites in optimal and heat-shocked conditions. To better understand the role of SSD1 may play in gene regulation in evolved CNV strains we evaluated genes significantly enriched in the CNYUCNYU motif. One well known target of SSD1p is UTH1, a mitochondrial protein involved in regulating both mitochondrial biogenesis and degradation, the overexpression of UTH1 has been shown to lead to cell death in yeast strains lacking SSD1 and is associated with the role of SSD1 in cellular response to aneuploid stress. Indeed, we find that UTH1 is amplified in every evolved strain and has lower Protein to RNA ratios in every evolved strain. A manual analysis of the ribosome profiling data found a large abundance of uORFs and numerous CNYUCNYU motifs in UTH1 (FET, 3-fold enriched, pval = 0.01) (Figure 5A). Similar co-occurrences of uORFs and CNYCNYU motifs were observed for several well described SSD1 targets including CLN2 SUN4, CTS1, DSE2, NSA3, SRL1, SCW4, SCW10, and CCW12. This co-occurrence was also observed in genes identified by a SSD1 KO study conducted in an aneuploid background, such as SCW4, SCW10, CCW12, and TOS1. Notably, we also found that SSD1 itself appears to have extensive uORFs and SSD1 motifs, suggesting it is subject to self-regulation (Figure 5B). We also find that it exhibits higher Protein to RNA ratios in every evolved strain, suggesting that post-transcriptional regulation may be involved. Indeed, SSD1 is transcriptionally repressed in all evolved strains, but translationally upregulated in all evolved strains resulting in a final protein abundance slightly higher than the ancestor, consistent with translational regulation. Using the CNYUCNYU motif, we scanned the annotated TLs of the genome to identify genes enriched in the SSD1 motif identifying 185 significantly enriched genes (FET, pval < 0.05) of which 130 were present in our data set. To test if these SSD1 motifs, in co-occurrence with uORFs, could be associated with changes in Protein to RNA ratios we compared the difference in the number observed expected SSD1p sites based on length (Figure 5C). SSD1p targets (Bayne et al.) are significantly enriched (MWU, pval = 5.7e-12), the subset of those with uORFs even more so (MWU, pval = 3.5e-15). Similarly, genes with significantly different Protein to RNA ratios are weakly enriched (MWU, pval = 0.096), the subset of those genes without uORFs are not significantly different from the background (MWU, pval = 0.82), while the subset with uORFs is significantly enriched (MWU, pval = 0.034). Taken together this suggests that SSD1 may be driving altered expression efficiency through uORF mediated translational regulation, affecting both CNV amplified and unamplified genes.","In this paper we set out to characterize how gene expression changes at the transcriptional, translational, and protein level in response to CNV associated adaptations to selection. To do this we quantified different aspects of gene expression in5 strains of yeast, 4 of which had undergone adaptation to glutamine-limited growth conditions for hundreds of generations and had gained copy-number variants (CNVs), as well as an ancestral strain unadapted to glutamine-limited growth and lacking any CNVs (Figure 1). In a comparison of gene expression using differential expression analysis (DESeq2 and Perseus), we observed that the majority of genes amplified by CNVs had significantly higher levels of expression than their ancestral counterparts (Figure 2) which agrees with previous reports of on the broad effects of CNVs. We also observed that the number of significantly divergent genes (including both amplified and unamplified) decreases at each successive level of expression, from a median of ~1500 per strain for RNA, to ~1100 for ribosome abundance, to ~600 per strain for protein abundance. Again, this agrees with previous reports of buffering of divergent gene expression at the translational and post-translational levels. We also sought to identify regulatory modulation of gene expression using ratios of abundance across the levels of gene expression between the evolved strain and ancestor (Figure 3). Using these ratios of abundance, we found that numerous genes had significant differences in their RPF to RNA ratios, Protein to RPF ratios, or Protein to RNA, relative to the ancestor. By correcting for changes in gene expression unassociated with CNV amplification, we were able to show that dosage compensation occurs at each level of gene expression, potentially most strongly at the level of protein abundance where it is wide spread and not limited to genes involved in protein complexes, in agreement with recent reports of DC in natural isolates of yeast with adaptive aneuploidy and humans cancers. It is worth noting however that the current method of identifying dosage compensation may be particularly underpowered for evaluating changes in transcription and translation as only two replicates exist of each, conversely measures utilizing the MS data has 5 replicates and appears to be of much greater sensitivity. An analysis of genes with significantly different Protein to RNA ratios found them to be significantly enriched with upstream open reading frames (uORFs, Figure 4A,B) suggesting that these translational regulatory elements may be playing a role in the post-transcriptional regulation of gene expression. A manual analysis of uORFs was performed for all CNV amplified genes and all genes with significantly different Protein to RNA ratios (Figure 4C,D). We found that CNVs were not specifically enriched in uORFs; but all genes with significantly different Protein to RNA ratios were significantly enriched, regardless of being CNV amplified or not. This suggests that global changes in gene expression in response to adaptation to nutrient-limitation and CNV amplification may be translationally mediated by uORFs. Finally, in the course of our uORF analysis of genes with significantly different Protein to RNA ratios we identified several well-known targets of the RNA-binding protein SSD1 (Figure 5B). SSD1, has previously been shown to be a stress responsive RBP under glucose starvation, heat shock and quiescence. It is capable of altering translation of bound RNA, potentially by preferentially binding to TLs and interfering with ribosome scanning. To determine if SSD1 may play a role in altering expression efficiencies, we first identified 130 genes in our data set that had significant enrichment of the conserved SSD1p binding motif. We found the greatest enrichment of SSD1p binding motifs in the subset of SSD1p binding targets with uORFs and the subset of genes with significantly different Protein to RNA ratios with uORFs (Figure 5C). These findings suggest that SSD1p may act as a stress responsive trans-acting factor driving uORF mediated translational regulation. Furthermore, these findings help contextualize the role of SSD1 in CNV and aneuploid yeast strains. Previous studies have shown a much higher level of gene specific dosage compensation in aneuploid yeast strains with a functional copy of SSD1 (such as the S288c strain, as was used in this study) compared to yeast strains lacking a functioning SSD1 (such as W303). This suggests that the importance of SSD1 in mitigating the fitness defects of CNVs may be a consequence of the broader role SSD1 plays in the translational regulation of gene expression under stress, and not specific to CNVs per se. Indeed, this may explain some of the observed but varying and incomplete overlap seen in SSD1 targets, genes involved in the hypo-osmotic stress pathway, CAGE, and analyses that sought to identify the CNV and aneuploid stress response at the transcriptional level. Our study is not without its limitations. Notably, the number of strains evaluated, and the number of replicates limit the scope of our inferences and the generalizability of our results. Beyond that, technical limitations of TMT-labeling (eg. ratio compression) makes some analysis difficult and prevents a straightforward fold-change analysis of protein abundance. Furthermore, the reliance on a manual, heuristic approach for uORF identification limits the pool of genes evaluated to only those of the highest relevance (eg. CNV amplified genes, genes with significantly different Protein to RNA ratios, and all genes on the left arm of chromosome XII as a background control). That said, this work presents several avenues for ongoing research, such as the potential for RBPs to interact with uORFs, and thereby alter downstream gene expression. While this is not unprecedented but it is largely unexplored. We also observe a shift in expression from an early “protein divergent” phase to a late “transcription / translation divergent” phase. If we consider the number of generations diverged from the ancestor, the generation 150 strains (Trans and ODIRA_A) to have we 2.4-fold more significantly diverged genes at the level of protein abundance relative to the generation 250 strains (ComQuad, ODIRA_B) (average of 1078 genes versus 444). Conversely, at the level of transcription this is reversed with the generation 250 strains having more significantly diverged genes relative to the generation 150 (1740 versus 546) with similar divergence seen at the translational level (1381 versus 361). While it is difficult to meaningfully extrapolate from such a small number of observations it supports previous observations of rapidly evolving regulatory networks acting to mitigate protein imbalances.",10.1101/2023.10.20.563336
PMC10635207,37961288,Measures of population immunity can predict the dominant clade of influenza A (H3N2) in the 2017–2018 season and reveal age-associated differences in susceptibility and antibody-binding specificity,"Background: For antigenically variable pathogens such as influenza, strain fitness is partly determined by the relative availability of hosts susceptible to infection with that strain compared to others. Antibodies to the hemagglutinin (HA) and neuraminidase (NA) confer substantial protection against influenza infection. We asked if a cross-sectional antibody-derived estimate of population susceptibility to different clades of influenza A (H3N2) could predict the success of clades in the following season. Methods: We collected sera from 483 healthy individuals aged 1 to 90 years in the summer of 2017 and analyzed neutralizing responses to the HA and NA of representative strains using Focus Reduction Neutralization Tests (FNRT) and Enzyme-Linked Lectin Assays (ELLA). We estimated relative population-average and age-specific susceptibilities to circulating viral clades and compared those estimates to changes in clade frequencies in the following 2017–18 season. Results: The clade to which neutralizing antibody titers were lowest, indicating greater population susceptibility, dominated the next season. Titer correlations between viral strains varied by age, suggesting age-associated differences in epitope targeting driven by shared past exposures. Yet substantial unexplained variation remains within age groups. Conclusions: This study indicates how representative measures of population immunity might improve evolutionary forecasts and inform selective pressures on influenza.","The epidemiological and evolutionary dynamics of antigenically variable pathogens are intrinsically sensitive to immunity in the host population. This understanding has long shaped vaccination strategies against influenza. Twice each year, representative strains from circulating clades are evaluated for their ability to escape antibodies to current vaccine strains, under the expectation that these clades might come to dominate and could be poorly matched by the current vaccine. As surrogates for the human population, influenza-naive ferrets are infected or vaccinated with one of a set of reference influenza strains (e.g., current vaccine strains), and their post-exposure sera are tested against candidate strains for the next vaccine. The extent to which these sera cross-react or neutralize candidate strains is taken as a measure of their immune escape or antigenic distance. These experimental measures of immune escape, alongside other estimates of variant growth rates and sequence-based fitness models, are used to anticipate the dominant clade and need for vaccine updates. In the past few years, escape from human sera has been considered too (e.g.,). An open question is whether more direct and representative estimates of population immunity could lead to better vaccine choices while potentially shedding light on the mechanisms of coevolution between the viral population and host immunity. In the past decade, large differences have occasionally appeared in the antigenic distances inferred from ferret compared to human sera. These differences might arise at the species level, although the antibody responses of ferrets and humans after their first influenza exposures appear roughly similar. A more likely explanation comes from observations of original antigenic sin, whereby individuals exposed to the same strain of influenza can mount antibody responses with different cross-reactivity profiles shaped by their distinct histories of exposure. These past infections and vaccinations lead to biases in which viral sites or epitopes antibodies recognize. Consequently, a mutation in one epitope might be antigenically important for some people (or ferrets) but not others. Since most influenza infections occur in people with preexisting immunity to influenza, and antibodies to influenza surface proteins contribute substantially to protection and transmission, accurate measures of population immunity may be useful in viral forecasting and vaccine strain selection. Using the 2017–2018 influenza season in North America as a case study, we characterized a cross-sectional, age-representative estimate of antibody-mediated immunity in an urban population and asked whether it could predict which of several circulating clades of H3N2 would dominate regionally in the next influenza season. Forecasting for vaccine strain selection often focuses on antigenic changes to the hemagglutinin (HA) surface protein, which vaccines attempt to match. We measured neutralizing antibody titers to the neuraminidase (NA) protein as well as to HA because antibodies to NA are also protective and should thus affect clade fitness. We found large differences in the expected susceptibility of the population to different clades’ HA and NA, and these differences in susceptibility predicted clade dominance. They also partially predicted the relative attack rates of clades by age. We furthermore quantified the heterogeneity in neutralizing titers in the population, finding patterns consistent with age-associated epitope targeting. Although data from a single timepoint cannot fully elucidate the role of population immunity in clade evolution, our results demonstrate for the first time how such measures can improve on traditional approaches.","Serological data Sera from 489 individuals were collected between May and August of 2017 from the Children’s Hospital of Philadelphia (1- to 17-year-olds) and from the University of Pennsylvania Health care system via Penn BioBank (18- to 90-y-olds), as reported in. Serum samples from children were leftover samples originally collected for lead testing and were de-identified for use in this study. We do not have access to clinical characteristics of the donors. The Penn BioBank routinely collects serum samples from individuals visiting the University of Pennsylvania Health care system. We did not include samples collected by the Penn BioBank from donors who had a pregnancy reported during the last 9 months, who had a medical history of cancer or organ transplantation, or who had a reported infectious disease within the previous 28 days. The study complied with all relevant ethical regulations and was approved by the Institutional Review Board of the University of Pennsylvania. Leftover de-identified samples collected at CHOP were considered exempt from human research (exemption 4) since the samples were leftover discarded samples that were completely de-identified before our research team received them. We performed foci reduction neutralization tests (FRNT) on 437 individuals’ sera using the 8 HA reference viruses (3C.3a, 3C.2a, 3C.2a1–1, 3C.2a1–2, 3C.2a1–3, 3C.2a2–1, 3C.2a2–2 and 3C.2a3), and enzyme-linked lectin assays (ELLA) on 352 individuals using the two NA reference viruses (3C2.A (NA) and 3C.2a2–2 (NA)) as described in. Full experimental details can be found in. Briefly, for FRNT, we treated serum samples with receptor-destroying enzyme, serially diluted them twofold, incubated them with virus at a concentration of ≈ 300 focus-forming units for 1 h at room temperature, and added the mixture to confluent MDCK-SIAT1 cell monolayers in a 96-well plate. We incubated the cells with the virus-serum mixture in 5% CO2 for 1h at 37 °C, then washed them with Minimal Essential Media (MEM). After additional steps, we imaged the plates to quantify foci using an ELISpot reader. We report FRNT titers as the reciprocal of the highest dilution of sera that reduced the number of foci by at least 90% relative to control wells with no serum. We assigned a titer of 10 to serum samples that failed to achieve a 90% reduction at the smallest dilution (1:20). For ELLA, we incubated virus and diluted heat-inactivated serum overnight at 37 °C in microtiter 96-well plates coated with fetuin diluted in coating solution, washed them and performed additional steps before reading the plates at an OD of 450 nm using a microplate reader. We report ELLA titers as the reciprocal of the highest dilution of sera that reduced the OD value by at least 50%, relative to control wells with no serum and after background subtraction. We assigned a titer of 10 to serum samples that did not show at least 50% OD reduction at the smallest dilution (1:20). There were no significant titer differences between batches. We visualized titers to different reference strains by age using locally estimated scatterplot smoothing (LOESS) curves with a smoothing parameter  and degree = 2. Viruses We generated all viruses using reverse genetics, as described in. Briefly, we cloned HA and NA genes for the reference strains into the pHW2000 reverse-genetics plasmid along with internal genes from A/Puerto Rico/8/1934. For viruses used in the FRNT assay, we used the NA gene of A/Colorado/15/2014. For viruses used in ELLA, we used an H6 gene from A/turkey/Massachusetts/3740/1965 and the NA gene from either A/Colorado/15/2014 (representing clade 3C.2a) or A/Pennsylvania/49/2018 (representing 3C.2a2–2). We transfected the plasmids into co-cultures of 293T and Madin-Darby Canine Kidney (MDCK)-SIA1 cells, harvesting supernatants 3 days after transfection and storing them at −80 °C. We sequenced HA and NA genes to confirm that no additional mutations arose during transfection. Genealogy of H3N2 and clade-specific amino acid substitutions Prior to our analyses, we downloaded all available H3N2 HA and NA sequences from the 2012–13 season through the 2017–18 season from GISAID (accessed in 01/10/2022). Sequences were aligned using MAFFT 7.310. We downsampled sequences to construct the phylogeny. From the 2012–13 through the 2015–16 season, we sampled 20 sequences per season. For the 2016–17 and 2017–18 season, 100 sequences were sampled per season. The GISAID accession IDs and metadata of the sequences used for the analysis are available in the Supporting Information. We used BEAST 2.6.6 to reconstruct the genealogy with a HKY substitution model with a four-category gamma site model with 4 and a log normal relaxed clock. A coalescent Bayesian Skyline tree was used for the prior. We ran the chain for 50 million steps and saved every 1000 trees, using 5 million steps as burn-in. The maximum clade credibility tree was obtained using TreeAnnotator 2.6.6 version. To visualize the tree, we used the R package ggtree 3.0.4. The trees were colored by clade. For the genealogy of the 2016–17 season, only tips of sequences collected in North America during the 2016–17 season were shown; these circled tips are colored according to their assigned clade. For sequences collected in other areas or seasons, only branches were shown. Similarly, for the genealogy of the 2017–18 season, only sequences collected in North America in that season are shown as colored circles. Sequence samples were assigned to reference viruses according to reference virus-specific mutations at segregating sites, shown in Fig 1B. Here, sequences were assigned to each reference virus rather than the subclade represented by each reference strain. This is because sequences with 171K, 121K, and 135K, such as reference strain 3C.2a1–3, occur multiple times in clade 3C.2a1, and thus these sequences do not belong to any one subclade of 3C.2a1. Additionally, within a subclade, mutations at segregating sites occur so that a sequence in the same clade as a reference virus may not share the same genetic characteristics. Due to frequent mutation at residue 142 across most of clades, we allowed residue 142 to have any amino acid across most of clades, except for clade 3C.2a2, which has a clade-specific 142K substitution. We confirmed that all the sequences assigned to a reference virus fall in the same subclade as the reference virus. Clade-specific substitutions were colored by epitope on the H3 structure using PyMOL version 2.3.3. Inferring relative susceptibility from titers We defined the relative susceptibility to a strain as the fraction of the population with titers to that strain below some threshold (here, initially 40 for HA and 80 for NA). To estimate this fraction for the U.S. population, we first computed the fraction below the threshold in different age bins in our sample. We then computed a weighted sum in which the weights were the projected fractions of the U.S. population in each age bin in 2017. This adjustment was necessary to obtain a representative estimate of immunity in the overall population, since sample availability varied by age. Suppose  is the fraction of the overall U.S. population susceptible to strain  and  is the fraction of serum samples in age bin  with titers to  below the threshold. Then  where  is the projected fraction of the U.S. population in age bin  in 2017. We started with age bins at the resolution of one year using data from. When there were fewer than eight titer measurements for a year of age, that age was grouped with the next year of age to form a larger bin, and so on until the bin contained at least eight titer measurements. To estimate the susceptible fraction for a particular age group in the U.S. population, we simply computed the sum above across age bins contained in that larger group, divided by the fraction of the U.S. population in those bins combined. For instance, to calculate the susceptible fraction among 5–17 year-olds in the U.S., we used  We found that using alternate titer thresholds for HA (Figs S6, S7, and S8) and NA (Figs S12 and S13) resulted in consistent relative susceptibilities across strains. We alternatively measured relative susceptibility by the geometric mean titer (GMT). The GMT was weighted analogously by the population fraction of each age bin. Because lower GMTs correspond to higher susceptibility, we used a reverse scale when showing the relative susceptibility by GMT. To test for meaningful differences in relative susceptibilities, we bootstrapped individuals to determine if the difference in inferred relative susceptibilities between two viruses was significantly greater than zero. For each age bin, individuals were resampled 1000 times with replacement, and the fraction of individuals susceptible to each virus was calculated. For a given pair of viruses, we defined the relative susceptibility difference observed in the data as . The bootstrap value of , was obtained 1000 times by resampling individuals. Then we obtained the null distribution of  and calculated the probability () of observing  or a greater value under this null distribution. If p < 0.05 (with no correction for multiple testing), the relative susceptibility difference is significantly greater than zero, i.e., susceptibility to the first virus significantly exceeded that to the second virus. For a given virus, we perform this comparison against all other viruses and counted the number of significant results. The more significant results, the lower the rank (closer to 1) of the relative susceptibility to a virus. We used the same approach and significance level for all other bootstrapping analyses. Frequencies of subclades To calculate the frequencies of different subclades, we downloaded sequences from the 2016–17 to the 2018–19 seasons available on GISAID on January 10, 2022, and assigned sequences to each subclade using the same method as was used to construct the genealogy. Because there were few sequences from Philadelphia, we calculated subclade frequencies in three different ways, using sequences collected from North America, United States, or the northeastern US. We considered Region 1, Region 2, and Region 3 of the U.S. Outpatient Influenza-like Illness Surveillance Network (ILINet,) as the Northeastern U.S. states. These states are Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont, New Jersey, New York, Delaware, the District of Columbia, Maryland, Pennsylvania, Virginia, and West Virginia. Region 2 of ILINet includes Puerto Rico and the Virgin Islands, but we excluded them from the analysis of the northeastern U.S. For estimates derived from North American sequences, we used 4488 and 5425 sequences from the 2016–17 and 2017–18 seasons, respectively. For the US, 3707 and 3782 sequences were used. For the northeastern US, 782 and 676 sequences were used. The GISAID accession IDs and metadata of the sequences used for the analysis are available in the Supporting Information. Correlations between titers to different strains For each age group and pair of viruses, we calculated Spearman’s  using the cor function in R. To account for the interval censoring of titer data and the presence of a lower limit of detection, we randomly imputed continuous titer values and calculated the average regression coefficient across 1000 imputations. Titers below the lower limit of detection (1:20) were uniformly sampled between the lowest possible titer (1:1, indicating no dilution) and 1:20. Titers at or above the limit of detection were randomly sampled from the interval between the recorded titer and the next dilution (e.g., a recorded titer of 1:160 was imputed a value between 1:160 and 1:320, with uniform probability). For this analysis, we excluded individuals with undetectable titers to all strains. We excluded 45/112 (40%) of <5 y olds, 10/164 (6%) of 5–17 y olds, 31/89 (35%) of 18–44 y olds, 28/62 (45%) of 45–64 y olds, and 22/62 (36%) of 65–90 y olds. For each virus pair, we tested the difference in correlation coefficients between the youngest age group and each other age group using the same bootstrapping procedure we used to test for differences in susceptibility among strains within an age group (Fig S16). We also used bootstrapping to evaluate differences in correlation coefficients between viral pairs within an age group. For each virus pair, we did a series of bootstrap tests comparing the pair’s correlation coefficient with the coefficient for each of the other pairs. Then, for each virus pair, the number of tests in which the pair’s correlation was significantly weaker than that of other pairs within the group was counted. In each age group, there are 28 virus pairs whose correlation coefficient was calculated. One of the pairs, for example, is 3C.3a and 3C.2a, and this pair’s correlation coefficient is compared with the other 27 correlation coefficients of other virus pairs. The 3C.3a v. 3C.2a pair’s correlation was weaker than 15 other pairs’ correlations. This number of tests in which the pair’s correlation was significantly weaker than other pairs within the group is shown as the color intensity of the heat map of Fig S18. For these boostrapping procedures, we used a significance level of 95% without correction for multiple testing, randomly imputing continuous titers once for each bootstrap replicate.","Human sera from the summer of 2017 poorly neutralize the clade that dominated in North America in the next influenza season We investigated whether neutralizing antibody titers to HA and NA from H3N2 clades circulating in early 2017 could predict the dominant (most frequent) clade in the next influenza season. Antibodies to HA can protect against infection, and we expected that the clade to which the largest fraction of the population had poorly neutralizing anti-HA titers would be most successful. This expectation implicitly assumes that exposure rates, other factors affecting susceptibility, and the average infectiousness or transmissibility of an infected person do not differ starkly between age groups; it also assumes that antibody-mediated protection derives primarily from neutralization and not Fc-mediated effector functions, or that the two are well correlated. Antibody neutralization was measured by the focus reduction neutralization test (FRNT) for anti-hemagglutinin antibodies and enzyme-linked lectin assay (ELLA) for the anti-neuraminidase antibodies, and these antibodies levels were assumed proxies for protection. Correlates of protection have not been established for FRNT-derived titers, but because microneutralization titers correlate well with hemagglutination inhibition (HAI), and a 1:40 HAI titer is traditionally associated with a 50% reduction in infection risk, we initially assumed a 1:40 FRNT titer corresponds to a 50% chance of infection, testing other assumptions in sensitivity analyses. We looked at the fraction of the population below this cutoff for each clade to obtain the expected relative susceptibility and ranked clades by this measure. Using a cutoff avoids overestimating protection that might arise from especially high titers in a subset of recently infected individuals, but for robustness, we also estimated the relative susceptibility according to the geometric mean titer (GMT) to each clade, with lower GMT implying higher susceptibility. With both measures, the population-level susceptibility was estimated by weighting the susceptibility of different age groups according to their proportion in the population (Methods). Protective thresholds for ELLA NA titers have not yet been established. We initially assumed 1:80 to be the 50% protective titer and later explored other assumptions. We collected serum samples from May to August of 2017 from the University of Pennsylvania BioBank and Children’s Hospital of Philadelphia (Methods; Fig S1). Samples from children were primarily obtained for lead testing. Adults with certain health conditions were excluded. Since we knew the age of each serum donor, we were able to adjust our estimates of population immunity to reflect the age distribution of the United States population (Methods). However, no information on vaccination status was available, and therefore we could not adjust our estimates to reflect vaccination status in the general population. We measured neutralizing titers to the 8 HA and 2 NA representing common current or recently circulating H3N2 clades (Fig 1A left for HA and Fig S2A left for NA). To represent the circulating diversity of H3N2 viruses, we identified major circulating clades by inspecting the nextstrain H3N2 genealogy in the early summer of 2017 (note that the genealogy in Fig 1A was constructed later with data up to January 2022). Two distinct clades, 3C.2a and 3C.3a, which last shared a common ancestor in 2012, circulated globally. These clades differed by amino acid substitutions in epitopes A and B (Fig 1B, C) and in non-epitope sites. Clade 3C.2a had gained a potential glycosylation site at epitope B (K160T; H3 numbering used throughout) and had lost a glycosylation motif at epitope A (N144S). Clade 3C.3a had lost a different glycosylation site in epitope A (T128A) (Fig 1B, C). We chose the vaccine strain A/Hong Kong/4801/2014 to represent clade 3C.2a and the vaccine strain A/Switzerland/9715293/2013 to represent 3C.3a, both close to the root of their respective clades. In addition to those two major clades, we further split clade 3C.2a into subclades 3C.2a1, 3C.2a2, and 3C.2a3. We constructed representative sequences for these subclades by introducing mutations into the sequence of the 3C.2a reference strain. For subclades 3C.2a1, 3C.2a2, and 3C.2a3, we constructed 3, 2, and 1 reference viruses, respectively, each carrying subclade-specific nonsynonymous substitutions and (for 3C.2a1 and 3C.2a2) potentially important amino acid polymorphisms within the subclade. Each subclade contained an epitope A substitution compared to the 3C.2a reference strain (Fig 1B, C). Notably, one reference virus for clade 3C.2a1 (virus 3C.2a1–3) had the T135K mutation, which removes a glycosylation motif in epitope A. Except for 3C.2a1–3, all representative strains had identical sequences to one or more naturally occurring strains (Table S1). Only the 3c.2a reference virus was represented among candidate vaccine viruses for the 2017–2018 season (identical to A/Hong Kong/4801/2014, A/Hawaii/47/2014, A/Victoria/673/2014 and A/Norway/2178/2014). Once we had determined the sequences of the representative strains, we constructed the viruses by reverse genetics (Methods). We used the same set of reference viruses for all serum donors. For all reference viruses, an undetectable HA titer (titer of 1:10) was the most common HA titer in all age groups except children 5–17 years old (Figs 2A, S3). Most people over 4 years old had detectable NA titers (1:≥ 20) (Figs 3A, S4). Even though detectable antibody to H3N2 HA or NA is expected among older children and adults, who have been infected and possibly vaccinated with H3N2, surprisingly large variation was observed among individuals of the same age (Figs 2A, 3A). These are likely genuine differences in titer, as technical replicates had high agreement. The population-level relative susceptibility inferred using the 1:40 protective cutoff in HA titer was highest to the 3C.2a2 subclade, specifically the group of viruses with 261Q in epitope E (3C.2a2–2 reference strain; the susceptibility to 3C.2a2–2 is higher than the susceptibility to 3C.2a2–1 and 3C.3a, both bootstrap p < 0.001), followed by the rest of the 3C.2a2 subclade (3C.2a2–1 reference strain; the susceptibility to 3C.2a2–1 is higher than the susceptibility to 3C.2a1–1, bootstrap p < 0.05) and the 3C.3a clade (p < 0.01 for the same test; Fig 2B, left panel) (Methods). Not only was the average population susceptibility highest to 3C.2a2–2, but susceptibility to it was also the highest or the second highest among the reference strains for all age groups except 1–4 year-olds. Using GMTs or alternative titer cutoffs also suggested the susceptibility was highest to the 3C.2a2–2 reference strain followed by 3C.2a2–1 (Figs S5–S8). The greatest protection or lowest susceptibility in the population by both measures was to strains of the 3C.2a1 subclade with 135K in epitope A and 121K in epitope D (reference strain 3C.2a1–3) and subclade 3C.2a3 (reference strain 3C.2a3). Consistent with simple predictions, clade 3C.2a2 dominated in North America in the 2017–18 season (Fig 1A, right panel; Fig 1E; Fig 2B, gray arrows), followed by 3C.3a. To assess dominance, influenza sequences were downloaded from GISAID. We assigned 9913 sequences collected in North America during the 2016–2017 and 2017–2018 influenza seasons to reference viruses based on their genetic similarity at segregating sites and found that the frequency of sequences genetically similar to reference strain 3C.2a2–2 in clade 3C.2a2 increased from 21% in the 2016–2017 season to 85% in the 2017–2018 season (Fig 2B; Fig S9). Clade 3C.3a increased from 6% to 8% over that period. We did not find a perfect correlation between the rank measured by inferred relative susceptibilities and rank by relative growth: despite having higher estimated susceptibility than subclade 3C.2a1 (3C.2a1–3), subclade 3C.2a1 (3C.2a1–2) experienced a more severe decline. Although the available sequences are not generated from any kind of systematic surveillance program and thus may not accurately reflect relative prevalence, trends were stable regionally (Fig S9A). The results suggest that population-average anti-HA neutralizing titers reflect strain fitness, but that other factors may be relevant for detailed predictions. We next measured antibody responses to NA reference strains representing the NAs of clades 3C.2a and 3C.2a2 (“3C.2a (NA)” and “3C.2a2–2 (NA)”, respectively) (Fig 3A). The two reference viruses differ by 7 amino acid substitutions in the NA head: 176, 245, 247, 329, 334, 339, and 386. We first estimated population-level relative susceptibilities to the two clades using a 1:80 protective cutoff (Fig 3B, left panel). Similar to our findings for HA, serological responses to NA indicated higher susceptibility to 3C.2a2–2 (NA) than to 3C.2a (NA) across all age groups, consistent with a positive correlation between HA and NA titers across all age groups and especially in children (Fig S10). Using GMT or alternative NA titer thresholds also suggested higher susceptibility to 3C.2a2–2 (Figs S11–S13). Because only two NA reference strains were used, we cannot conclude if anti-NA titers would have predicted clades’ rank frequencies as accurately or perhaps better than titers to HA, but they are generally consistent with higher susceptibility to the 3C.2a2 clade compared to the ancestral 3C.2a. Age groups differ in their susceptibility to and relative attack rates with different H3N2 clades Because age-specific patterns of antibody titers have been associated with age-specific infection risk, we estimated relative susceptibility to each clade within each age group and measured correlations with their estimated relative clade-specific infection rates in the 2017–2018 influenza season. Age groups differed slightly in their expected susceptibilities to different clades of H3N2 (Fig 2B, right panel). Assessed by their anti-HA titers, children 1 to 4 years old appear equally susceptible to all reference viruses. The anti-HA titers of older children and adults showed heightened susceptibility to the 3C.2a2 clade: titers from 5- to 17-year-olds indicated the highest susceptibility to the basal 3C.2a2 clade (reference strain 3C.2a2–1) followed closely by reference strain 3C.2a2–2, the 3C.2a2 subclade with the R261Q substitution. People aged 18–64y had pronounced susceptibility to reference strain 3C.2a2–2 compared to other clades. Because 3C.2a2–2 differs from 3C.2a2–1 by a single amino acid substitution (R261Q), these results suggest that many HA antibodies in adults target the mutated site. All age groups with previous influenza experience (≥ 5 y) were least susceptible to clades 3C.2a1 and 3C.3a (reference strains 3C.2a1–3 and 3C.2a3, respectively). Interestingly, 5- to 17-year-olds were least susceptible to 3C.3a, while adults were relatively susceptible to 3C.3a. We also found that children 1 to 4 years old had comparable susceptibility to the two clades of NA, and all older age groups demonstrated greater susceptibility to the 3C.2a2 clade (3C.2a2–2 (NA)) (Fig 3B, right panel). We evaluated whether the age-associated trends in relative susceptibilities to different clades in the summer of 2017 were mirrored in their relative rate of infection with each clade in the 2017–2018 influenza season. Due to lack of systematic surveillance, unbiased estimates of attack rates by clade do not exist for this population. We nonetheless examined the ages associated with sequences uploaded into GISAID to approximate the proportion of infections caused by each clade in each age group. Because the 3C.2a2 clade dominated in the 2017–2018 season and all but the youngest age groups showed particularly high susceptibility to this clade, we expected clade 3C.2a2 to be the most frequent within each age group. This is what we found (Fig 4, Figs S14–S15). However, we observed that children < 5 y old, who seemed approximately equally susceptible to all clades by HA and NA, had a relatively lower proportion of 3C.2a2 infections compared to adults (chi-square test, p < 0.001). Children 5–17 y old, who were only slightly more susceptible to 3C.2a2 than other clades, also had a lower proportion of 3C.2a2 infections compared to adults (p < 0.001). Consistent with our observation that 18- to 64-year-olds were disproportionately susceptible to clade 3C.2a2, the age distribution of that clade was slightly more skewed toward adults compared to non-3C.2a2 clades, which were more common in children (Fig 4). Correlations between titers to different strains vary by age, suggesting age-associated differences in epitope targeting We next investigated the correlations in titers to different clades (Fig 5A): Do individuals with high titers to 3C.3a tend also to have high titers to 3C.2a2, for instance? Closely correlated titers to related viruses suggest that individuals might target epitopes conserved among them, which could underlie differences in neutralizing titers between age groups. (High titers to multiple strains could also indicate recent infections or immunizations with each of those strains and responses to their non-shared epitopes, although H3N2 infections typically occur at least several years apart and are less frequent in adults compared to children.) Aside from providing insight into the specificity of the antibody response, understanding the structure of titers within the population might lead to improved estimates of selective pressures on viruses. For instance, weakly correlated titers to different clades suggest a population with more heterogeneous immunity, which can affect viral coexistence, vaccination thresholds, and other dynamics. After removing individuals with undetectable titers to all strains from the analysis, we found that the strength of correlation differed by age group and virus pair. In general, titers to all the reference viruses but 3C.3a were highly correlated in children and less correlated in older ages (Figs 5B, S16). This suggests that children target epitopes common to many reference viruses or have been infected by close relatives of each, whereas older age groups target epitopes conserved among only a subset. Results hold when age groups are chosen to span an equal number of years (Fig S17), showing that the weaker correlations in adults 18–44 y, 45–64 y, and 65–90 y are not due simply to the groups’ relative sizes or the diversity of childhood exposures represented in them. In contrast to children and younger adults, 45- to 90-year olds have their highest titers and strongest titer correlations to a distinct subset of reference strains (3C.2a, 3C.2a3, 3C.2a1–1, and 3C.2a1–2; Figs 2 and 5) that share many epitope A residues not conserved in the other strains (e.g. 128T, 131T, 135T, 138A, 142R), suggesting antibody responses focused on this antigenic region. In all age groups, titers to 3C.3a were least correlated with titers to other viruses (Fig S18). This might be explained by reduced exposure to 3C.3a viruses, especially in adults (Fig S3), and/or the targeting of sites on 3C.2a clade viruses that are not shared with 3C.3a (e.g., sites 128, 138, 142, and 144 in epitope A, the last potentially masked by a glycan in 3C.3a).","Current approaches for forecasting influenza and mapping its antigenic evolution rely on antigenic distance measurements that do not always reflect immunity in the human population. Understanding the size of the difference and how much it matters would require analyzing discrepancies between antibody titers and traditional ferret-based measurements over multiple years from representative cross-sectional surveys in different populations. Multiple years of sampling could also resolve the subpopulations and measures needed to assess immune selective pressures and compare them to other factors influencing fitness and growth rates. Here, as a proof of principle, we demonstrate how human sera can reveal differences in expected susceptibility to circulating HA clades that predict the clade circulating in the following season. The sera also demonstrate high heterogeneity in neutralizing titers by age. The consequences of these differences remain unclear, but they partly predict the relative susceptibility of different age groups to different clades in 2017–2018. These findings might have been useful before the 2017–2018 influenza season in the United States. Although impractical to update the vaccine strain so near the start of the season, efforts to increase coverage to offset the expected low vaccine effectiveness might have blunted the season’s unusual severity, especially in the most vulnerable age groups. The 2017–2018 season caused approximately 41 million illnesses and 52,000 deaths. The low effectiveness of the vaccine against H3N2 that season was attributed to egg adaptations that created a mismatch to circulating strains. The H3N2 component of the vaccine, A/Hong Kong/4801/2014 (a basal 3C.2a strain), had been unchanged from the previous season because no clear indication of antigenic evolution was apparent by early 2017, when vaccine strain composition for the Northern Hemisphere was decided; the 3C.2a2 clade was nonetheless noted to be growing quickly. Over 90% of 3C.2a2 strains isolated from the United States in the 2017–2018 season were described as well inhibited by ferret antisera raised against the cell-propagated reference virus for A/Hong Kong/4801/2014 (A/Michigan/15/2014), and in early 2018, the H3N2 vaccine component was updated only to avoid egg adaptations, not because antigenic change had been detected. (Notably, a later investigation of H3N2 viruses circulating in Japan in 2017–2018 did detect antigenic differences between 3C.2a and 3C.2a2 strains using ferret antisera.) Our study shows that antigenic changes in fact were detectable in human sera by at least the summer of 2017, and they could predict the dominance of 3C.2a2 and the populations more susceptible to infection. Consistent with this prediction, Ursin et al. found that individuals testing positive for H3N2 in the 2017–2018 season had consistently lower serum neutralization titers to the 3C.2a2 clade than those testing negative—with no differences between the two groups’ titers to cell-grown A/Hong Kong/4801/2014—underscoring the consequences of these neutralization differences for protection and potentially transmission. While our results suggest that the R261Q mutation contributed to 3C.2a2’s success, smaller clades that acquired the same mutation independently never rose to high frequencies. Measurements of population immunity could be substantially more efficient and useful for forecasting if we understood exactly what to measure and in whom. Antibody titers to HA have been an established correlate of protection for half a century, and antibodies to NA for approximately a decade. The generally good concordance between hemagglutination inhibition assays and microneutralization suggest neutralization is a decent surrogate, but it is unclear how much protection each immune response confers in different people and whether measures of neutralization, total binding, antibody-dependent cellular cytotoxicity or phagocytosis activity, and/or potentially other B- or T-cell or innate immune measures could improve estimates of relative susceptibility. Correlates likely vary in quality over time and between age groups: large discrepancies between binding antibody titers and neutralization or protection have been reported and are associated with priming to other strains. Furthermore, immune measures associated with reduced transmission rather than simply protection against disease would provide more accurate estimates of viral fitness. It might also be important to weight immunity in different subpopulations differently: for instance, an infected child might be more likely to transmit than an infected adult. These considerations would affect the need to sample particular populations, such as unvaccinated members of certain age groups. Over larger geographic scales, samples from typical “source” populations may be better predictors or provide a longer lead time than populations that export fewer strains. Evolutionary forecasts, including for vaccine strain selection, might benefit from such improved measures of population immunity but would require additional modifications. Clonal interference with multiple mutations can prevent any one clade from dominating a particular season and complicate the relationship between susceptibility and clade frequency. Detailed clade frequency predictions might require incorporating estimates of population immunity into formal, potentially spatially explicit models that also consider other consequences of mutations on fitness. For influenza, estimates of viral fitness based purely on sequence data or on ferret-derived antigenic distances have modest success in out-of-sample forecasts, and models based on human population immunity have shown promise in predicting the frequencies of SARS-CoV-2 variants. Using clade frequencies to predict absolute disease burden would be a further challenge. To inform vaccine strain selection, human sera would need to be assayed much earlier relative to the influenza season than we did here. Perhaps most important, given variation in the immunogenicity and cross-immunity of different strains, it might not be optimal to be vaccinated against the strain forecasted to dominate. Our data revealed variation in antibody titers between age groups that are broadly consistent with influenza’s epidemiology but lack precise explanation. Children over five years old had the highest geometric mean titers to all strains. This is consistent with the high attack rates in school-age children and other studies that report young children having the high titers to recent strains. Children also had relatively high vaccination coverage (approximately 59% in the 2016–2017 season in children ≥ 6 mos.) compared to younger adults. These two factors might interact, since recent infection can boost vaccine immunogenicity. The relatively high vaccination coverage in the oldest age group (approximately 65% in adults ≥ 65 y) might explain their higher titers compared to middle-aged and younger adults. Surprisingly, most middle-aged individuals had no detectable neutralizing antibodies to the HAs of circulating H3N2 clades. These results suggest antibodies to HA may be a poor correlate of protection in this age group and complement other reports of their discrepant anti-HA titers. We also observed that unlike children, adults had highly correlated titers to a subset of 3C.2a strains suggestive of antibody responses focused on epitope A. Consistent with this observation, Welsh et al. recently applied deep mutational scanning to H3N2 and sera isolated in 2020. They found that compared to children, adults derived a larger fraction of their neutralizing response to epitope A, which had been immunodominant during the early 1990s. Although not presented here, we fitted dozens of generalized linear mixed models to attempt to explain individuals’ titers to these strains as a function of potential recent infections, vaccinations, early infections with strains with homologous epitopes, and individual-specific biases in the contributions of different epitopes to titers. These models were inconclusive, suggesting a need for more careful study of how a person’s antibody titers change over time in response to exposures, and potentially with some deconvolution of the response to specific epitopes. Our results demonstrate the feasibility of detecting significant differences in neutralizing titers to different H3N2 clades in a convenience sample of few hundred sera. This approach could entail substantial improvements over the use of ferret sera, which do not capture the immune history and heterogeneity in the human population. Testing improved sampling protocols and forecasting models, which would be facilitated by the existence of global blood banks and common standards, might yield rapid advances in forecasting not only the dominant clade but also potentially the dominant subtype, and ideally at longer lead times than shown here. If linked to other forms of surveillance, cross-sectional sera might also help predict season severity and attack rates by age, as suggested here. The same samples and approximations of fitness might also predict the dynamics of other pathogens.",10.1101/2023.10.26.23297569
PMC10081185,37034801,Micro to macro scale analysis of the intact human renal arterial tree with Synchrotron Tomography,"Background: The kidney vasculature is exquisitely structured to orchestrate renal function. Structural profiling of the vasculature in intact rodent kidneys, has provided insights into renal haemodynamics and oxygenation, but has never been extended to the human kidney beyond a few vascular generations. We hypothesised that synchrotron-based imaging of a human kidney would enable assessment of vasculature across the whole organ. Methods: An intact kidney from a 63-year-old male was scanned using hierarchical phase-contrast tomography (HiP-CT), followed by semi-automated vessel segmentation and quantitative analysis. These data were compared to published micro-CT data of whole rat kidney. Results: The intact human kidney vascular network was imaged with HiP-CT at 25 μm voxels, representing a 20-fold increase in resolution compared to clinical CT scanners. Our comparative quantitative analysis revealed the number of vessel generations, vascular asymmetry and a structural organisation optimised for minimal resistance to flow, are conserved between species, whereas the normalised radii are not. We further demonstrate regional heterogeneity in vessel geometry between renal cortex, medulla, and hilum, showing how the distance between vessels provides a structural basis for renal oxygenation and hypoxia. Conclusions: Through the application of HiP-CT, we have provided the first quantification of the human renal arterial network, with a resolution comparable to that of light microscopy yet at a scale several orders of magnitude larger than that of a renal punch biopsy. Our findings bridge anatomical scales, profiling blood vessels across the intact human kidney, with implications for renal physiology, biophysical modelling, and tissue engineering.","The kidney receives up to 20% of cardiac output, carried into the organ by arteries branching off the abdominal aorta and entering the renal hilum. Once within the kidney, the renal arteries divide hierarchically, into interlobar and interlobular vessels enroute to the renal cortex and, within the cortex, as arcuate arteries and efferent arterioles. This network perfuses specialised glomerular capillaries for plasma ultrafiltration, before peritubular capillaries and vasa recta facilitate dynamic solute exchange. Thereafter, venous return follows the arterial supply out of the organ. Structural and molecular changes to the renal vasculature accompanied by alterations in tissue oxygenation occur in diabetes, hypertension, transplant rejection and chronic kidney disease in both animal models and patients. Studying renal vascular structure and heterogeneities thus has implications for understanding the basis of renal function in health and disease. Advances in imaging modalities such as micro-computed tomography (μCT), magnetic resonance imaging (MRI), lightsheet microscopy (LSM), and photoacoustic imaging have generated detailed analyses of blood vessels in intact mammalian kidneys, particularly rodents. Although LSM has been used to quantify cortical vessel diameters in humans, it has not captured the intact renal vascular network of the human kidney without tissue subsampling. Here, we map the intact arterial network of an entire human kidney using Hierarchical Phase-contrast Tomography (HiP-CT). Previously, we quantified glomerular morphology across cubic centimetres of intact human kidney with HiP-CT and now we extend HiP-CT analysis to the topology of the intact human renal arterial network. We compare topological metrics between rodent and human kidneys and identify spatial heterogeneities that may contribute to oxygenation gradients within the intact human organ.","Sample preparation An intact human right kidney was obtained from a 63-year-old male (cause of death: pancreatic cancer) who consented to body donation to the Laboratoire d’Anatomie des Alpes Françaises, (Grenoble, France) before death. Post-mortem study was conducted according to Quality Appraisal for Cadaveric Studies scale recommendations. The body was embalmed by injecting 4500 mL of 1.15% formalin in lanolin followed by 1.44% formalin into the right carotid artery, before storage at 3.6 °C. During evisceration of the kidney, vessels were exposed, and surrounding fat and connective tissue removed. The kidney was post-fixed in 4% neutral-buffered formaldehyde at room temperature for one week. The kidney was then dehydrated through an ethanol gradient over 9 days to a final equilibrium of 70%. The volume of each dehydration solution was four-fold greater than the volume of the organ and during dehydration, the solution was degassed using a diaphragm vacuum pump (Vacuubrand, MV2, 1.9m3/h) to remove excess dissolved gas. The dehydrated kidney was transferred to a polyethylene terephthalate jar where it was physically stabilised using a crushed agar-agar ethanol mixture, and then imaged. Scanning, image acquisition and reconstruction Imaging was performed on the BM05 beamline at the ESRF following the HiP-CT protocol. Initially the whole kidney was imaged at 25 μm/voxel (edge length). Volumes of interest within the same kidney were also imaged at 6.5 and 2.6 μm/voxel. Tomographic reconstruction was performed using PyHST2 software. Briefly, a filtered back-projection algorithm with single-distance phase retrieval coupled to an unsharp mask filter was applied to the collected radiographs. The reconstructed volumes at all three scales were binned (average binning) to 50, 13, and 5.2 μm3/voxel, respectively, to reduce computational load for subsequent image segmentation and quantification. (Scanning parameters can be found in Supplementary Table S1) Image filtering, enhancement, and segmentation Prior to semi-automated segmentation, images were filtered to enhance blood vessel wall contrast. A 3D median filter (iterative and 26-adjacent analysis was used to reduce image noise; image normalisation was performed using background detection correction (Amira v2021.1; type: B-spline, voxel dimensions: 91, 130, 227). Semi-automated segmentation of the arterial networks was performed in Amira v2021.1 using a region growing tool. In this method, the user selects seed locations and an intensity threshold, then any pixels within the connected neighbourhood of the seed point and within the threshold are added to the region, and this continues in an iterative fashion expanding the region. The annotator continues to select seed points and thresholds until the interior of all vessels are filled. This method was applied to segment the whole renal arterial network from the intact human kidney from the imaging data at 50 μm3/voxel, and portions of the same network in the 13 and 5.2 μm3/voxel datasets. The segmentations were proof read by a second independent annotator and segmentation between the initial and the second annotator were iterated until no further changes were found. Arteries and veins, were distinguished due to the thicker walls of arteries, and ultimate connectivity to the renal artery. Visualization and skeletonization To quantify the human kidney blood vasculature, the segmented 3D vascular network at 50 μm3/voxel was skeletonized using the centreline tree algorithm in Amira (tube parameters: slope = 6 and zeroval = 16). The resulting spatial graph describes the vascular network in terms of ‘points’, ‘segments’, and ‘vessels’ with definitions for each shown in Fig.S1A. A vessel was defined as the connector between a start and end point, which corresponds to either a branching point leading into another vessel branch or a terminal end where no further branches were detectable. Segments are assumed to be cylinders with a circular cross section joining points that discretise the vessel length. Morphological analysis Branching angle: the angle between the two daughter vessels from a common parent vessel, where the vessel vectors are considered to be the Euclidean path between the common branching point of the parent and the two daughters and the end points for each daughter vessel Tortuosity: the Euclidean distance between the start and end points of a vessel divided by the sum of all lengths of segments that make up that vessel, Radius: calculated for each vessel as the mean of all segment radii, or in the case of vessel partial collapse, the maximum segment radius or in the case of complete vessel collapse, an equivalent radius for the perimeter of vessel cross-section in the binary image. Length: the sum of all distances between points in a single vessel Branching ratio: the number of vessels of a given generation/order divided by the number vessels of the order or generation above. Adherence to Murray’s Law: this states that the cube of the parent vessel diameter should be equal to the sum of the cubed daughter vessel diameters, and was assessed by analysing the measurements of vessel diameters and branching points. Murray’s Law suggests that the blood flow in a network is distributed in a manner that minimizes resistance to flow when the diameters of the parent and daughter vessels are in a certain ratio. Inter-vessel distance, used to infer tissue perfusion and oxygenation across the human kidney due to its correlation with diffusion distance, was calculated as the distance of every tissue voxel from its nearest vessel voxel via a 3D distance transform applied to the binary vessel image. All codes for the calculation of the above metrics are provided in Supplementary Data. To assess vessel generations, two methods were applied, i) a ‘topological’ approach, starting from the renal artery as generation one, with each branching point increasing the generation number by one (Fig.1D); ii) the Strahler ordering system, where the terminal ends of the network are assigned as the first order apart from a single manually selected point which is designated as the root point (the renal artery in this case), iterating through the network from these end points, at every branch points, when two vessels with the same order intersect, the resulting vessel has an order one greater. Alternatively, if two vessels with different orders intersect, the higher generation of the two is given to the resulting vessel (Fig.1E). Morphological metrics of the network were calculated from the spatial graph as follows (see also Figure S1A):  These metrics can provide insights into the mechanisms underlying the formation and maintenance of the vascular network, as well as the functional implications of deviations from Murray’s Law. Additionally, the above metrics in our human kidney were compared to those of the rat kidney taken from Nordsletten et al. scanned at 20 μm3/voxel using a micro CT scanner, where a radiopaque silicone polymer was perfused to enhance contrast. Kidney Compartment Segmentation Segmentation of the compartments within the human kidney, including cortex, medulla and hilum, was performed in Dragonfly (version: 2021.3) using the segmentation wizard to train a 2D convolutional neural network (CNN). The final hyperparameters of the CNN are given in Table S2. Manual correction of the CNN output was performed in Amira (v2021.1), to provide the final compartment segmentation. These segmentations were used to compare the inter-vessel distance variation between the cortex, medulla, and hilum. Statistical analysis Statistical comparisons of length and radius between human and rat kidneys was performed in GraphPad Prism (version: 2021), and all graphs and plots were created with Origin 2021b. For statistical comparison of normalised vessel radius and length , a p value of less than 0.05 was considered statistically significant. Log of radius and length were plotted against Strahler generation for each of the human and rat datasets, enabling a linear least squares regression. A sum of squares F test was performed with the null hypothesis that a single set of global parameters for slope and intercept would fit vessel radius or vessel length for both the rat and human cases. In the case of vessel radius p < 0.0001; F (DFn, DFd) = 700.6 (2, 12). In the case of vessel length p = 0.4213; F (DFn, DFd) = 0.9299 (2, 12).","HiP-CT maps the entire blood vascular network in the intact human kidney Using HiP-CT (Fig. 1A), we imaged the intact kidney of a 63-year-old male organ donor at 25 μm3/voxel . After 3D reconstruction and pre-processing, we segmented all visible renal arteries/arterioles thus extracting the entire arterial network of the organ (Fig. 1B). By generating 3D renderings of the arterial network, we identified a segmental pattern of anterior, posterior, superior and inferior territories supplying the renal parenchyma (Supplementary Video 1). Each vascular territory (shown in different colours in Fig. 1C) had a corresponding renal arterial branch originating from the hilum which bifurcated before hierarchical branching towards the cortical parenchyma. The vascular dataset consisted of a spatial graph containing 5730 end or branching points, 303,595 points, and 5,718 vessels. Generational analysis of the network was applied to classify the vessels into established biological hierarchies. We resolved 24 topological generations, or eight Strahler generations, with an exponentially increasing branching ratio down to the eight generations (Fig. 1F). We mapped each generation to established anatomical groups: Strahler generations 7–8 (n = 17 vessels; mean radius = 1249 ± 797 μm) mapped to the branches of the renal artery entering the kidney hilum. Generations 5–6 comprise interlobar arteries (n = 160 vessels; mean radius = 320 ± 131 μm), and generations 2–4 arcuate arteries (n = 2672 vessels; mean radius = 90 ± 52 μm). Interlobular arteries fall within generations 1–3 (n = 5185 vessels; mean radius = 59 ± 27μm). The human kidney appeared to have a, lower number of generations than previously found in rat (8 in human as compared to 11 in rat). To understand this discrepancy, we examined selected areas of the human kidney at higher resolution. We segmented arterioles from selected regions of interest within the same human kidney scanned at 2.6 μm3/voxel. From these high-resolution images, we were able to segment a further three generations, corresponding to efferent and afferent arterioles as evidenced by the presence of glomeruli terminal ends of the arteriolar network (Fig.1G). Thus, we are able to verify that HiP-CT of the whole human kidney can resolve the renal arterial network down to the level of interlobular arteries, and use HiP-CTs higher local resolution, to verify that the number of branching generations in the human kidney matches that of the rat. Analysis of vascular network metrics in the human kidney reveals concordance with a rodent model organism Vascular network metrics provide a means for quantitative comparison between individuals, species or pathologies and are increasingly used for the simulation of oxygenation, drug delivery, or generating realistic vascular networks for in silico medical trials. These applications require metrics including branching angle, radius, tortuosity and length as inputs, and often assume adherence to e.g. Murray’s Law. To our knowledge, no such metrics exists for the human kidney, and thus we report them here (Fig. S1B–E, Table 1). Previously, computational models of mammalian renal blood flow have been derived from quantitative analysis of micro-CT images of the intact rat kidney vasculature until now, it is not been possible to assess how these analyses compare to the human organ. We therefore compared our human kidney HiP-CT data with those derived from micro-CT data of rat, relating normalised vessel metrics from each species at corresponding generations of the renal arterial network. The increasing trend in vessel radius with generation from interlobular arteries to major renal artery branches was similar between human and rat kidney (Fig. 2B), albeit there was a significantly lower normalised vessel radius at each generation in humans as compared to rat (p < 0.0001). Conversely, normalised vessel lengths were similar between human and rat kidney at each generation (Fig. 2C, p = 0.4213). Branching ratio provides an indication of network symmetry, with a perfectly symmetrical network having a branching ratio of 2. It is hypothesised that the symmetry or asymmetry of a blood vascular network is indicative of constraints imparted upon its architecture by metabolic demands. We found that the branching ratio of the human arterial network was 2.41 whilst the rat kidney is reported as 2.85,  indicating that both human and rat kidney possess slight branching asymmetry as do many other mammalian networks e.g. the rat lung (3.31) and human torso.  Murray’s law which prescribes the vessel diameter ratio for parent and child vessels that minimises resistance to blood flow, is an often used constraint in branching structure growth simulations. We demonstrate agreement with Murray’s Law (Fig.2D, r = 0.95, R2 = 0.758) for the human arterial renal network, as has previously been demonstrated for the rat. In summary, branching metrics and trends are conserved between the rodent and human kidney. However, certain metrics such as vessel radius vary; an important consideration when extrapolating or translating vascular analyses from rodent to human. Regional heterogeneity in vascular branching metrics provides a basis for local hypoxia within the kidney microenvironment Regional heterogeneity within the kidney creates local microenvironments that enable specialised renal functions. The renal medulla possesses low oxygen tension, generating hypoxia that is inherent to the medulla’s urinary concentration mechanisms. A longstanding hypothesis, supported by MRI studies, is that vascular rarefaction in acute and chronic kidney diseases results in hypoxia within the renal cortex, stimulating neighbouring cells into a pro-fibrotic phenotype and manifesting in loss of organ function. Mapping the regional anatomical layout of the vasculature is fundamental to understand local microenvironments, including the generation of physiological, or susceptibility to pathological hypoxia. To assess regional vascular heterogeneity in the human kidney, HiP-CT scans were semi-automatically segmented into hilar, medullary and cortical zones (Fig. 3A). The volume of each zone in addition to the number of vessels, length, radius and volume of segmented vessels within each zone were quantified (Table 2). Despite the majority of the volume of the human kidney being occupied by the cortex (64.7%) as compared with the medulla (27.3%) or hilum (8.0%), the cortex had the lowest vessel volume (9.5% vs. 14.3% vs. 42.8%). (NB total percentage does not sum to 100%, as vessels that straddle two regions are excluded). As a proxy for renal tissue oxygenation, we quantified (Fig. 3B–C) and mapped (Fig. 3D–E) the inter-vessel distance, reflecting the extravascular distance across which oxygen and solutes diffuse, compartmentalised by hilum, medulla, and cortex. Mean inter-vessel distances for medulla, cortex and hilum (Table. 2) show that the cortex has the lowest inter-vessel distance (1.3 × 103 ± 824 μm) followed by the hilum (1.5 × 103 ± 1400 μm) and medulla (1.6 × 103 ± 1000 μm), thus following anticipated distributions. Notably, there were large portions within the medulla where inter-vessel distance was > 4.5 mm (Fig. 3D), in line with the hypoxic character of the medulla. Whilst the cortex has the lowest inter-vessel distance, small areas with high inter-vessel distance > 4.5mm were found, predominantly towards the renal capsule, which we attribute to the previously discussed resolution limit of these data (see Fig. 1G).","Owing to the limited volume of tissue imageable using modalities such as micro-CT and LSM, and comparatively low resolution of routine clinical imaging, it has been impossible, until now, to capture the entire vascular network of the intact human kidney. We have overcome these limitations using HiP-CT, enabling 3D imaging and segmentation of an entire human kidney arterial network at twenty-fold greater resolution than conventional clinical CT scanners (200 μm voxels), instead comparable to that of light microscopy (1–8 μm pixels) yet at a scale several orders of magnitude larger than that of a renal punch biopsy. The balance between imaging volume and resolution afforded by HiP-CT thus bridges the scale between local cellular structures and global tissue changes, providing quantitative vascular metrics from an intact human organ for the first time. The metric we report here are the first of their kind and thus represent a baseline for the morphology of the human renal arterial network. As further studies are performed, our data will provide a benchmark for natural the variation in human anatomy and also for pathological variations from e.g. diabetes or renal cancer. Using HiP-CT, we show that the human kidney’s arterial network is exquisitely organised across the organ, with branching metrics optimally arranged to minimise resistance to flow akin to rodent species, whilst also possessing regional heterogeneity likely contributing to physiological gradients in local oxygen tension and susceptibility to hypoxia. Beyond biophysical modelling and providing a platform to study renal pathologies, the dataset generated has immediate practical applications, such as providing inputs for bioprinting of artificial kidneys or planning tumour resection whilst preserving renal function. Limitations of this work include the low throughput of the pipeline, the resolution limit of the organ-wide scan, and the limited access to the technique, currently. Emerging solutions to these limitations are provided by, i) machine learning methods for automated segmentation of vessels from imaging data, ii) improvements of the ESRF beamline which are already extending the resolution limit for whole organs down to 8 μm; iii) the release of the data through the Human Organ Atlas portal (https://human-organ-atlas.esrf.eu/). Ultimately, we envisage that mapping microstructural detail will become possible at the scale of the whole kidney, providing a means to link cellular events with organ physiology and pathology.",10.1101/2023.03.28.534566
PMC10516116,37744460,Developing a Novel Image Marker to Predict the Clinical Outcome of Neoadjuvant Chemotherapy (NACT) for Ovarian Cancer Patients,"Objective: Neoadjuvant chemotherapy (NACT) is one kind of treatment for advanced stage ovarian cancer patients. However, due to the nature of tumor heterogeneity, the clinical outcome to NACT vary significantly among different subgroups. The patients with partial responses to NACT may lead to suboptimal debulking surgery, which will result in adverse prognosis. To address this clinical challenge, the purpose of this study is to develop a novel image marker to achieve high accuracy prognosis prediction of the NACT at an early stage. Methods: For this purpose, we first computed a total of 1373 radiomics features to quantify the tumor characteristics, which can be grouped into three categories: geometric, intensity, and texture features. Second, all these features were optimized by principal component analysis algorithm to generate a compact and informative feature cluster. This cluster was used as input for developing and optimizing support vector machine (SVM) based classifiers, which indicated the likelihood of the patient receiving suboptimal cytoreduction after the NACT treatment. Two different kernels for SVM algorithm were explored and compared. To validate this scheme, a total of 42 ovarian cancer patients were retrospectively collected. A nested leave-one-out cross-validation framework was adopted for model performance assessment. Results: The results demonstrated that the model with a Gaussian radial basis function kernel SVM yielded an AUC (area under the ROC [receiver characteristic operation] curve) of 0.806 ± 0.078. Meanwhile, this model achieved overall accuracy (ACC) of 83.3%, positive predictive value (PPV) of 81.8%, and negative predictive value (NPV) of 83.9%. Conclusion: This study provides meaningful information for the development of radiomics based image markers in NACT treatment outcome prediction.","Being the most lethal gynecological malignancy, ovarian carcinomas are anticipated to result in approximately 19,710 new cases and 13,270 deaths within the US during 2023. Among all the diagnosed patients, approximately 90% are categorized as epithelial carcinoma, which can be further divided into four subtypes, namely, serous, endometrioid, clear cell and mucinous carcinomas. Meanwhile, the remaining 10% patients are diagnosed with non-epithelial ovarian cancers, which includes germ-cell or sex cord stromal tumors. Given that there are not effective early stage screening approaches, most ovarian cancer patients are diagnosed at an advanced stage (Ⅲ or Ⅳ). Currently, the standard of care is to first perform primary debulking surgery (PDS) to remove the visible tumors. Then adjuvant chemotherapy (ACT) is followed up to control the residual invisible tumor cells. Recently, one alternative has emerged, suggesting the initiation of neoadjuvant chemotherapy (NACT) before cytoreductive surgery (i.e. interval debulking surgery [IDS]) and additional follow-up chemotherapy. Some investigations have indicated its advantages for certain groups of patients who are not well-suited for PDS. However, due to the tumor heterogeneity, the tumor responses to NACT therapy vary significantly across different patient subcategories. Therefore, the well responsive patients can receive optimal debulking in IDS with either no visible residual tumors or residual tumors measuring less than 1 cm in diameter, which will obtain the best overall survival after the following treatment. Nevertheless, the remaining patients experiencing partial response may have to undergo suboptimal debulking treatment, leaving residual tumors larger than 1 cm in diameter. Therefore, to achieve an optimal treatment effect, it is of crucial importance to develop a prognostic model which is able to identify these patients who have to receive suboptimal debulking surgery after NACT therapy. To date, different approaches have been explored to predict the outcome of ovarian cancer treatments, which are based on FIGO (International Federation of Gynecology and Obstetrics) staging system, demographic information, radiology findings, CA-125 (cancer antigen 125), HE4 (Human Epididymis Protein 4), BRCA 1/2 gene mutations, and so forth. However, due to the limited reliability and robustness of these models, they are not yet widely recognized in clinical practice. As far as we know, this study is one of the pioneering investigations which developed a CT image based clinical marker to predict outcomes of IDS for ovarian cancer patients. We discovered a unique radiomics feature signature that has a strong association with IDS. This feature signature was identified from a comprehensive set of 1373 radiomics features which can be categorized into 3 groups, namely, shape, density, and texture features. We developed a clinical marker based on the feature signature. The effectiveness of this marker was evaluated by a dataset containing a total of 42 patients. The model achieved an overall accuracy of 83.3%, and an area under the ROC (receiver characteristic operation) curve of 0.806 ± 0.078. Among all those technologies used in the prognostic assessment, the computed tomography (CT) scan has its unique superiority over the others due to its advantages of wide availability and low operating cost. Meanwhile, radiomics, a recent emerging approach, can extract and quantify the tumor characteristics from CT images, aiming to uncover the underlying biological mechanism associated with patient’s diagnosis or prognosis. However, few studies have been conducted on using these novel techniques for clinical outcome prediction in NACT treatment for ovarian cancer patients. In this investigation, our objective is to develop a radiomics based CT image marker to accurately identify the patients who may not be able to receive optimal cytoreduction after NACT treatment. The major contributions of this study are summarized as follows:  The details are presented in the following sections.","Database This study was approved by the Institutional Review Board (IRB 13649) at the University of Oklahoma. The patients’ consents were not needed as all the image data were collected retrospectively. This dataset contains a total of 42 advanced stage ovarian cancer patients received NACT regimens. Among them, 28 patients underwent optimal debulking in the following IDS, while the remaining patients did not. All the selected patients were diagnosed with recurrent epithelial ovarian cancer (EOC). Their treatments were performed at the University of Oklahoma Health Sciences Center. The patients’ histology types include endometrial, serous, clear cell, and mucinous. For each patient, we collected the pre-therapy CT images following the standard acquisition protocols. Specifically, the image obtainment was conducted on a GE LightSpeed VCT 64-detector or GE Discovery 600 16-detector CT machines. X-ray tube current was set to be from 100 to 600 mA to fit the various body size, with a tube power of 120 kVp. The examination was conducted by the following procedures: The contrast agent (Isovue 370, 100 cc) was first injected to the patient intravenously at a rate of 2–3 cc per second. The initial scan started 60 seconds after injection began, while the second scan started 5 minutes after the completion of injection. CT images were reconstructed to 1.25 mm in axial axis, and 2.5 mm in sagittal and coronal directions. Image Feature Computation Prior to the image feature computation, we performed tumor segmentation on each case to generate the 3D volume of interest (VOI). An experienced radiologist identified each tumor on the acquired CT images and attached annotation to the image showing the largest tumor area. With the annotated reference images, all the slices containing metastatic tumors can be determined accurately and processed by a previously developed segmentation algorithm, which combines a multilayer topographic region growth algorithm with adaptive thresholds and a dynamic edge tracking method. Due to the heterogeneity of metastatic tumors, the automated segmentation results may not be adequately accurate for the following process. Therefore, the automatic segmentation results were visually evaluated by experienced researchers and the tumor contours were manually adjusted as needed. Next, a large amount of radiomics features were computed from the segmented tumors in the original CT images, using the open-source platform Pyradiomics v3.0.1. Three categories of features were computed: shape, first order, and texture features. The shape features characterize the 3D size and geometry of the lesion from diverse perspectives, such as voxel volume and elongation. First order features depict the intensity distribution of the voxels inside the segmented VOI. The texture features include gray level co-occurrence matrix (GLCM) features, gray level dependence matrix (GLDM) features, gray level run length matrix (GLRLM) features, gray level size zone matrix (GLSZM) features, and neighboring gray tone difference matrix (NGTDM) features. These features enable a quantitative depiction of the tumor textures by assessing the spatial distribution and variation of voxel values. For instance, GLDM features are generated from a matrix that describes the appearance of qualified neighboring voxels surrounding the central voxel to measure the dependency relationships across intensity levels. Furthermore, we also obtained extra first order and texture features from several filtered image types: exponential, gradient, local binary pattern 3D (LBP3D), logarithm, square, square root, and wavelet filters. In summary, a total of 1373 features extracted from the smallest tumor in each case were utilized for training the model, as illustrated in table 1. Develop a Machine Learning based Model to Predict the Clinical Outcome Using the established feature pool, we developed a classification model to predict whether the disease can be optimally debulked in the IDS (Figure 1). At the onset of the model, all the features were first scaled by the formula: , where  is a vector containing original values of a feature,  and  are the corresponding mean and standard deviation calculated from training set, respectively. Given the imbalanced nature of the dataset, SMOTE (synthetic minority over-sampling technique) was employed to augment data in the minority class, which is based on the imbalanced-learn library v0.9.0. In this method, it first selects a sample  from the randomly shuffled minority class. Using  as a reference point, the algorithm locates its  nearest neighbors among minority group. A new sample  will then be synthesized by the formula: , where  is one of the  nearest neighbors and  is randomly selected from the interval (0, 1). This synthetic process will persist until the minority dataset is augmented to the balanced level. Next, feature dimension reduction was conducted using principal component analysis (PCA) on the matrix  containing features of all training instances. As a popular method, PCA can effectively reduce the dimension while retaining variance as much as possible, especially when the dataset is relatively small. Accordingly, PCA generates the principal components (PCs) based on the co-variance matrix . Each PC is an eigenvector of the matrix M representing a direction in the redefined feature space, while the variance explained by this PC is represented by its corresponding eigenvalue. Based on the spectral theorem, these variances quantify the importance of each PC to the matrix. Since the variance decreases significantly, it becomes feasible to use only a small portion of the most important PCs to approximate the original feature vectors with a satisfactory accuracy. We used PCA to reduce the original feature set to less than 10 features prior to classification. After that, a support vector machine (SVM) classifier was implemented using scikit-learn v1.0.2 library, which aims to differentiate between sub-optimally debulked and optimally debulked cases within the dataset. As a robust classification algorithm, SVM serves well in other related studies of prognostic modeling of NACT and has demonstrated its effectiveness in small size dataset based classification tasks. In binary classification tasks, rather than just finding one hyperplane that simply separates the two classes, SVM seeks an optimal hyperplane that has the largest margin between the two classes, relying on the samples closest to the hyperplane (i.e., support vectors). The SVM classifier with linear kernel (Linear-SVM) or Gaussian radial basis function kernel (RBF-SVM) was adopted in the classification task. The regularization parameters C for both kernels were searched among a logarithmic array: 10−5, 10−4, …, 102. For RBF kernel specifically, its kernel coefficient gamma was chosen from another logarithmic array: 10−3, 10−2, …, 10. The pipeline of this machine learning model is depicted in Fig. 1a. Finally, we employed a nested leave-one-out cross-validation (LOOCV) to evaluate the classification performance of the SVM-based clinical outcome prediction models (Figure 1b). LOOCV is a unique type of cross-validation technique, in which the testing set only contains one case and all the remaining cases are used to train the model. This approach can maximally mitigate the influence of random partitioning of dataset, ensuring an almost unbiased evaluation result. However, since hyperparameter tuning was also implemented within the LOOCV process, it incorporates testing data into model selection, which may result in overestimating the model’s generalization performance. Thus, we adopted the nested LOOCV with 2 levels: The outer LOOCV is sorely responsible for the model evaluation; the inner LOOCV is embedded within the outer LOOCV to fine-tune the hyperparameters of each module in the model, including SMOTE, PCA, and SVM. This nested structure effectively decouples the testing data from training data, leading to a significant decrease in evaluation bias, and bringing error estimation close to the true level. The model performance was assessed by the receiver operating characteristic (ROC) curve, and various metrics such as the area under curve (AUC) value, overall accuracy (ACC), positive predictive value (PPV), and negative predictive value (NPV). Overall, the model employed in this preliminary study has limited complexity given the small size of dataset, which aims to minimize the risk of learning from noise rather than the actual patterns. We utilized PCA for dimension reduction and SVM for prognosis classification, recognizing their simplicity and robustness. Similar methodologies have been adopted successfully in prior radiomics based classification studies.","Figure 2 demonstrates one sample case of our tumor segmentation results as well as 6 of the extracted 1373 features from this tumor. All the CT images containing this tumor are displayed from left to right in the first row, and the outline of the ROI tightly wrapping the tumor tissue in each image is identified and marked by red color. The tumor in each CT image is isolated and illustrated in the second row which contains all the information that is going to be extracted for this case. The 3D shape of this tumor is shown in the third row, accompanied by 6 of its logarithm and gradient features. The hierarchically clustered heatmap of Pearson’s correlation coefficients for the extracted 1373 radiomic features is demonstrated in Figure 3, with the values ranging from −1 to 1. The negative value is represented using a brown color, while the positive value is represented by a green color. Darker colors indicate a larger absolute value of the correlation coefficient. The entire heatmap shows that the feature correlations are relatively low, and the dendrogram indicates that the associations occur on the features from different categories. Figure 4 provides the distributions of correlation values between features of the same feature type or image type. From Figure 4a, we can observe that the features within most subgroups have low association, while the only exception is the shape features. When sorting the features based on image types (Figure 4b), it becomes evident that certain features extracted from the same image type exhibit relatively higher correlations with each other. For example, the median correlation coefficient for features derived from gradient filter processed images is around 0.38, while this value is only 0.23 for the wavelet filter subgroup. Figure 1 plots the histogram of the absolute values of all the calculated correlation coefficients. It can be noted that 90.2 % of the correlation coefficient values are less than or equal to 0.5. The results indicate that the extracted 1373 raw features contain comprehensive information describing tumor attributes with negligible redundancy. Table 2 summarizes the Linear-SVM based models’ performance when only using one type of the features in modeling. The GLSZM features perform best among all different categories, yielding an AUC value of 0.638 ± 0.094 and an ACC value of 71.4 %. Meanwhile, the GLCM features achieve the second-highest performance, with AUC and ACC values of 0.633 ± 0.094 and 64.3 %, respectively. Similarly, the shape and first order features can still distinguish the positive cases from negative cases to a certain extent, resulting in AUC values of 0.617 ± 0.095 and 0.559 ± 0.096, respectively. In contrast, the remaining feature classes did not show significant discriminative powers. Figure 6 demonstrates the averaged weight of different feature types applying on the PCA-derived new features when all the 1373 features are used as the input of the Linear-SVM based model. It reflects that the features from different subcategories have approximately equal contributions, with the GLDM and GLRLM contributing slightly more than the others. The ROC curves used to evaluate the performance of the models trained with 1373 features are plotted in Figure 7. The corresponding AUC value of the ROC curve for Linear-SVM model is 0.745 ± 0.086, which was produced by a ROC curve fitting program based on the maximum likelihood estimation method (ROCKIT, http://metz-roc.uchicago.edu/, University of Chicago). In contrast, the RBF-SVM based model produced a ROC curve that is generally higher than that of the Linear-SVM, enhancing its AUC value to 0.806±0.078. Figure 8 shows the confusion matrices for both models. As can be seen from Figure 8(a), out of the 10 cases predicted as “Suboptimal” by the Linear-SVM based model, 7 of them were truly sub-optimally debulked in IDS. This yields a positive predictive value (PPV) of 70% (7/10). On the other hand, among the 32 cases predicted to be “Optimal” by the Linear-SVM, 7 cases were confirmed receiving suboptimal debulking, corresponding to a negative prediction value (NPV) of 78.1% (25/32). Meanwhile, the RBF-SVM model (Figure 8b) offered PPV and NPV values of 81.8% (9/11) and 83.9% (26/31), respectively. Based on above calculations, the total accuracy (ACC) values of the linear and RBF kernel based SVM models reached to 76.2% (32/42) and 83.3% (35/42), respectively. This performance enhancement can be attributed to the presence of nonlinear prognostic information in the dataset, a characteristic that nonlinear RBF-SVM captures more efficiently than Linear-SVM.","In this investigation, a novel image marker was initially developed and verified to categorize patients into optimal and suboptimal debulking groups prior to the NACT treatment. The major significance is this marker’s potential to accurately stratify patients at an early stage. If subsequent research successfully validates this strategy, it could assist gynecologic oncologist to identify patients who may not benefit from the planned NACT treatment. Thus, alternative treatments can be chosen before chemotherapy is administered, allowing patients to avoid ineffective treatments and minimize the risk of severe side effects. Besides its significance, this study has following unique characteristics. Different from the conventional image based NACT prognostic models, our proposed model utilized radiomics features to comprehensively quantify the tumor characteristics. The conventional models rely on either visual findings on the presence of malignancy sites (e.g. pleural effusion or mesentery deposits) or some simple measurement depicted on the acquired images (e.g. tumor diameters). Despite the association between these features and cytoreduction, this method suffers from the inter- and intra- radiologist variability and time-varied diagnostic criteria, which causes low robustness and unsatisfactory performance. In our study, the copious tumor information was thoroughly conveyed by the highly varied radiomics features. Followed by the PCA and SVM, this information was further identified and synthesized to generate a single clinical marker for outcome prediction. In a dataset containing 42 ovarian cancer patients, the marker achieved an overall accuracy of 76.2% and 83.3% when utilizing linear and RBF kernel based SVM algorithms, respectively. The results indicate the effectiveness of our proposed method. In addition, as compared to the other investigated molecular markers, our proposed method will not create additional financial burdens on the patients as CT imaging is a routine examination for ovarian cancer patients. Furthermore, to the best of our knowledge, the computer feature pool in this study is by far one of the largest features pools for this specific or similar medical imaging tasks. We computed a total of 1373 radiomics features for NACT prognosis prediction, which can be divided into three main categories (e.g., shape, intensity, and texture). The prognostic capacities of various feature types were evaluated by training a Linear-SVM based model with each type individually. The result shows the five subcategories of texture features have varying levels of performance on this task: only GLSZM and GLCM indicate significant discriminative power on classifying optimal and suboptimal debulking in IDS (Table 2). Given that these two texture feature subgroups describe the connected or co-occurrence zones with the same grayscale level, it implies the NACT prognosis is more sensitive to such an intensity change. However, in the final feature cluster, all these features demonstrate an approximately equal contribution (Figure 6), which may be attributed by the fact that the features from different groups provide complementary information to the final marker. This observation also implies the necessity of radiomics method: each of these computed 1373 features characterizes the tumor heterogeneity uniquely, which is highly desirable in generating the final marker. Despite the encouraging results, we acknowledge that the study has following limitations. First, the dataset only contains 42 patients from a single institution. The performance and robustness of our proposed algorithm should be further verified on a comprehensive multiple-institution database with diversified patients. Second, the model is developed based on the conventional SVM algorithm, and our study only used radiomics features. Some emerging technologies such as deep learning were not adopted. One possible improvement of this study would be to combine radiomics and deep learning techniques together to further enhance the prediction accuracy. Third, the feature repeatability and reproducibility were not investigated in current study. Utilizing radiomic features that remain consistent across various scanning setups, such as scanning equipment and acquisition software, can effectively decrease the risk of type Ι error and ensure a broader validity of the model. Fourth, given the limited size of the dataset, we did not conduct model parameter uncertainty study. Despite these limitations, this investigation initially demonstrates the efficacy of using radiomic features for predicting NACT outcome in ovarian cancer treatment and lays a solid foundation for advancing precision treatment in future research.",
PMC10690292,38045474,Reproducible image-based profiling with Pycytominer,"Technological advances in high-throughput microscopy have facilitated the acquisition of cell images at a rapid pace, and data pipelines can now extract and process thousands of image-based features from microscopy images. These features represent valuable single-cell phenotypes that contain information about cell state and biological processes. The use of these features for biological discovery is known as image-based or morphological profiling. However, these raw features need processing before use and image-based profiling lacks scalable and reproducible open-source software. Inconsistent processing across studies makes it difficult to compare datasets and processing steps, further delaying the development of optimal pipelines, methods, and analyses. To address these issues, we present Pycytominer, an open-source software package with a vibrant community that establishes an image-based profiling standard. Pycytominer has a simple, user-friendly Application Programming Interface (API) that implements image-based profiling functions for processing high-dimensional morphological features extracted from microscopy images of cells. Establishing Pycytominer as a standard image-based profiling toolkit ensures consistent data processing pipelines with data provenance, therefore minimizing potential inconsistencies and enabling researchers to confidently derive accurate conclusions and discover novel insights from their data, thus driving progress in our field.","In the past thirty years, high-content microscopy has undergone a remarkable technological transformation that has given scientists the ability to acquire thousands of single-cell measurements in high-throughput experiments. This deluge of microscopy data greatly expands opportunities to study cell biology, particularly from a systems biology perspective. In turn, there has been a proliferation of open-source software tools tailored to image analysis, including Fiji, CellProfiler, QuPath, napari, and others. While these tools can rapidly derive biological insights from large microscopy datasets, they lack downstream bioinformatics processing of image-based features, which often must be transformed prior to data analysis. Recently, the field of image-based profiling has emerged to fill these gaps by performing bioinformatics data processing of cell features extracted from microscopy images. By capturing images of cells treated with perturbations such as chemicals or genetic agents, and extracting image-based features, researchers can analyze observed changes occurring in organelles, compare cell states, and develop new hypotheses for biological mechanisms. Thus far, the primary application for image-based profiling has been in drug development. Specifically, image-based profiling plays a central role in Phenotypic Drug Discovery (PDD), offering a route for disease phenotype discovery, target identification, drug repurposing, toxicity assessment, and the exploration of novel therapeutic hypotheses. For example, image-based profiling can determine if certain small molecules can drive disease cell states towards to healthy states. Furthermore, integrating image-based profiling with omics analyses enhances biological insights, as omics and cell imaging provide complementary information about cell states. Image-based profiling has provided novel biological insights, which have been uncovered through extensive data processing steps. However, there is an increasing demand for an actively maintained, open-source software toolkit that supports reproducible and scalable image-based profiling functionalities. In 2017, a consortium of experts established best practices, including feature extraction, data processing, and analysis. A subset of us initiated a library in R (called cytominer), but later pivoted to Python (Pycytominer), finding its extensive ecosystem of machine learning and image processing libraries more conducive for development. The shift toward Python aligns with the current trend of the scientific and image analysis communities, where Python has gained widespread adoption due to its available packages for scientific software. Furthermore, other image-based profiling software tools like the open-source BioProfiling.jl address gaps in other programming languages while also implementing best practices. Similarly, StratoMineR is a commercially available, web-based tool built for analyzing high-dimensional morphological datasets from high-content screening experiments. Other software, like Squidpy, also contains portions of image-based profiling for processing spatial data, placing a distinct focus on cell-type identification and gene expression. Earlier software packages have inspired current work and pursued similar goals, but maintenance has slowed over time and therefore these packages lack critical functionality and user support. Nevertheless, these projects have aided in establishing image-based profiling as a science that has already contributed to novel insights in cell biology and drug discovery. Here we introduce Pycytominer, an open-source Python package for processing image-based profiles extracted from microscopy images of cells. This project resulted from community discussions and consensus stemming from our inaugural work together. Pycytominer is developed through the usage of Python, Pandas, Apache Parquet, and SQLAlchemy to provide a reliable and user-friendly environment for working with image-based features. It uses a modular API, enabling flexible image-based profiling. Pycytominer is actively maintained by a community of users and developers. The project’s repository includes a clear code of conduct, usage documentation, and contributing guidelines, which outline the process for contributors who want to participate in package development. The repository also contains an extensive testing suite, along with continuous integration and continuous development (CI/CD) pipelines, to ensure the program’s stability. Currently, Pycytominer curates and processes image-based features extracted from CellProfiler and DeepProfiler (a deep learning complement to CellProfiler), and we have plans to expand support. Pycytominer is also actively used in several academic and industry labs; for example, scientists used Pycytominer to process two of the largest publicly-available Cell Painting datasets: Joint Undertaking in Morphological Profiling (JUMP) and Library of Integrated Network-Based Cellular Signatures (LINCS). Pycytominer is also used to process the majority of the currently 31 Cell Painting datasets in the Cell Painting Gallery. In this resource paper, we describe Pycytominer as the standard toolkit for image-based profiling. We describe the Pycytominer core API, available data processing functions, and how to orchestrate Pycytominer’s functionality into reproducible pipelines. Furthermore, we present Pycytominer community practices, establishing it as an adaptable and extensible tool for researchers studying image-based profiling.",,"Generating the inputs for Pycytominer processing Pycytominer processes image-based profiles, which are initially extracted from microscopy images. Image-based profiles are information-rich, providing unbiased measurements of cell morphology including cell shape, cell size, stain intensities, textures, and more, which give insights into biological mechanisms. Alternatively, profiles can be derived from deep learning-based networks used as feature extractors, trained by a rapidly advancing set of strategies such as self-supervised learning, vision transformers, and masked autoencoders. Either way, before scientists can use Pycytominer, they must first perform two core steps: (1) Data collection (including experimental design) and (2) image analysis (Figure 1A). Researchers first design an experiment, often subjecting cells to small molecule or genetic perturbations, and incubate them for a certain amount of time. Following an incubation period, cells can undergo staining, where fluorescence dyes are applied to mark specific cellular compartments. The most common image-based profiling assay is the Cell Painting assay. It is important to note that while fluorescence microscopy is a common approach, image-based profiling methods can extract morphologies from any microscopy image type. For example, brightfield imaging, which as a non-destructive technique, can be used for measuring dynamic processes, is becoming more common. We also expect that brightfield imaging will expand in conjunction with the development of virtual stain prediction techniques. After acquiring the microscopy images, scientists must perform image analysis. The raw microscopy images undergo a series of image processing algorithms to enhance image quality, perform cell segmentation, and extract morphology features. For example, images that are acquired through optical microscopy are known to have an uneven distribution of lighting on the images, such as vignetting, which can be corrected with (flat field) illumination correction. Next, single-cell segmentation algorithms produce binary masks, containing pixel information that distinguishes between cell objects and image background. The masks generated by segmentation algorithms, commonly generated by software like CellProfiler and CellPose, provide boundaries of cells and subcellular structures within the images. Feature extraction algorithms use these masks to quantify diverse morphological characteristics per single cell, including cell shape, size, stain intensity, and texture. Alternatively, features can be extracted directly from the images without segmenting single-cells using, for instance, the application of a deep learning-based feature extractor directly on full corrected images (Figure 1B). Either way, image analysis results in a set of quantitative features that describe the observed cellular and subcellular properties. It is essential to process these features before carrying out custom analyses for various biological applications, and Pycytominer provides a standardized toolkit for processing these image-based profiles (Figure 1C). Pycytominer core interface and philosophy Pycytominer provides user-friendly access to image-based profiling processing functions, allowing researchers to efficiently handle and derive insights from large-scale image-based datasets. It encompasses five core methods: Aggregation, Normalization, Feature Selection, Batch Correction, and Cyto Utilities, each of which plays a crucial role in the data processing pipelines (Figure 2A). Pycytominer applies these methods collectively to reduce dataset complexity, ensure consistent feature scaling, identify informative features, rectify batch effects, and offer custom functionality for CellProfiler, DeepProfiler, and Morpheus data. We elaborate on these core methods in Table 1. We rigorously apply open-source best practices during Pycytominer’s development in four main categories: Implementation, testing, release, and community (Figure 2B). (1) Implementation. Pycytominer eases the process of contributing code by providing development container specifications usable in VSCode or GitHub Codespaces that contain the full set of software dependencies needed to develop and test the codebase. When changes are ready, contributors submit pull requests, which must be reviewed to ensure adherence to best practices such as modularization, code styling, and documentation. (2) Testing. Pycytominer’s comprehensive testing suite, including unit tests and code coverage analysis, serves as a crucial step to ensure the correctness and functionality of the software implementation. Testing every new change against the full test suite reduces the introduction of software bugs and ensures consistent behavior across versions. (3) Release. Pycytominer follows semantic versioning and maintains a changelog to ensure users are kept informed of new features and important changes. Releases are made available directly on GitHub and are also packaged for use within Python’s two major package repositories PyPI and conda. In addition, Pycytominer supports operating environment containerization (facilitated by Docker), encapsulating dependencies to enhance reproducibility. (4) Community. Pycytominer cultivates our open-source community by welcoming new contributors with clear contributing instructions and guidelines and a code-of-conduct to ensure professional standards are kept. These community efforts are essential for good collaboration, maintaining quality, and ensuring project sustainability. Embracing the full set of these best software practices fosters a collaborative environment that facilitates continuous improvements, encourages reproducibility, welcomes newcomers, and contributes to package usability for developers and users alike. Pycytominer can facilitate analysis of publicly-available resources and outputs several intermediate data types The increase in high-throughput cell imaging has resulted in an increase in public imaging data repositories, which has highlighted the need for standardized image-based profiling tools. Repositories such as Image Data Resources (IDR) and Bioimage Archive serve as significant sources for raw images. For instance, the IDR repository currently contains 123 studies encompassing 13.7 million images, resulting in 135 terabytes of data. There is also a growing body of numerical image-based profiling data hosted on other generalist repositories like Zenodo, Figshare, and GitHub. Furthermore, the Cell Painting Gallery on the Registry of Open Data (RODA), hosted by AWS, currently contains over 650 terabytes of both image and image-based profiling data across more than two dozen large-scale image-based profiling datasets. In fact, many of the datasets in the Cell Painting Gallery contain image-based profiles generated by Pycytominer. Datasets downloaded from public imaging data repositories can integrate into existing image-analysis workflows, which use Pycytominer to create image-based profiles for subsequent biological discovery and hypothesis testing (Figure 3A). The image-based profiling pipeline uses and generates several different intermediate data types, which we refer to as “data levels”, as established by the LINCS consortium (Figure 3B). Level 1 data are output by microscopy imaging and early image analysis steps (e.g., illumination-corrected images), and represent the rawest data type. These data are often analyzed directly by deep learning models that process pixels. Level 2 data are extracted profiles of image-based features that are output following single-cell segmentation and/or by deep learning models (e.g., DeepProfiler) and other feature extraction tools (e.g., CellProfiler). This is where Pycytominer comes in: Pycytominer generates data levels 3, 4a, 4b, and 5, which represent aggregated, normalized, feature selected, and consensus data, respectively (Figure 3C). Level 4 data are the most common profiles analyzed downstream, but the level 5 data are often used as a consensus phenotypic signature (sometimes referred to as a “fingerprint”) in large-scale drug screens. Scientists can use level 5 data to quickly compare an unknown compound to existing libraries to infer which compound mechanisms may be most similar. Furthermore, Pycytominer users have the option of skipping the aggregation step to normalize single-cell profiles to perform single-cell analyses. Pycytominer outputs all data levels in standardized formats including comma-separated values (CSV), tab-separated values (TSV), and Parquet files. These formats are widely recognized and easily used by other software packages for downstream analysis. Using Pycytominer with the image-based profiling recipe Pycytominer offers a flexible API that enables users to string functions together to perform image-based profiling (Figure 4). The cytomining/profiling-recipe GitHub repository contains a workflow that incorporates many image-based profiling functionalities provided by Pycytominer (Figure 4A). Users can modify the configuration file, enabling precise control over specific analysis parameters to align with their specific research questions (Figure 4B). This control not only enhances customization but also supports data provenance and reproducibility by providing a simple interface for understanding the exact analytical methodologies used. Moreover, we have detailed all these practical steps in our profiling handbook (Figure 4C). Starting from raw microscopy images, the image-based profiling handbook outlines a step-by-step process for delivering analysis-ready image-based profiles.","We developed Pycytominer to serve the growing demand for reproducible, consistent, and open-source image-based profiling. The absence of a standardized approach gives rise to a range of issues that each stem from inconsistent analysis and the tendency to duplicate efforts unnecessarily. Pycytominer reduces analytical inconsistencies and promotes uniformity and reliability in analyses across our field. Furthermore, a rigorous testing framework ensures high code quality with minimal risk of introducing errors. Pycytominer offers an intuitive API that can seamlessly integrate into diverse workflows, ensuring straightforward implementation and customization for various cell types, microscopy methods, assay conditions, and data sources. While this balance between flexibility and modularity promotes a standardized and reliable framework, it also provides a community pathway for future method development and other technical enhancements. Community collaboration is at the center of Pycytominer’s development. Clear contribution guidelines facilitate participation from the open-source Cytomining community, which is focused on developing image-based profiling tools and methods. This collaborative approach ensures ongoing improvements and the validation of Pycytominer’s functionality. However, Pycytominer is not without its limitations. One notable constraint is that it is written in Python, which may exclude users proficient in other programming languages and necessitates integrating their analytical pipelines into Python. To address this, our future roadmap includes more containerization and the addition of command line interface (CLI) options, which will broaden access and offer multilingual support. Second, there may be more optimal image-based profiling methods not yet discovered. In anticipation of these future developments, we have deliberately designed the Pycytominer API, profiling recipe, and testing framework with modularity in mind. This approach allows for the easy incorporation of new methods as they surpass the current state of the art. Third, it is important to note that Pycytominer focuses on a specific segment of the entire image analysis pipeline. Consequently, users are required to be proficient in other software for preliminary processing steps like quality control and segmentation. This decision to concentrate on core image-based profiling functionality comes with drawbacks, but was made to simplify software maintenance and foster direct image-based profiling innovation. Looking to the future, Pycytominer is poised to play an important role as an integral tool for image-based profiling. With a steadfast commitment and a growing community consistently contributing new and optimized functionality, Pycytominer offers a reliable and standardized toolkit that empowers researchers to unveil new insights in multiple fields from cell biology to drug discovery. Disclosure of competing interests The authors declare they have no competing interests relevant to this work. Lead contacts For further information and resource requests, please direct your inquiries to Gregory Way (gregory.way@cuanschutz.edu) and Shantanu Singh (shsingh@broadinstitute.org)",
PMC10592946,37873271,Joint inference of discrete cell types and continuous type-specific variability in single-cell datasets with MMIDAS,"Reproducible definition and identification of cell types is essential to enable investigations into their biological function, and understanding their relevance in the context of development, disease and evolution. Current approaches model variability in data as continuous latent factors, followed by clustering as a separate step, or immediately apply clustering on the data. Clusters obtained in this manner are considered as putative cell types in atlas-scale efforts such as those for mammalian brains. We show that such approaches can suffer from qualitative mistakes in identifying cell types robustly, particularly when the number of such cell types is in the hundreds or even thousands. Here, we propose an unsupervised method, MMIDAS (Mixture Model Inference with Discrete-coupled AutoencoderS), which combines a generalized mixture model with a multi-armed deep neural network, to jointly infer the discrete type and continuous type-specific variability. We develop this framework in a way that can be applied to analysis of both uni-modal and multi-modal datasets. Using four recent datasets of brain cells spanning different technologies, species, and conditions, we demonstrate that MMIDAS significantly outperforms state-of-the-art models in inferring interpretable discrete and continuous representations of cellular identity, and uncovers novel biological insights. Our unsupervised framework can thus help researchers identify more robust cell types, study cell type-dependent continuous variability, interpret such latent factors in the feature domain, and study multi-modal datasets.","Understanding the nature and extent of cellular diversity in the brain is key to unraveling the complexity of neural circuits, their connectivity and roles in behavior, in health and disease. Despite recent advances in high-throughput measurement technologies and related computational approaches, reproducible analysis of high-dimensional single-cell data remains a challenge due to the interplay between hundreds of cell types and multiple sources of continuous biological variability. Cellular phenotypes form a dynamical landscape. In the adult brain, neurons reside within stable “wells of attraction” of this landscape. Here, the well that a cell belongs to represents a discrete source of variability, giving rise to the notion of cell type identity that is accessible via physiological properties, neurotransmitter phenotypes, overall gene expression, and morphological features. On the other hand, the location within the well is a continuous source of variability. Since these two sources jointly determine the phenotype, computational approaches that focus only on one of these sources may infer a qualitatively different characterization of the overall landscape. Moreover, individual wells can have complicated shapes as manifested by within-type variability in molecular, physiological, and anatomical features (e.g., gene expression levels, connectivity strengths), exacerbating the problem. Statistically, such a collection of discrete wells with non-canonical shapes can be modeled by a generalized mixture model. Here, we introduce Mixture Model Inference with Discrete-coupled AutoencoderS (MMIDAS), a mixture model-based method designed to jointly infer interpretable discrete (categorical) and continuous latent factors. We achieve this by utilizing discrete-coupled autoencoders, where the different autoencoding neural networks infer the parameters of their own mixture models from augmented copies of a given sample. We find that encouraging the networks to agree on their categorical assignments via the objective function avoids the notorious mode collapse problem common in generative models, even with a high-dimensional discrete space. MMIDAS breaks from existing methods in two key aspects. First, while numerous deep generative models infer low-dimensional continuous representations, post-hoc clustering on these latent variables is suboptimal, especially in scenarios involving a large number of cell types. This is because the first step of dimensionality reduction is typically not informed about the subsequent clustering objective. Instead, MMIDAS jointly performs clustering and dimensionality reduction in an end-to-end differentiable variational formulation. Second, to interpret within-cluster variability, conventional approaches first cluster cells into discrete types and then characterize the continuous diversity within each cluster separately. Yet, the discrete identity and remaining continuous variability are intimately related to each other. Ignoring these dependencies can produce spurious cluster definitions, which can, in turn, produce flawed characterizations of the remaining within-cluster variability. To address this, MMIDAS offers a unified and interpretable solution by jointly inferring discrete and continuous variabilities and avoids the need for separate modeling for each cluster. This not only simplifies the analysis but also uncovers intricate relationships that might be missed otherwise. Unlike existing methods, MMIDAS is applicable to both uni-modal and multi-modal data. Importantly, it is a fully unsupervised method and does not require any priors on the relative abundances of cell types, labelled data, or batch information. We benchmark MMIDAS on a diverse selection of datasets including two uni-modal single-cell RNA sequencing (scRNA-seq) datasets profiling multiple regions of the mouse cortex obtained by different platforms, a multi-modal dataset with both electrophysiological and transcriptomic profiles of cortical neurons in mice, and a recent scRNA-seq dataset from the human middle temporal gyrus (MTG) exploring changes in gene expression in disease, the Seattle Alzheimer’s Disease Brain Cell Atlas (SEA-AD). We demonstrate that the mixture models learned by MMIDAS discover accurate and interpretable discrete and continuous factors of cellular diversity. It proposes better cell type definitions, as demonstrated by significantly improved clustering and reproducibility metrics. Furthermore, we show that MMIDAS offers novel insights into the molecular mechanisms governing the cellular landscape. The inferred continuous variability encodes biological processes such as cell metabolism and disease stages, which is achieved by identifying genes whose expression controls the continuous within-type variability.","Mixture model inference with discrete-coupled autoencoders (MMIDAS) MMIDAS consists of  mixture variational autoencoders (VAE), i.e. -tuple of independent and architecturally identical autoencoding arms. For an observation , a VAE learns a generative model  and a variational distribution , where  is a latent variable with a parameterized distribution  and  Here, a mixture model structure is imposed on the latent variable by introducing a categorical latent variable , denoting the class label defined in a -simplex, and a continuous latent variable s, which is conditioned on the discrete variable c. We refer to the continuous variable s as the state or style variable interchangeably. c and s together form the latent variables of each (mixture) autoencoder. For completeness, we first derive the evidence lower bound (ELBO) for a single mixture VAE, in which an observation x can be described by one categorical random variable and one continuous random variable, with conditional dependency between c and s. The variational approach to inferring the latent variables corresponds to solving the optimization equation  where  is a family of density functions over the latent variables. Evaluating the objective function requires knowledge of , which is usually unknown. Therefore, we rewrite the divergence term as  Since  is not function of the optimization parameters, instead of minimizing Eq. 2, the variational lower bound with the distributions parameterized by  and ,  can be maximized. The first term in Eq. 3 is referred to as the (negative) “reconstruction loss” or . To generate a sample (conditioned on the observation x), first a discrete latent vector c is sampled from . Given sample , a continuous latent vector is sampled from . The decoder outputs the parameters of , from which we can sample an output vector. We choose  to be a factorized Gaussian, parameterized using the reparametrization trick, and assume that the corresponding prior distribution is also a factorized Gaussian, . For the categorical variable, existing VAE-based solutions impose a uniform structure on . However, this assumption is inadequate for single-cell datasets where the abundances of the clusters typically differ significantly. As a result, earlier solutions have limitations in effectively learning interpretable mixture representations with high-dimensional discrete variables in real-world applications. Additionally, maximizing the ELBO in Eq. 3 creates trivial solutions that underestimate posterior variabilities  and , known as the mode collapse problem, where the network ignores a subset of latent variables. While recent mixture VAE studies, such as JointVAE and CascadeVAE, attempt to resolve this issue, they remain vulnerable to mode collapse in high-dimensional discrete settings and cluster heterogeneity. The multi-arm framework of MMIDAS introduces a consensus constraint on the discrete factors of variability across the VAE models (arms) during the training process to address the aforementioned issues. In this framework, individual VAE arms receive a collection of non-identical copies,  of the given sample, . These copies are sampled from a small neighborhood around x to remain within the same well of attraction and belong to the same category as . (See Data augmentation below.) This novel approach serves to regularize the mixture representations by introducing variability during training, thus inferring a more robust and accurate model. While each arm has its own mixture representation with potentially non-identical parameters, all arms cooperate to learn , where , via a cost function at the time of training. Accordingly, the cost function of a set of VAEs with  arms can be formulated as a collection of constrained variational objectives as follows:  where  is the variational loss for a single arm  (Eq. 3). Propositions 1 and 2 (Supplementary Note 3) show that the shared categorical assignment inferred from , under the  constraint of the multi-arm framework improves the accuracy of the categorical assignment on expectation, and having more arms increases the expected log posterior for the true categorical latent variable unless it is already at its maximum. Additionally, our theoretical results show that the required number of arms satisfying Eq. S35 is a function of the categorical distribution and the likelihood (Eq. S39, Supplementary Note 3). In the particular case of uniformly abundant categories, one pair of coupled arms is enough to satisfy Eq. S35 (Corollary 1, Supplementary Note 3). We note that MMIDAS is an unsupervised method. As such, it does not require weak supervision, unlike. Instead, it relies on representations that are invariant under non-identical copies of observations. Moreover, unlike, the multi-arm framework is not restricted to the continuous space. Reconstruction error. MMIDAS uses two different noise models for handling single-cell data in this paper: (i) Gaussian Noise Model: For log-transformed scRNA-seq data obtained via the Smart-seq technique and electrophysiological data, a multivariate normal distribution with equal variances across features can effectively approximate the single-cell measurements. To achieve this, gene expression values are first normalized to counts per million (CPM) and then transformed using the formula log⁡(CPM + 1). In this case, the corresponding objective that minimizes the reconstruction error in Eq. 4 is the mean squared error (MSE). (ii) Zero-Inflated Negative Binomial (ZINB) Model: For scRNA-seq data obtained through the 10x platform, we use the ZINB model:   with  where  and  are probability mass functions for the count variable .) is an indicator function, which equals 1 when , and  approximates the probability of gene dropout in 10x data.  and  denote the number and probability of success (not being a zero) in the NB distribution, respectively. In this case, MMIDAS uses the negative log likelihood of the ZINB distribution as the reconstruction loss in Eq. 4. Data augmentation In MMIDAS, VAE arms receive non-identical observations that share the discrete variational factor. To achieve this in a fully unsupervised setting, we use type-preserving data augmentation that generates independent and identically distributed copies of data while preserving its categorical identity. Augmentation can be considered as a generative process. Hence, we seek a generative model that learns the transformations representing within-class variability in an unsupervised manner. Learning such transformations is not straightforward: while conventional transformations such as rotation, scaling, or translation can serve as type-preserving augmentations for image data, they may not capture the richness of the underlying process. Moreover, such transformations cannot be used when within-class variability is unknown. Alternatives in the literature either rely on the availability of the class label, or are specific to image data. To this end, inspired by DAGAN, we propose an unsupervised augmentation scheme using a VAE-GAN-like architecture, parametrized by a neural network , which implicitly learns the underlying conditional data distributions.  should generate type-preserving samples (i.e., samples that remain within the well of attraction of the original observation)  conditioned on the given sample, . This is achieved by concatenating the low dimensional representation of  with Gaussian noise n in . To prevent  from disregarding the noise, we formulate the training procedure as the following minmax optimization problem which uses a discriminator network  as a regularizer:  where,     While training,  generates two samples:  and . The former denotes , and the latter is a sample generated in the absence of noise. In Eq. 7 is the value function for the joint training of the discriminator and generator;  is the reconstruction loss, which operates only over  is the triplet loss that prevents network  from disregarding noise and generating identical samples; and  is the Euclidean distance between the latent variables in the absence and presence of noise.  is a regularizer to encourage original and noisy samples to be located close to one another in the latent space. , and  are hyperparameters, each constrained to the range of 0 to 1.  denotes the latent representation of network , not to be confused with the structured latent variables of the mixture VAE arms. Importantly, the augmenter learns to generate samples in the vicinity of a given sample in the latent space, which is independent of s and c. The augmenter does not use any label information in any way. We call this a type-preserving augmenter not because label information was utilized during training or label preservation is guaranteed, but because the augmenter generates samples ‘similar’ to the original x that typically remain in the same well of attraction (i.e., belong to the same cluster). In Supplementary Note 3, Remark 1 further discusses an under-exploration scenario in data augmentation, in which the augmented samples are not conditionally independently distributed and are concentrated around the given sample. Pairwise coupling In MMIDAS, the mixture representation is obtained through the optimization in Eq. 4. Not only is it challenging to solve the maximization in Eq. 4 due to the equality constraint, but the objective remains a function of  which is unknown, and typically non-uniform. To overcome this, we begin with an equivalent formulation for Eq. 4 by applying a pairwise coupling paradigm as follows (details of derivation in Supplementary Note 1):  We next relax the optimization in Eq. 12 into an unconstrained problem by marginalizing the joint distribution over a mismatch measure between categorical variables (full derivation in Supplementary Note 2):  In Eq. 13, in addition to entropy-based confidence penalties known as mode collapse regularizers, the distance measure  encourages a consensus on the categorical assignment controlled by , the coupling hyperparameter. We refer to the model in Eq. 13 as MMIDAS. In this model, VAE arms try to achieve identical categorical assignments while independently learning their own style variables. In experiments, we set  universally. While the bottleneck architecture already encourages interpretable continuous variables, this formulation can be easily extended to include an additional hyperparameter to promote disentanglement of continuous variables as in -VAE. Additional analyses to assess the sensitivity of the performance of MMIDAS to its coupling factor can be found in Supplementary Note 8. In summary, we can cast the optimization in Eq. 13 in an equivalent a collection of constrained variational models as follows:  where  denotes the strength of the consensus constraint. Here,  indicates the assigned category by any one of the arms, , imposing structure on the discrete variable to approximate its prior distribution. Distance between categorical variables  denotes the distance between a pair of -dimensional un-ordered categorical variables, which are associated with probability vectors with non-negative entries and sum-to-one constraint that form a -dimensional simplex, where . In the real space, a typical choice to compute the distance between two vectors is using Euclidean geometry. However, this geometry is not suitable for probability vectors. Here, we utilize Aitchison geometry, which defines a vector space on the simplex. Accordingly, the distance in the simplex, i.e.  is defined as , where  denotes the isometric centered-log-ratio transformation in the simplex. This categorical distance satisfies the conditions of a mathematical metric. Seeking consensus in the simplex An instance of the mode collapse problem manifests itself in the minimization of  (Eq. 13): its trivial local optima encourages the network to abuse the discrete latent factor by ignoring many of the available categories. In the extreme case, the representations can collapse onto a single category; . In this scenario, the continuous variable is compelled to act as a primary latent factor. The model fails to deliver an interpretable mixture representation despite achieving an overall low loss value. To avoid such undesirable local equilibria while training, we add perturbations to the categorical representation of each arm. If posterior probabilities in the simplex have small dispersions, the perturbed distance calculation overstates the discrepancies. Thus, instead of minimizing , we minimize a perturbed distance . This corresponds to the distance between additively perturbed  and  vectors in Aitchison geometry. Here,  and  indicate the mini-batch variances of the -th category, for arms  and . We showed that the perturbed distance  is bounded by  and non-negative values  (Proposition 6, Supplementary Note 4). Accordingly, when  and  are similar and their spread is not small within the mini-batch,  closely approximates . Otherwise, it diverges from  to avoid mode collapse. Consensus score. To report the overall agreement between the categorical representations learned by each VAE arm, at the time of inference, we define the consensus score,  as follows.  where , and .) is an indicator function, which equals 1 when . An ideal consensus score of 1 indicates that the VAE arms exhibit identical categorical representations for the discrete diversity present in the data. Clustering as mapping to a simplex Clustering methods have been widely used to identify cell types. In MMIDAS, we define clustering as membership probability of each cell in a simplex. Vertices of the simplex, inferred as the discrete latent representation, formalize the notion of clusters (cell types). This clustering approach can be considered as a non-parametric technique, which offers flexible modeling of the data distribution for complex data structures. Over-parameterized simplex and network pruning to determine the number of discrete classes Clustering is a classical ill-defined problem, popularly summarized as the lumpers vs splitters problem. Therefore, clustering methods, including non-parametric techniques, cannot determine the number of clusters truly automatically. Instead, the user either needs to provide an estimate of the count or have some prior knowledge about the underlying data structure, forming parameters or hyperparameters of the method. In the same vein, we propose an over-parameterization technique to determine the number of categories: at the beginning of the training, we initialize the dimensionality of the discrete latent space with a number larger than the expected number of cell types, e.g. if one expects around 100 types, set 200 as an upper bound of the number of clusters. To choose the number of clusters, the model evaluates the importance or contribution of each vertex based on the consensus measure among arms. Those vertices which do not contribute with similar probability across arms are pruned. This iterative process continues until the model reaches a minimum level of consensus (a pre-defined hyperparameter) for all vertices in the simplex. Comparative analysis on cell types In the absence of ground truth, evaluating the clustering performance can be challenging. To address this for single-cell analysis, we perform two complementary analyses to quantitatively compare sets of cell types identified by different methods in a given dataset. In the first analysis, we use a classification approach to obtain a measure of post hoc identifiability of the inferred labels. To achieve this, we train a Random Forest classifier and calculate the balanced accuracy score for each set of labels. The balanced accuracy score is the average of the sensitivity and the specificity of the classifier, and is particularly useful in the presence of imbalanced clusters. In the second analysis, we conduct a Silhouette analysis for each set of cell type labels. The Silhouette score provides a measure of how similar a given sample is to other samples in the same cluster as compared to samples in other clusters. The Silhouette score is a real value between −1 and 1. A Silhouette score of 1 corresponds to ideal clusterability where samples in a cluster are identical to each other and different from samples in other clusters. For each set of cell type labels, we report the average Silhouette score per cell type. For both of the aforementioned analyses, we calculated accuracy and the Silhouette score for two different latent embeddings of the data: a linear embeddings using PCA with 100 components, and a nonlinear embeddings, which is denoted as z variable in MMIDAS (depicted in Fig. S10). For each group of cell type labels, we report the highest average performance across embeddings. In instances where the cell type labels are of the t-type labels, the highest scores are consistently attained through the linear embedding. Conversely, in the case of MMIDAS, the maximum scores are consistently achieved through the nonlinear embedding. When considering Merged t-type labels, we find that for Patch-seq and SEA-AD datasets, the nonlinear embedding yields the highest performance, while for the remaining dataset, the linear embedding exhibits the best. Exploring continuous latent factors To interpret and investigate the continuous representation obtained from the data, we conduct two sets of analysis, namely traversal analysis and regression analysis. (i) Traversal analysis: Here, we aim to understand the contribution of the continuous latent factors to the observed profiles, while holding the discrete factor fixed. We achieve this by modifying the continuous variables according to the approximate posterior, . We explore the latent space, quantify how changes in the continuous variables affect the data, while keeping the discrete factor constant. For a given cell  characterized by categorical type c, we change s along the direction that accounts for the maximum amount of variation. Subsequently, we calculate the normalized variation for each feature using the following formulation.  where  denotes the -th feature of x (e.g., -th gene),  and  denote the mean and standard deviation of of the continuous variable, respectively.  stands for the decoder function, such that  denotes the standard deviation of the -th feature across all cells. We use the notation  in the paper for ease of reference. (ii) Regression analysis: The regression analysis serves to explore the intricate relationships between the continuous latent factors and the meta-information associated with the data, such as brain regions or disease progression. In the mouse 10x dataset, we use a Random Forest regression model to quantify the predictability of the cortical regions from the continuous representation. Thus, this study explores the hypothesis that gene expression within a cell type can form spatial gradients in the brain. For the SEA-AD dataset, we opt for a deep regression approach to better capture the intricate and flexible patterns that might exist between the continuous latent factors and the associated meta-data (Fig. S11a). In order to optimize the regression parameters, we use mean squared error (MSE) minimization, as follows:  where the variable  represents the metadata associated with the -th cell, such as the disease pseudo-progression score or the BRAAK score.  is a neural network parametrizing the nonlinear fit between s and y. We quantify the quality of the fit between the latent variable and the meta-information with the coefficient of determination, . Datasets Mouse Smart-seq dataset. This single-cell dataset was released as part of a transcriptomic cell types study. RNA sequencing of 22,365 neurons from the anterolateral motor cortex (ALM) and primary visual cortex (VISp) regions of adult mice was performed with the Smart-seq (SSv4) platform. The cell isolation technique utilized for Smart-seq involved fluorescence-activated cell sorting (FACS). The reference transcriptomic taxonomy proposed for this dataset has 115 cortical neuronal types. To focus on genes relevant to neuronal cell types, genes associated with sex or mitochondria, as well as those showing high expression in non-neuronal cells were removed. Subsequently, a subset of 5,032 highly variable genes (HVGs) was selected. Mouse 10x isocortex dataset. This single-cell dataset was made available as part of a comprehensive transcriptomic study of the mouse cortex. The dataset was generated using the 10xv2 RNA sequencing platform and includes 519,093 glutamatergic (excitatory) neurons and 94,493 GABAergic (inhibitory) neurons collected from seven cortical regions: anterior cingulate area (ACA), auditory cortex (AUD), primary motor cortex (MOp), retrosplenial cortex (RSP), the posterior parietal associative area (PTLp), primary somatosensory area (SSp), and visual cortex (VIS). The proposed transcriptomic cell type taxonomy for this dataset encompasses 113 excitatory and 92 inhibitory cortical neurons. Similar to the gene selection approach used for the mouse SSv4 dataset, a subset of 10,000 HVGs was selected as input for MMIDAS. Patch-seq dataset. This multimodal dataset profiles both transcriptomic and electrophysiological features of neurons in the adult mouse visual cortex. Profiles of inhibitory neurons have already been released by, and those of the excitatory neurons will be available on the Brain map portal (https://portal.brain-map.org/). Following patch-clamp recordings to measure intrinsic electrophysiological features in individual cells, scRNA-seq was performed on the same cells using the SSv4 platform. The reference taxonomy for this dataset includes 92 cell types, which was established by mapping the transcriptomic profiles of neurons to the taxonomy proposed in. The dataset comprises data from a total of 5,165 cortical neurons, each characterized by both a gene expression vector and a set of electrophysiological features. For gene selection, a two-step filtering procedure was followed. In the first step, genes associated with sex or mitochondria, as well as those exhibiting high expression levels in non-neuronal cells were excluded. In the second step, we removed genes that showed strong associations with the experimental platform, thereby reducing any potential platform-related bias. After applying these steps, we obtained a subset of 1,252 HVGs for our analysis. For the electrophysiological modality, following previous studies, we applied principal component analysis (PCA) to each time series. We retained principal components sufficient to explain 99% of the data, resulting in a total of 108 PCs to summarize the electrophysiological time series. We also included an additional 25 measurements of intrinsic physiological features that were computed by the IPFX library (https://ipfx.readthedocs.io/). To ensure consistency across experiments, the PCs were scaled to have unit variance, while the remaining IPFX features were individually normalized to have zero mean and unit norm. The following abbreviations are used for labeling electrophysiological features: action potential (AP), first action potential (AP1), inter-spike interval (ISI), amplitude (Amp), membrane potential (V), current (i), long square (LS), and short square (SS). A more detailed description of each electrophysiology feature can be found in the “Electrophysiology Overview” document available at https://help.brain-map.org/display/celltypes/Documentation. Seattle Alzheimer’s disease dataset (SEA-AD). This dataset is derived from the recently published Seattle Alzheimer’s Disease Cell Atlas (SEA-AD) data, which is publicly accessible through the SEA-AD portal (https://SEA-AD.org/). The dataset consists of scRNA-seq data collected via 10xv3 from the Middle Temporal Gyrus (MTG) tissue of the human brain. Additionally, it includes essential meta-information such as cognitive score, BRAAK score, and demographic details. The cohort consists of 84 donors covering a broad spectrum of Alzheimer’s disease (AD) pathology. The dataset also includes a continuous disease pseudo-progression score (DPS) per donor, ranging from low (score 0) to high (score 1), which was derived using a quantitative image-based neuropathology approach, enabling the ordered arrangement of donors based on their burden of AD pathology. Here, our focus was on investigating four neuronal subclasses that are vulnerable to AD pathology: 299,563 L2/3 Intratelencephalic (IT) glutamatergic neurons, 152,052 L4 IT glutamatergic neurons, 45, 872 Somatostatin-expressing (Sst) GABAergic neurons, and 79,976 Parvalbumin-expressing (Pvalb) GABAergic neurons. The dataset comprises 42 transcriptional types, referred to as “supertypes.” These supertypes were identified through an iterative projection of MTG data from young neurotypical reference donors to itself, and by using scANVI to predict the class label, as explained in. In line with previous datasets, we performed a gene selection process to identify genes relevant to neuronal types. Genes associated with sex or mitochondria, as well as those exhibiting high expression in non-neuronal cells, were excluded. To focus on tissue-specific biology while reducing tissue-specific or technical variability, we independently selected the top 2000 HVGs for each donor. The union of these gene subsets was then utilized for our single-cell analysis. Both the gene selection and single-cell studies were carried out separately for each subclass in this dataset. KEGG Pathway and gene module database In the continuous traversal study, we explored total variation across 18 pathways identified in the KEGG database. Each pathway encompasses a gene module formed by the intersection of KEGG-suggested genes and those available in our datasets. Collection of KEGG pathways from the KEGG database along with the corresponding genes can be found at https://github.com/AllenInstitute/MMIDAS/blob/main/KEGG/KEGG.toml.","Coupled mixture variational model MMIDAS is a collective learning framework that uses multiple variational autoencoding neural networks (VAE) to jointly infer interpretable and accurate discrete (categorical) and continuous factors of variability in the presence of a highsional discrete space (Fig. 1a). MMIDAS consists of a set of independent and architecturally identical VAEs. Each VAE “arm” parameterizes cell type identity (discrete diversity) and continuous type-dependent variability (continuous diversity) (Fig. 1b), and their parameters are tuned by efficient and scalable stochastic gradient descent. In unsupervised VAE-based models, parameter optimization can result in underestimation of discrete variability, known as the mode collapse problem, where the network ignores a subset of discrete latent factors, by overloading the available continuous latents. To avoid this problem, MMIDAS optimizes the parameters of all VAEs simultaneously using a consensus constraint on the discrete factors of variability across the networks, which regularizes the mixture representations at the time of training. While each arm has its own mixture representation, all arms cooperate to learn a consensus category, c, for a given cell, representing the discrete diversity encoding the cell type. This assignment lives in a simplex whose dimensionality, , denotes the expected number of cell types in a given dataset (Fig. 1c). The within-type deviation of the cell’s phenotype around the cell type representative is captured by the continuous latent factor, s (Fig. 1c). While the true number of types and their relative abundances are unknown, MMIDAS utilizes a pruning technique on an over-parameterized simplex to automatically identify the number of discrete cell types in the dataset (Fig. 1d). The underlying principle is that it is harder to reach a consensus over spurious types (Methods). We use the Gumbel-Softmax distribution, a continuous relaxation of the categorical distribution, which allows for a differentiable approximation of sampling from a categorical distribution in the simplex (Methods). The consensus constraint is defined based on a distance metric according to Aitchison geometry in the simplex, which avoids the mode collapse problem (Methods and Supplementary Note 4). Theoretical analysis (Methods and Supplementary Note 3) and experimental results that we present in the next section show that this approach enhances accuracy, robustness, and interpretability of the inferred factors without requiring prior information on the relative abundances of the categories. Before demonstrating MMIDAS on single-cell datasets, we first study a benchmark machine learning dataset, the MNIST handwritten digits dataset (Supplementary Note 5 and Supplementary Note 7). Although MNIST is not as complex as single-cell datasets, we have included it in our evaluation to enable interpretability and facilitate the assessment of MMIDAS results with known ground truth. Our results indicate that MMIDAS outperforms earlier mixture model-based methods, and its computational cost remains comparable to that of the baselines (Fig. S1, Table S1, and Table S2). Supplementary Note 8 and Fig. S4 demonstrate that MMIDAS’s performance is minimally affected by variations in the consensus (coupling) factor. Thus, we do not fine-tune this hyperparameter in any of our studies. scRNA-seq analysis of mouse cortical regions When analyzing uni-modal scRNA-seq datasets, each VAE arm receives a noisy copy of a cell’s profile, on which they try to infer a shared discrete factor through a variational optimization (Methods). In the unsupervised setting, the noisy copies are generated by “type-preserving” augmentation, which generates independent and identically distributed copies of data within a small neighborhood around the original sample, thus remaining in the well of attraction and preserving the cell’s type identity with high probability (Methods, Supplementary Note 6, Fig. S2b, c, and Fig. S3). Transcriptomic cell type identification. We first study cellular diversity in a Smart-seq dataset of the mouse primary visual (VISp) and anterior lateral motor (ALM) cortices with 115 transcriptomic types (t-types) according to the reference hierarchical clustering scheme (Fig. 2, Fig. S5a). In this dataset, MMIDAS with a pair of VAE arms (Supplementary Note 10), uncovers 92 distinct transcriptomic categories using a pair of VAE arms(Fig. 2a). At the time of inference on a test set, the two VAEs agree with each other on 94% of their categorical assignments, which we define as the consensus score (Fig. 2b, Methods). These results were obtained by optimizing an initial, over-parameterized 120-simplex and a 2-dimensional continuous space (Supplementary Note 9). Mutual information analysis shows strong alignment between categories inferred by MMIDAS and the reference t-types (Fig. 2a). The MMIDAS categories, however, uncover a more reproducible and separable discrete representation of the underlying data as quantified by the classification performance on 10-fold cross-validation (Fig. 2c) and the average Silhouette score (Fig. 2d), which takes values between −1 and 1, where 1 denotes perfect clusterability. To isolate the improvement due to MMIDAS, Fig. 2c and Fig. 2d also show the results due to merging of the t-types according to the reference hierarchy until 92 clusters are obtained. We next analyze the expression landscape of the mouse cortex as profiled by the 10x platform, where the reference taxonomy reports 113 glutamatergic and 97 GABAergic t-types (Fig. 3, Fig. S5b,c). In contrast, MMIDAS infers 97 glutamatergic and 70 GABAergic categories (Fig. 3a,b). As before, Fig. 3c,d,e and Fig. S9 quantify that MMIDAS offers a significantly more reproducible and identifiable characterization of the discrete neuronal diversity compared to the reference taxonomy. Indeed, the superiority of MMIDAS is even more evident in analyzing scRNA-seq data obtained by the  platform, whose throughput, noise level, and gene dropout rate all tend to be higher. Inferring genes contributing to continuous diversity. While the discrete representation uncovers relevant transcriptomic categories, the latent representation of continuous type-specific variability captures the intrinsic structure of each type. Fig. 2e demonstrates the continuous representation, a 2-dimensional latent space of all cells in the Smart-seq dataset, highlighting the type-dependent variability for a few example types. As expected, clusterability of the overall dataset based on the continuous latent variable appears much lower since this information is captured by the categorical-like variable c. Yet, the remaining continuous variability, s given c, is also highly structured and displays different dominant axes and shifts across categories. The continuous representation can help to characterize the contribution of each gene to both the discrete and continuous heterogeneity. To study correlates of the continuous representation with gene expression, we introduce a traversal analysis. We hold the discrete variable at a constant value (thereby fix the cell type), move along the span of the continuous representation, and evaluate the change in reconstructed expression values for each gene (Methods). To quantify this change and enable comparisons across cell types, we introduce , the normalized change in the expression of gene  triggered by traversing the continuous variable s along the primary principal component of the continuous representations of cells in the given type (Methods). Fig. 2f and Fig. 2g showcase the results of this traversal analysis for six MMIDAS T categories within the Lamp5, Sst, Vip, L5 NP, and L2/3 IT subclasses. In Fig. 2f, the x-axis lists two gene groups: the first comprises 15 neuropeptide precursor (NPP) genes contributing to neuropeptide signaling pathways, and the second consists of a set of 35 housekeeping genes (HKGs) primarily involved in cellular oxygen consumption within the oxidative phosphorylation pathway. We observe that NPP genes, which exhibit highly specific expression across different neuronal types, display lower variability under the continuous variable traversal. In contrast, HKGs, integral to fundamental cellular maintenance functions, exhibit greater variation (Fig. 2f). Consistent with previous studies, this implies that NPP genes play a more substantial role in the discrete heterogeneity rather than the continuous one. HKGs from the Cox, Ndufa, and ND families, which are related to the electron transport chain and oxidative phosphorylation, are far more variable within cell types, suggesting there may be modules of genes related to a notion of cellular state that the continuous variable captures. Our results also suggest that the expression of activity-regulated genes (e.g. Cox) is cell type-dependent so that these genes respond differently to neuronal activation based on the underlying cell type (compare sub-figures in Fig. 2f). To investigate this further, we selected gene modules implicated in a broad range of pathways, that a priori are rather generic cellular processes not expected to govern cellular identity (Fig. 2g). We observe that different pathways drive continuous variability across types. For example, variability in synaptic vesicle cycle pathway genes is the most in cells of type L5 NP T_32, whereas that in glycolysis/gluconeogenesis pathway genes is much higher in cell type Vip T_46 (among the types shown in Fig. 2g). A more comprehensive picture is provided in the Supplementary Figures S6, S7, and S8. The state variables inferred by MMIDAS can thus be viewed as a handle to explore and identify cell type specific gene modules that may be driving variability within populations. Spatial gradient analysis of neuron types across regions of the isocortex. We next study whether the continuous type-dependent factor can inform about spatial organization. Fig. 3f and Fig. 3i demonstrate such encoding of a contiguous set of cortical regions – anterior cingulate area (ACA), auditory cortex (AUD), primary motor cortex (MOp), posterior parietal associative area (PTLp), retrosplenial cortex (RSP), primary somatosensory area (SSp), and visual cortex (VIS) – by the continuous latent factor, s, for L2/3 IT and L4/5 IT neurons in the mouse 10X dataset, consistent with previous literature. The power of the continuous representation in predicting spatial location is more evident in Fig.3h and Fig.3k, which quantify the separability of cortical areas based on inferred  (PTLp is a relatively small region with a smaller number of cells in this dataset, which can explain its confusion with other regions). We contrast this with a glutamatergic subclass, the Vip cells, for which MMIDAS did not identify such spatial gene gradients across the cortical sheet (Fig. 3l,n). Thus, the continuous type-dependent latent factor inferred by MMIDAS can effectively encode spatial gradients of gene expression, when they exist. Multimodal single-cell analysis Simultaneous capture of a diverse range of cellular features from individual cells is key to attaining a definitive characterization of neuronal diversity. A primary challenge in analyzing multimodal single-cell datasets is disentangling factors of variability that are shared among modalities (e.g., cell types) from those that are unique to specific modalities, without access to (and the potential bias of) labeled data. MMIDAS resolves this issue by enabling joint inference of both (shared) discrete and (modality-specific) continuous variabilities. In the context of multimodal data analysis, individual VAE arms learn mixture representations for their respective input modalities (i.e., one VAE arm per modality) while being constrained to learn similar discrete representations across modalities (Fig. 4a). We study transcriptomic and electrophysiological diversity in a large dataset of mouse visual cortical neurons (Sorensen, Gouwens, Wang et al., unpublished) (Fig. 4). These two observation modalities were profiled by the Patch-seq technique, which combines scRNA-seq (T data) with whole-cell patch clamp electrophysiology (E data). In this dataset, MMIDAS uncovers 60 shared TE categories (Fig. 4b,c) with a consensus score of 81% (Fig. 4b) between the T and E modalities. As in the previous datasets, the classification and Silhouette analyses for each modality indicate that consensus TE categories inferred by MMIDAS offer a more accurate and reproducible discrete representation than the t-type or the merged t-type labels (Fig. 4d,e,f). Noting that the reference transcriptomic taxonomy suggested 97 t-types (and MMIDAS inferred 92 categories in the analysis of the VISp Smart-seq dataset), we observe that the presence of another data modality, E data, influences cell type definitions and leads to fewer cell types. On the other hand, the pronounced alignment between TE categories and t-type labels(Fig. 4c) suggests that a diagonally-dominant, square-like confusion matrix is potentially attainable between transcriptomic and morpho-electric characterizations. Fig. 4g and Fig. 4h illustrate the continuous representations corresponding to a subset of categories for each data modality. We examine the relationship between the features of each modality and their corresponding continuous representations through the continuous traversal analysis. Fig. 4i,j,k showcase the findings of this analysis for four MMIDAS TE categories within the Lamp5, Pvalb, Sst, and L4 IT subclasses. Each individual panel in Fig. 4i illustrates the variation of electrophysiological features resulting from traversing  along the primary principal axis (associated with the category) within the continuous representation of the  modality. Here, the x-axis corresponds to a set of 25 intrinsic physiological features (IPFX features – see the Datasets section in Methods). We observe that, unlike most other features, the average firing rate (Avg_Rate_LS) does not seem to be modulated within individual categories. Moreover, the intrinsic electrophysiology of some categories seem to be highly stereotyped (e.g., Pvalb TE_60), while others display much higher within-type variability (e.g., Sst TE_39, L4 IT TE_2). Fig. 4j and Fig. 4k present the outcomes of continuous traversal (for the same TE types) obtained by traversing  along the principal axis for the T modality. In Fig. 4j, the x-axis shows four gene sets: 16 NPP genes contributing to neuropeptide signaling, 15 genes linked to Wnt signaling, 15 genes related to MAPK signaling, and 15 genes involved in circadian entrainment. Among their many functions, the Wnt- and MAPK signaling pathways are thought to regulate neuronal homeostasis and synaptic plasticity. Consistent with our earlier findings (Fig. 2f,g), NPP genes display lower variability under changes in the continuous latent factor relative to the remaining gene subsets (Fig. 4j). We further refine our focus on pathways comprising a minimum of 15 genes in this dataset (Fig. 4k). As in the mouse Smart-seq dataset, the level of variability is a function of both the pathways and the TE types. For instance, the variation in the gene set related to Wnt signaling is most pronounced in cell type L4 IT TE_2, whereas the variation in genes associated with MAPK signaling is notably higher in cell type Lamp5 TE_17 (among the considered types). Cellular heterogeneity in Alzheimer’s disease Many neurological diseases, such as Alzheimer’s disease (AD), display progressive pathology and clinical symptoms. Such progression impacts cells differentially, although imperfect understanding of phenotypic changes in cell types as a function of disease progression presents a challenge. As a further complication, disease progress may include changes to the relative abundance of cell types as well as within-type continuous transitions over the gene expression and pathology landscapes. We address these challenges by exploring the recent Seattle Alzheimer’s Disease Brain Cell Atlas (SEA-AD) dataset, which includes scRNA-seq profiles from the middle temporal gyrus (MTG) of 84 donors spanning the spectrum of AD pathology (Methods). We train a MMIDAS model with two arms on three subclasses of neurons in this dataset; the IT, Sst, and Pvalb neurons. The original SEA-AD study describes 42 transcriptomic categories for this set (13 IT, 16 Sst, 13 Pvalb), referred to as “supertypes” and obtained by mapping transcriptomic data to the BICCN neurotypical reference data and annotating cells via scANVI. In contrast, MMIDAS infers 30 categories (14 IT, 9 Sst, 7 Pvalb), explaining GABAergic variability with a smaller number of discrete types. Instead, the continuous variability around those (fewer) discrete types contributes to the observed transcriptomic diversity. Beyond this difference, the mutual information scores (Fig. 5a,b) suggest a reasonable correspondence between the inferred categories and the SEA-AD supertypes. We also observe more confusion between the categorical assignments of L2/3 IT neurons and some Sst neurons. Similar to our previous results, the classification and Silhouette analyses of transcriptomic types indicate that consensus T categories inferred by MMIDAS are more accurate and reproducible than the suptertypes or the merged supertypes (Fig. 4c,d). We next study changes to the discrete and continuous factors of variability across the spectrum of AD pathology. For this analysis, we use the disease pseudo-progression score (DPS), which is a number between 0 and 1, increasing with disease progression and calculated per donor, capturing neuropathological changes in the MTG associated with AD. By calculating the expected categorical assignment probabilities, , over cells belonging to a donor, we observe declines in the relative abundances of L2/3 IT and Sst neurons (i.e., their categorical probabilities) during early pathology (Fig. 5e, Fig. S11), suggesting selective death of those neurons during early stages of AD. In contrast, L4 IT and Pvalb neurons display reductions to their relative abundances during later stages of the disease (Fig. 5e, Fig. S11). The rate of cellular loss in Alzheimer’s disease can be summarized by the change in  between the early and late stages of the disease. Notably, a considerable amount (35%) of the transcriptomic types demonstrate a significant reduction (, Welch’s t-test) as they transition from the early to the late stage of the disease. Lastly, we conduct a non-linear regression analysis to explore the relationship between the continuous latent variables inferred by MMIDAS and the DPS (Methods, Fig. S11a). Fig 5f and Fig 5h reveal novel correlations between type-dependent continuous variables and the disease stage for IT neurons in the MTG. Similarly, Fig. S11c,d,e,f demonstrate that the within-type, continuous factor of variability, s, can also serve as an accurate predictor of the BRAAK score (staging of AD-associated neurofibrillary pathology) for IT neurons. In contrast, the continuous variability of GABAergic neurons does not exhibit significant predictive power over the DPS (, Fig 5f,h). In order to asses the potential influence of batch effects on our findings, we conducted another set of analyses where we intentionally omit data from a few donors during the training phase. Later, in the testing phase, we utilize the trained model to predict outcomes for the excluded donors (Fig. S11b). These results reveal meaningful predictions that are robust against donor-specific variations and batch effects.","In response to an ever-changing environment to serve the organism, the cell’s phenotype is in a constant flux. This exacerbates the challenge of analyzing the landscape of cellular diversity, which is perhaps most confounding in the mammalian brain with hundreds, and perhaps thousands, of types of brain cells. In this work, we introduced a novel and precise deep variational inference method, MMIDAS, for generalized mixture models that is especially suitable for high-dimensional data and large number of categories. It utilizes multiple autoencoders that are collaboratively trained to achieve a consensus discrete representation, enabling us to overcome various technical challenges associated with generalized mixture model inference. We demonstrated MMIDAS on four datasets spanning multiple technologies, brain regions, conditions, and species. MMIDAS models cellular heterogeneity as continuous variability around discrete cluster representatives. We demonstrated that this approach significantly improves the identifiability and clusterability scores of the inferred cell types with respect to those of the reference clusters. Importantly, we also observed that the number of cell types inferred by MMIDAS tends to be significantly smaller than that of the reference taxonomies, and the present work may help contribute to resolving the large number of clusters observed in single cell experiments. Dissecting discrete and continuous factors of variability enables identification of key genes or other features associated with aspects of cellular identity or organism-level observables, such as disease progression. In multimodal settings, MMIDAS allows for the identification of biological factors that influence multiple modalities, as well as aspects that are specific to individual modalities. This integrated analysis provides a comprehensive and accurate description of cellular diversity. In both unimodal and multimodal analysis, if it is desirable to use existing cell type labels, one autoencoder in MMIDAS can be replaced to supply those available labels during training. This way, new cells can be analyzed into their discrete and continuous factors based on those available training labels. The present approach has certain limitations for data without an underlying categorical basis. MMIDAS assumes that the underlying data manifold can be faithfully represented via a discrete variable that denotes the cluster identity (type), and a continuous variable that denotes the within-cluster variability. Consequently, datasets that deviate from this model are ill-suited for MMIDAS. Nevertheless, contemporary cell type analysis is based on the assumption of fundamental underlying types with variable state, as opposed to a continuum of states, and MMIDAS was developed supporting this approach. Another challenge is the choice of hyperparameters, which MMIDAS shares with other unsupervised approaches. Fig. S4 shows that MMIDAS’ performance is robust to the value of the coupling factor within a relatively large range. Supplementary Note 9 discusses the difficulty of model selection for the dimensionalities of the latent factors. Lastly, Table S3 shows that MMIDAS’ performance further improves with more VAE arms, although this produces diminishing returns with considerable computational cost. Brain cells occupy a diverse set of locations in a complicated landscape of cellular phenotypes. As the signals they receive and the homeostatic needs change, cell states move within wells of attraction of that landscape to best help the organism they are a part of. However, identifying the wells of attraction and the shapes of those wells may become ambiguous or ill defined. We believe MMIDAS can help researchers to dissect such perplexing single-cell datasets in a principled and efficient manner.",10.1101/2023.10.02.560574
